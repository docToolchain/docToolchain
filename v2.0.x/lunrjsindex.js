var documents = [

{
    "id": 0,
    "uri": "030_news/2021/2021-11-23-Open-Source-Camp.html",
    "menu": "news",
    "title": "Open Source Camp",
    "text": " Table of Contents Open Source Camp Open Source Camp .center { text-align: center; } The next Cyberland Open Source Camp is just around the corner. This time, docToolchain will be one of the featured open source projects! Calling all contributors and supporters of the project to think about the tasks we could realistically solve in those few hours. Here are some ideas from our list of open issues to get your brains into gear: Autocompletion for the CLI . generateSite – additional navigation level on the left . Create multi-branch GitHub pages example . Investigations with Gitpod . A new InitRepo task . If you have another idea, please let me know . To vote for a task, go to GitHub and ❤️ the issue. "
},

{
    "id": 1,
    "uri": "030_news/2021/2.0.3-release.html",
    "menu": "news",
    "title": "Release v2.0.3",
    "text": " Table of Contents docToolchain v2.0.3 Has Been Released! About This Release Changelog v2.0.3 docToolchain v2.0.3 Has Been Released! About This Release This is a bug-fix release. Thanks to everyone who contributed! Changelog v2.0.3 Added #681 - Please reactivate single page manual on v2.0.x site Fixed 2021-11-10 fix #693 - on windows powershell, targetDir is set wrong fix #695 - generateSite: toc attributes 2021-11-09 fix #690 - previewSite: exception if folder does not exist 2021-11-08 fix #687 - wrong encoding of emojis fix #688 - htmlSanityCheck: config of sourceDir is wrong fix #689 - code highlight: css clash with blog post tags fix #682 - generateSite: copyImages uses the wrong target "
},

{
    "id": 2,
    "uri": "030_news/2021/2.0.2-release.html",
    "menu": "news",
    "title": "Release v2.0.2",
    "text": " Table of Contents docToolchain v2.0.2 has been released! About This Release Changelog v2.0.2 docToolchain v2.0.2 has been released! About This Release This is a bug-fix release. Thanks to everyone who contributed! Changelog v2.0.2 Added 2021-10-19 use :jbake-rightcolumnhtml: to add some html to the right column Changed 2021-10-19 Add #667 - GH Actions Default Build 2021-10-18 Fix #664 - doctoolchain.org link and typos Fixed 2021-10-19 fix example for gitRepoUrl in config fix projectRootDir fix status.png and siteTitle 2021-10-17 fix #660 - generateSite: projectRootDir wrong 2021-10-11 fix #651 - powershell: broken install when space in user path 2021-10-15 fix #658 - generateSite - subsequent runs won&#8217;t fail with an invalid or missing site theme "
},

{
    "id": 3,
    "uri": "030_news/2021/2.0.1-release.html",
    "menu": "news",
    "title": "Release v2.0.1",
    "text": " Table of Contents docToolchain v2.0.1 has been released! About This Release Changelog v2.0.1 docToolchain v2.0.1 has been released! About This Release This is our first bug-fix release. Thanks to everyone who contributed! Changelog v2.0.1 Added 2021-10-06 experimental: globalReferences Changed 2021-10-04 fix #616 - exportOpenAPI: Enhance the Confluence Open-API Documentation API to refer to URLs Fixed 2021-10-06 fix #636 - theme: larger admonition icons fix #649 - generateSite on powershell wrong file-separator 2021-10-04 fix #645 - exportJiraIssues: Could not get unknown property 'configFile' for task ':exportJiraIssues' of type org.gradle.api.DefaultTask 2021-09-30 fix #632 - generateHTML: broken images with generateHTML "
},

{
    "id": 4,
    "uri": "030_news/2021/2.0.0-release.html",
    "menu": "news",
    "title": "Release v2.0.0",
    "text": " Table of Contents docToolchain v2.0.0 has been released! Changes and New Features Coming Soon A Personal Thank You from Ralf Changelog v2.0.0 docToolchain v2.0.0 has been released! Today marks an important milestone for docToolchain: the release of v2.0.0, aka ‘ng’ or next generation! The focus of this huge release is simplified setup, better usability and distraction-free writing. Changes and New Features The most important change is the switch-to-a-command-line based approach . You can now fully focus on your documentation without crowding your repo with lots of other files like scripts and themes. This is made possible by the new docToolchain wrapper (or dtcw for short). dtcw is a little script that resides together with a config file in your repository. It ensures that the right docToolchain version is in use, ensures that all prerequisites are met, and it even checks whether Docker or a local installation should be used to execute docToolchain. And, if docToolchain is not currently installed, dtcw will install it for you. In this release, we have tried very hard to avoid error messages . Where we can’t avoid them, we’ve tried to make them as meaningful as possible. Another huge feature is the generateSite -Task which includes jBake as static site generator to help you to create awesome docs. You can now create a full documentation website with a few commands. We chose the beautiful docsy theme from hugo, and it even contains a local search implemented with lunr. To get you started with your solution architecture docs, we’ve added the downloadTemplate -task which delivers the famous arc42 template in four languages. And last but not least, we have used the new generateSite -Feature to restructure docToolchain’s very own documentation! Coming Soon Our goal for the coming weeks is to provide tutorials that demonstrate all of the new features. A Personal Thank You from Ralf Thank you to all docToolchain contributors, sponsors and users for your efforts in getting this major release out the door. I wouldn’t have made it this far without you. Happy writing! Ralf Changelog v2.0.0 Added 2021-09-21 added lunrjs as local search engine 2021-09-06 added warning when running on WSL added dtcw.bat to avoid execution restrictions updated developer docs added docsy as theme 2021-05-21 if the outputPath from the config starts with a '/', it will be considered as absolute path. This way, you can move the build folder outside of your repository. outputPath = System.getenv('HOME')+'/.doctoolchain/build/'+(new File('.')).canonicalPath.md5() repository theme gets only copied to build if it is defined in siteFolder generateSite will now add meta-data to all asciidoc files which have no meta-data defined. The menu name and order will be created corresponding to the folder and file name. The title will be extracted as first headline from the file itself. 2021-05-18 Headless mode for themes fix landing page (only gets copied once when microsite config isn&#8217;t set yet.) 2021-04-07 generateSite can now handle themes 2021-04-09 downloadTemplate can now handle further templates 2021-02-26 copyThemes task generateSite task 2021-02-23 first version of powershell wrapper 2021-02-22 manual test script 2021-01-05 dtc wrapper 2021-01-08 first definition of a static site taken from arc42-template-project updated gradle wrapper (6.6.1) added downloadTemplates task added feature to automatically create a Config file if it doesn&#8217;t exist configured gradle to run without daemon Changed 2021-09-22 streamingExecute (exportPPT, exportEA) now emits a note instead ot an error when running on linux brushed up powershell wrapper Fixed 2021-09-21 updated exportContributors to follow moved files fixed downloadTemplate 2021-09-18 fixed favicons fixed blog 2021-05-22 fix copyThemes to also copy the external theme 2021-05-06 fix #574: publishToConfluence: Problem with wrong ancestorId 2021-04-28 fix copyImages for generateSite 2021-03-02 removed default imagesdir for generateSite 2021-03-01 fixed imagesdir typo 2021-03-01 updated docs for generateSite fixed menu for generateSite 2021-02-27 fixed createDist task fixed plantUML for generatePDF fixed plantUML for generateSite 2021-02-24 [543] dtcw: added pre-requisites check and alternative curl instead of wget 2021-02-22 handling of images for generateHTML "
},

{
    "id": 5,
    "uri": "030_news/2021/last-release-candidate.html",
    "menu": "news",
    "title": "Last Release Candidate",
    "text": " Table of Contents Last Release Candidate Last Release Candidate Good news! We’ve just (we hope!) released the final last release candidate. All known major issues have been fixed and lots of cool features have been added. All remaining and unknown issues will be fixed in 2.0.x versions. "
},

{
    "id": 6,
    "uri": "030_news/index.html",
    "menu": "news",
    "title": "Overview",
    "text": " Table of Contents Overview Overview Find all published news on this page. "
},

{
    "id": 7,
    "uri": "015_tasks/03_task_exportEA.html",
    "menu": "tasks",
    "title": "exportEA",
    "text": " Table of Contents exportEA About This Task The Optional Parameter Configurations Glossary export Further Reading Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportEA Important Currently this feature is WINDOWS-only. See related issue 4 minutes to read About This Task By default, no special configuration is necessary. However, several optional parameter configurations are available to support a project and packages to be used for export. These parameters can be used independently from one another. A sample of how to edit your projects' Config.groovy is provided in the 'Config.groovy' of the docToolchain project itself. The Optional Parameter Configurations connection Either set the connection to a certain project, or comment it out to use all project files inside the src folder or its child folder. packageFilter Add one or multiple packageGUIDs to be used for export. All packages are analysed, if no packageFilter is set. exportPath Relative path to base 'docDir' to which the diagrams and notes are to be exported. Default: src/docs. Example: docDir = 'D:\work\mydoc\' ; exportPath = 'src/pdocs' ; Images will be exported to 'D:\work\mydoc\src\pdocs\images\ea', Notes will be exported to 'D:\work\mydoc\src\pdocs\ea', searchPath Relative path to base 'docDir', in which Enterprise Architect project files are searched Default: src/docs. Example: docDir = 'D:\work\mydoc\' ; exportPath = 'src/projects' ; Lookup for eap and eapx files starts in 'D:\work\mydoc\src\projects' and goes down the folder structure. Note : In case parameter 'connection' is already defined, the searchPath value is also used. exportEA starts opening the database parameter 'connection' first then looks for further project files either in the searchPath (if set) or in the docDir folder of the project. glossaryAsciiDocFormat Whether or not the EA project glossary is exported depends on this parameter. If not set or an empty string, no glossary is exported. The glossaryAsciiDocFormat string is used to format each glossary entry in a certain AsciiDoc format. The following placeholders are defined for the format string: ID, TERM, MEANING, TYPE. One or more can be used by the output format. For example: A valid output format is to include the glossary as a flat list. The file can be included where needed in the documentation. glossaryAsciiDocFormat = TERM:: MEANING Other format strings can be used to include it as a table row. The glossary terms are sorted in alphabetical order. glossaryTypes This parameter is used in case a glossaryAsciiDocFormat is defined, otherwise it is not evaluated. It&#8217;s used to filter for certain types. If the glossaryTypes list is empty, all entries will be used. For example: glossaryTypes = [Business, Technical] diagramAttributes If set, the string is used to create and store diagram attributes to be included in the document alongside a diagram. These placeholders are defined and populated with the diagram attributes, if used in the diagramAttributes string: %DIAGRAM_AUTHOR% , %DIAGRAM_CREATED% , %DIAGRAM_GUID% , %DIAGRAM_MODIFIED% , %DIAGRAM_NAME% , %DIAGRAM_NOTES% , %NEWLINE% Example: diagramAttributes = Last modification: %DIAGRAM_MODIFIED% You can add the string %NEWLINE% where a line break will be added. The resulting text is stored next to the diagram image using the same path and file name, but a different file extension (.ad). This can be included in the document if required. If diagramAttributes is not set or an empty string, no file is written. additionalOptions This parameter is used to define the specific behavior of the export. Currently these options are supported: KeepFirstDiagram If diagrams are not uniquely named, the last diagram will be saved. If you want to prevent diagrams from being overwritten, add this parameter to additionalOptions. Glossary export By setting the glossaryAsciiDocFormat, the glossary terms stored in the EA project will be exported into a folder named 'glossary' below the configured exportPath. In case multiple EA projects are found for export, one glossary per project is exported - each named using the project&#8217;s GUID plus extension '.ad'. Each individual file will be filtered (see glossaryTypes) and sorted in alphabetical order. In addition, a global glossary is created by using all single glossary files. This global file is named 'glossary.ad' and is also placed in the glossary folder. The global glossary is also filtered and sorted. If there is only one EA project, only the global glossary is written. Further Reading Read the JIRA to Sparx EA and Did you Ever Wish you Had Better Diagrams? blog posts. Source build.gradle task exportEA( dependsOn: [streamingExecute], description: 'exports all diagrams and some texts from EA files', group: 'docToolchain' ) { doFirst { } doLast { logger.info(docToolchain &gt; exportEA:  + docDir) logger.info(docToolchain &gt; exportEA:  + mainConfigFile) def configFile = new File(docDir, mainConfigFile) def config = new ConfigSlurper().parse(configFile.text) def scriptParameterString =  def exportPath =  def searchPath =  def glossaryPath =  def readme = This folder contains exported diagrams or notes from Enterprise Architect. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportEA` to re-export files  if (!config.exportEA.connection.isEmpty()) { logger.info(docToolchain &gt; exportEA: found  + config.exportEA.connection) scriptParameterString = scriptParameterString + -c \${config.exportEA.connection}\ } if (!config.exportEA.packageFilter.isEmpty()) { def packageFilterToCreate = config.exportEA.packageFilter as List logger.info(docToolchain &gt; exportEA: package filter list size:  + packageFilterToCreate.size()) packageFilterToCreate.each { packageFilter -&gt; scriptParameterString = scriptParameterString +  -p \${packageFilter}\ } } if (!config.exportEA.exportPath.isEmpty()) { exportPath = new File(docDir, config.exportEA.exportPath).getAbsolutePath() } else { exportPath = new File(docDir, 'src/docs').getAbsolutePath() } if (!config.exportEA.searchPath.isEmpty()) { searchPath = new File(docDir, config.exportEA.searchPath).getAbsolutePath() } else if (!config.exportEA.absoluteSearchPath.isEmpty()) { searchPath = new File(config.exportEA.absoluteSearchPath).getAbsolutePath() } else { searchPath = new File(docDir, 'src').getAbsolutePath() } scriptParameterString = scriptParameterString +  -d \$exportPath\ scriptParameterString = scriptParameterString +  -s \$searchPath\ logger.info(docToolchain &gt; exportEA: exportPath:  + exportPath) //remove old glossary files/folder if exist new File(exportPath, 'glossary').deleteDir() //set the glossary file path in case an output format is configured, other no glossary is written if (!config.exportEA.glossaryAsciiDocFormat.isEmpty()) { //create folder to store glossaries new File(exportPath, 'glossary/.').mkdirs() glossaryPath = new File(exportPath, 'glossary').getAbsolutePath() scriptParameterString = scriptParameterString +  -g \$glossaryPath\ } //configure additional diagram attributes to be exported if (!config.exportEA.diagramAttributes.isEmpty()) { scriptParameterString = scriptParameterString +  -da \$config.exportEA.diagramAttributes\ } //configure additional diagram attributes to be exported if (!config.exportEA.additionalOptions.isEmpty()) { scriptParameterString = scriptParameterString +  -ao \$config.exportEA.additionalOptions\ } //make sure path for notes exists //and remove old notes new File(exportPath, 'ea').deleteDir() //also remove old diagrams new File(exportPath, 'images/ea').deleteDir() //create a readme to clarify things new File(exportPath, 'images/ea/.').mkdirs() new File(exportPath, 'images/ea/readme.ad').write(readme) new File(exportPath, 'ea/.').mkdirs() new File(exportPath, 'ea/readme.ad').write(readme) //execute through cscript in order to make sure that we get WScript.echo right logger.info(docToolchain &gt; exportEA: parameters:  + scriptParameterString) %SystemRoot%\\System32\\cscript.exe //nologo ${projectDir}/scripts/exportEAP.vbs ${scriptParameterString}.executeCmd() //the VB Script is only capable of writing iso-8859-1-Files. //we now have to convert them to UTF-8 new File(exportPath, 'ea/.').eachFileRecurse { file -&gt; if (file.isFile()) { println exported notes  + file.canonicalPath file.write(file.getText('iso-8859-1'), 'utf-8') } } //sort, filter and reformat a glossary if an output format is configured if (!config.exportEA.glossaryAsciiDocFormat.isEmpty()) { def glossaryTypes if (!config.exportEA.glossaryTypes.isEmpty()) { glossaryTypes = config.exportEA.glossaryTypes as List } new GlossaryHandler().execute(glossaryPath, config.exportEA.glossaryAsciiDocFormat, glossaryTypes); } } } scripts/exportEAP.vbs ' based on the Project Interface Example which comes with EA ' http://stackoverflow.com/questions/1441479/automated-method-to-export-enterprise-architect-diagrams Dim EAapp 'As EA.App Dim Repository 'As EA.Repository Dim FS 'As Scripting.FileSystemObject Dim projectInterface 'As EA.Project Const ForAppending = 8 Const ForWriting = 2 ' Helper ' http://windowsitpro.com/windows/jsi-tip-10441-how-can-vbscript-create-multiple-folders-path-mkdir-command Function MakeDir (strPath) Dim strParentPath, objFSO Set objFSO = CreateObject(Scripting.FileSystemObject) On Error Resume Next strParentPath = objFSO.GetParentFolderName(strPath) If Not objFSO.FolderExists(strParentPath) Then MakeDir strParentPath If Not objFSO.FolderExists(strPath) Then objFSO.CreateFolder strPath On Error Goto 0 MakeDir = objFSO.FolderExists(strPath) End Function ' Replaces certain characters with '_' to avoid unwanted file or folder names causing errors or structure failures. ' Regular expression can easily be extended with further characters to be replaced. Function NormalizeName(theName) dim re : Set re = new regexp re.Pattern = [\\/\[\]\s] re.Global = True NormalizeName = re.Replace(theName, _) End Function Sub WriteNote(currentModel, currentElement, notes, prefix) If (Left(notes, 6) = {adoc:) Then strFileName = Mid(notes,7,InStr(notes,})-7) strNotes = Right(notes,Len(notes)-InStr(notes,})) set objFSO = CreateObject(Scripting.FileSystemObject) If (currentModel.Name=Model) Then ' When we work with the default model, we don't need a sub directory path = objFSO.BuildPath(exportDestination,ea/) Else path = objFSO.BuildPath(exportDestination,ea/&amp;NormalizeName(currentModel.Name)&amp;/) End If MakeDir(path) post =  If (prefix&lt;&gt;) Then post = _ End If MakeDir(path&amp;prefix&amp;post) set objFile = objFSO.OpenTextFile(path&amp;prefix&amp;post&amp;strFileName&amp;.ad,ForAppending, True) name = currentElement.Name name = Replace(name,vbCr,) name = Replace(name,vbLf,) if (Left(strNotes, 3) = vbCRLF&amp;|) Then ' content should be rendered as table - so don't interfere with it objFile.WriteLine(vbCRLF) else 'let's add the name of the object objFile.WriteLine(vbCRLF&amp;vbCRLF&amp;.&amp;name) End If objFile.WriteLine(vbCRLF&amp;strNotes) objFile.Close if (prefix&lt;&gt;) Then ' write the same to a second file set objFile = objFSO.OpenTextFile(path&amp;prefix&amp;.ad,ForAppending, True) objFile.WriteLine(vbCRLF&amp;vbCRLF&amp;.&amp;name&amp;vbCRLF&amp;strNotes) objFile.Close End If End If End Sub Sub SyncJira(currentModel, currentDiagram) notes = currentDiagram.notes set currentPackage = Repository.GetPackageByID(currentDiagram.PackageID) updated = 0 created = 0 If (Left(notes, 6) = {jira:) Then WScript.echo  &gt;&gt;&gt;&gt; Diagram jira tag found strSearch = Mid(notes,7,InStr(notes,})-7) Set objShell = CreateObject(WScript.Shell) 'objShell.CurrentDirectory = fso.GetFolder(./scripts) Set objExecObject = objShell.Exec (cmd /K groovy ./scripts/exportEAPJiraPrintHelper.groovy  &amp; strSearch &amp; &amp; exit) strReturn =  x = 0 y = 0 Do While Not objExecObject.StdOut.AtEndOfStream output = objExecObject.StdOut.ReadLine() ' WScript.echo output jiraElement = Split(output,|) name = jiraElement(0)&amp;:&amp;vbCR&amp;vbLF&amp;jiraElement(4) On Error Resume Next Set requirement = currentPackage.Elements.GetByName(name) On Error Goto 0 if (IsObject(requirement)) then ' element already exists requirement.notes =  requirement.notes = requirement.notes&amp;&lt;a href='&amp;jiraElement(5)&amp;'&gt;&amp;jiraElement(0)&amp;&lt;/a&gt;&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Priority: &amp;jiraElement(1)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Created: &amp;jiraElement(2)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Assignee: &amp;jiraElement(3)&amp;vbCR&amp;vbLF requirement.Update() updated = updated + 1 else Set requirement = currentPackage.Elements.AddNew(name,Requirement) requirement.notes =  requirement.notes = requirement.notes&amp;&lt;a href='&amp;jiraElement(5)&amp;'&gt;&amp;jiraElement(0)&amp;&lt;/a&gt;&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Priority: &amp;jiraElement(1)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Created: &amp;jiraElement(2)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Assignee: &amp;jiraElement(3)&amp;vbCR&amp;vbLF requirement.Update() currentPackage.Elements.Refresh() Set dia_obj = currentDiagram.DiagramObjects.AddNew(l=&amp;(10+x*200)&amp;;t=&amp;(10+y*50)&amp;;b=&amp;(10+y*50+44)&amp;;r=&amp;(10+x*200+180),) x = x + 1 if (x&gt;3) then x = 0 y = y + 1 end if dia_obj.ElementID = requirement.ElementID dia_obj.Update() created = created + 1 end if Loop Set objShell = Nothing WScript.echo created &amp;created&amp; requirements WScript.echo updated &amp;updated&amp; requirements End If End Sub ' This sub routine checks if the format string defined in diagramAttributes ' does contain any characters. It replaces the known placeholders: ' %DIAGRAM_AUTHOR%, %DIAGRAM_CREATED%, %DIAGRAM_GUID%, %DIAGRAM_MODIFIED%, ' %DIAGRAM_NAME%, %DIAGRAM_NOTES% ' with the attribute values read from the EA diagram object. ' None, one or multiple number of placeholders can be used to create a diagram attribute ' to be added to the document. The attribute string is stored as a file with the same ' path and name as the diagram image, but with suffix .ad. So, it can ' easily be included in an asciidoc file. Sub SaveDiagramAttribute(currentDiagram, path, diagramName) If Len(diagramAttributes) &gt; 0 Then filledDiagAttr = diagramAttributes set objFSO = CreateObject(Scripting.FileSystemObject) filename = objFSO.BuildPath(path, diagramName &amp; .ad) set objFile = objFSO.OpenTextFile(filename, ForWriting, True) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_AUTHOR%, currentDiagram.Author) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_CREATED%, currentDiagram.CreatedDate) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_GUID%, currentDiagram.DiagramGUID) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_MODIFIED%, currentDiagram.ModifiedDate) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_NAME%, currentDiagram.Name) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_NOTES%, currentDiagram.Notes) filledDiagAttr = Replace(filledDiagAttr, %NEWLINE%, vbCrLf) objFile.WriteLine(filledDiagAttr) objFile.Close End If End Sub Sub SaveDiagram(currentModel, currentDiagram) Dim exportDiagram ' As Boolean ' Open the diagram Repository.OpenDiagram(currentDiagram.DiagramID) ' Save and close the diagram set objFSO = CreateObject(Scripting.FileSystemObject) If (currentModel.Name=Model) Then ' When we work with the default model, we don't need a sub directory path = objFSO.BuildPath(exportDestination,/images/ea/) Else path = objFSO.BuildPath(exportDestination,/images/ea/ &amp; NormalizeName(currentModel.Name) &amp; /) End If path = objFSO.GetAbsolutePathName(path) MakeDir(path) diagramName = currentDiagram.Name diagramName = Replace(diagramName,vbCr,) diagramName = Replace(diagramName,vbLf,) diagramName = NormalizeName(diagramName) filename = objFSO.BuildPath(path, diagramName &amp; .png) exportDiagram = True If objFSO.FileExists(filename) Then WScript.echo  ---  &amp; filename &amp;  already exists. If Len(additionalOptions) &gt; 0 Then If InStr(additionalOptions, KeepFirstDiagram) &gt; 0 Then WScript.echo  --- Skipping export -- parameter 'KeepFirstDiagram' set. Else WScript.echo  --- Overwriting -- parameter 'KeepFirstDiagram' not set. exportDiagram = False End If Else WScript.echo  --- Overwriting -- parameter 'KeepFirstDiagram' not set. End If End If If exportDiagram Then projectInterface.SaveDiagramImageToFile(filename) WScript.echo  extracted image to  &amp; filename If Not IsEmpty(diagramAttributes) Then SaveDiagramAttribute currentDiagram, path, diagramName End If End If Repository.CloseDiagram(currentDiagram.DiagramID) ' Write the note of the diagram WriteNote currentModel, currentDiagram, currentDiagram.Notes, diagramName&amp;_notes For Each diagramElement In currentDiagram.DiagramObjects Set currentElement = Repository.GetElementByID(diagramElement.ElementID) WriteNote currentModel, currentElement, currentElement.Notes, diagramName&amp;_notes Next For Each diagramLink In currentDiagram.DiagramLinks set currentConnector = Repository.GetConnectorByID(diagramLink.ConnectorID) WriteNote currentModel, currentConnector, currentConnector.Notes, diagramName&amp;_links Next End Sub ' ' Recursively saves all diagrams under the provided package and its children ' Sub DumpDiagrams(thePackage,currentModel) Set currentPackage = thePackage ' export element notes For Each currentElement In currentPackage.Elements WriteNote currentModel, currentElement, currentElement.Notes,  ' export connector notes For Each currentConnector In currentElement.Connectors ' WScript.echo currentConnector.ConnectorGUID if (currentConnector.ClientID=currentElement.ElementID) Then WriteNote currentModel, currentConnector, currentConnector.Notes,  End If Next if (Not currentElement.CompositeDiagram Is Nothing) Then SyncJira currentModel, currentElement.CompositeDiagram SaveDiagram currentModel, currentElement.CompositeDiagram End If if (Not currentElement.Elements Is Nothing) Then DumpDiagrams currentElement,currentModel End If Next ' Iterate through all diagrams in the current package For Each currentDiagram In currentPackage.Diagrams SyncJira currentModel, currentDiagram SaveDiagram currentModel, currentDiagram Next ' Process child packages Dim childPackage 'as EA.Package ' otPackage = 5 if (currentPackage.ObjectType = 5) Then For Each childPackage In currentPackage.Packages call DumpDiagrams(childPackage, currentModel) Next End If End Sub Function SearchEAProjects(path) For Each folder In path.SubFolders SearchEAProjects folder Next For Each file In path.Files If fso.GetExtensionName (file.Path) = eap OR fso.GetExtensionName (file.Path) = eapx Then WScript.echo found &amp;file.path If (Left(file.name, 1) = _) Then WScript.echo skipping, because it start with `_` (replication) Else OpenProject(file.Path) End If End If Next End Function 'Gets the package object as referenced by its GUID from the Enterprise Architect project. 'Looks for the model node, the package is a child of as it is required for the diagram export. 'Calls the Sub routine DumpDiagrams for the model and package found. 'An error is printed to console only if the packageGUID is not found in the project. Function DumpPackageDiagrams(EAapp, packageGUID) WScript.echo DumpPackageDiagrams WScript.echo packageGUID Dim package Set package = EAapp.Repository.GetPackageByGuid(packageGUID) If (package Is Nothing) Then WScript.echo invalid package - as package is not part of the project Else Dim currentModel Set currentModel = package while currentModel.IsModel = false Set currentModel = EAapp.Repository.GetPackageByID(currentModel.parentID) wend ' Iterate through all child packages and save out their diagrams ' save all diagrams of package itself call DumpDiagrams(package, currentModel) End If End Function Function FormatStringToJSONString(inputString) outputString = Replace(inputString, \, \\) outputString = Replace(outputString, , \) outputString = Replace(outputString, vbCrLf, \n) outputString = Replace(outputString, vbLf, \n) outputString = Replace(outputString, vbCr, \n) FormatStringToJSONString = outputString End Function 'If a valid file path is set, the glossary terms are read from EA repository, 'formatted in a JSON compatible format and written into file. 'The file is read and reformatted by the exportEA gradle task afterwards. Function ExportGlossaryTermsAsJSONFile(EArepo) If (Len(glossaryFilePath) &gt; 0) Then set objFSO = CreateObject(Scripting.FileSystemObject) GUID = Replace(EArepo.ProjectGUID,{,) GUID = Replace(GUID,},) currentGlossaryFile = objFSO.BuildPath(glossaryFilePath,/&amp;GUID&amp;.ad) set objFile = objFSO.OpenTextFile(currentGlossaryFile,ForAppending, True) Set glossary = EArepo.Terms() objFile.WriteLine([) dim counter counter = 0 For Each term In glossary if (counter &gt; 0) Then objFile.Write(,) end if objFile.Write({ term : &amp;FormatStringToJSONString(term.term)&amp;, meaning : &amp;FormatStringToJSONString(term.Meaning)&amp;,) objFile.WriteLine( termID : &amp;FormatStringToJSONString(term.termID)&amp;, type : &amp;FormatStringToJSONString(term.type)&amp; }) counter = counter + 1 Next objFile.WriteLine(]) objFile.Close End If End Function Sub OpenProject(file) ' open Enterprise Architect Set EAapp = CreateObject(EA.App) WScript.echo opening Enterprise Architect. This might take a moment... ' load project EAapp.Repository.OpenFile(file) ' make Enterprise Architect to not appear on screen EAapp.Visible = False ' get repository object Set Repository = EAapp.Repository ' Show the script output window ' Repository.EnsureOutputVisible(Script) call ExportGlossaryTermsAsJSONFile(Repository) Set projectInterface = Repository.GetProjectInterface() Dim childPackage 'As EA.Package ' Iterate through all model nodes Dim currentModel 'As EA.Package If (InStrRev(file,{) &gt; 0) Then ' the filename references a GUID ' like {04C44F80-8DA1-4a6f-ECB8-982349872349} WScript.echo file GUID = Mid(file, InStrRev(file,{)+0,38) WScript.echo GUID ' Iterate through all child packages and save out their diagrams call DumpPackageDiagrams(EAapp, GUID) Else If packageFilter.Count = 0 Then WScript.echo done ' Iterate through all model nodes For Each currentModel In Repository.Models ' Iterate through all child packages and save out their diagrams For Each childPackage In currentModel.Packages call DumpDiagrams(childPackage,currentModel) Next Next Else ' Iterate through all packages found in the package filter given by script parameter. For Each packageGUID In packageFilter call DumpPackageDiagrams(EAapp, packageGUID) Next End If End If EAapp.Repository.CloseFile() ' Since EA 15.2 the Enterprise Architect background process hangs without calling Exit explicitly EAapp.Repository.Exit() End Sub Private connectionString Private packageFilter Private exportDestination Private searchPath Private glossaryFilePath Private diagramAttributes Private additionalOptions exportDestination = ./src/docs searchPath = ./src Set packageFilter = CreateObject(System.Collections.ArrayList) Set objArguments = WScript.Arguments Dim argCount argCount = 0 While objArguments.Count &gt; argCount+1 Select Case objArguments(argCount) Case -c connectionString = objArguments(argCount+1) Case -p packageFilter.Add objArguments(argCount+1) Case -d exportDestination = objArguments(argCount+1) Case -s searchPath = objArguments(argCount+1) Case -g glossaryFilePath = objArguments(argCount+1) Case -da diagramAttributes = objArguments(argCount+1) Case -ao additionalOptions = objArguments(argCount+1) End Select argCount = argCount + 2 WEnd set fso = CreateObject(Scripting.fileSystemObject) WScript.echo Image extractor ' Check both types in parallel - 1st check Enterprise Architect database connection, 2nd look for local project files If Not IsEmpty(connectionString) Then WScript.echo opening database connection now OpenProject(connectionString) End If WScript.echo looking for .eap(x) files in  &amp; fso.GetAbsolutePathName(searchPath) ' Dim f As Scripting.Files SearchEAProjects fso.GetFolder(searchPath) WScript.echo finished exporting images "
},

{
    "id": 8,
    "uri": "015_tasks/03_task_publishToConfluence.html",
    "menu": "tasks",
    "title": "publishToConfluence",
    "text": " Table of Contents publishToConfluence Configuration CSS Styling Source .gravatar img { margin-left: 3px; border-radius: 4px; } publishToConfluence 6 minutes to read This target takes the generated HTML file, splits it by headline and pushes it to Confluence. This enables you to use the docs-as-code approach while getting feedback from non-techies through Confluence comments. And it fulfills the requirement of &#8230;&#8203; but all documentation is in Confluence. Special features: [source] -blocks are converted to code-macro blocks in confluence. only pages and images which have changed between task runs are really published and hence only for those changes notifications are sent to watchers. This is quite important - otherwise watchers are easily annoyed by too many notifications. :keywords: Keywords are attached as labels to every generated Confluence page. The rules for page labels should be kept in mind. See https://confluence.atlassian.com/doc/add-remove-and-search-for-labels-136419.html . Several keywords are allowed. They must be separated by comma, e.g. :keywords: label_1, label-2, label3, &#8230;&#8203; . Labels (keywords) must not contain a space character! Use '_' or '-'. Note code-macro-blocks in confluence render an error if the language attribute contains an unknown language. See https://confluence.atlassian.com/doc/code-block-macro-139390.html for a list of valid language and how to add further languages. Configuration We tried to make the configuration self-explaining, but there are always some note to add. ancestorId this is the page ID of the parent page to which you want your docs to be published. Go to this page, click on edit and the needed ID will show up in the URL. Specify the ID as string within the config file. api for cloud instances, [context] is wiki preambleTitle the title of the page containing the preamble (everything the first second level heading). Default is 'arc42' disableToC This boolean configuration define if the Table of Content (ToC) is disabled from the page once uploaded in confluence. (it is false by default, so the ToC is active) pagePrefix/pageSuffix Confluence can&#8217;t handle two pages with the same name. Moreover, the script matches pages regardless of the case. It will thus refuse to replace a page whose title only differs in case with an existing page. So you should create a new confluence space for each piece of larger documentation. If you are restricted and can&#8217;t create new spaces, you can use this pagePrefix / pageSuffix to define a prefix/suffix for this doc so that it doesn&#8217;t conflict with other page names. credentials Use username and password or even better username and api-token. You can create new API-tokens in your profile . To avoid having your password or api-token versioned through git, you can store it outside of this configuration as environment variable or in another file - the key here is that the config file is a groovy script. e.g. you can do things like credentials = user:${new File(/home/me/apitoken).text}.bytes.encodeBase64().toString() To simplify the injection of credentials from external sources there is a fallback. Should you leave the credentials field empty, the variables confluenceUser and confluencePass from the build environment will be used for authentication. You can set these through any means allowed by gradle like the gradle.properties file in the project or your home directory, environment variables or command-line flags. For all ways to set these variables, have a look at the gradle manual . apikey In cases where you have to use full user authorization because of internal confluence permission handling, you need to add the API-token in addition to the credentials. The API-token cannot be added to the credentials as it is used for user and password exchange. Therefore the API-token can be added as parameter apikey, which makes the addition of the token as a separate header field with key: keyId and value of apikey . Example including storing of the real value outside this configuration: apikey = ${new File(/home/me/apitoken).text} . extraPageContent If you need to prefix your pages with a warning that this is generated content - this is the right place. enableAttachments If value is set to true , your links to local file references will be uploaded as attachments. The current implementation only supports a single folder. This foldername will be used as a prefix to validate if your file should be uploaded or not. In case you enable this feature, and use a folder which starts with attachment*, an adaption of this prefix is required. jiraServerId the jira server id your confluence instance is connected to. If value is set, all anchors pointing to a jira ticket will be replaced by the confluence jira macro. To function properly jiraRoot has to be configured (see exportJiraIssues ). Example: All files to attach will require to be linked inside the document. link:attachement/myfolder/myfile.json[My API definition] attachmentPrefix The expected foldername of your output dir. Default : attachment proxy If you need to provide a proxy to access Confluence, you may set a map with keys host (e.g. 'my.proxy.com' ), port (e.g. '1234' ) and schema (e.g. 'http' ) of your proxy. useOpenapiMacro If this option is present and equal to confluence-open-api then any source block marked with class openapi will be wrapped in Elitesoft Swagger Editor macro: (see Elitesoft Swagger Editor ) For backward compatibility: If this option is present and equal to true , then again the Elitesoft Swagger Editor macro will be used. If this option is present and equal to open-api then any source block marked with class openapi will be wrapped in Open API Documentation for Confluence macro: (see Open API Documentation for Confluence ). A download source (yaml) button is shown by default. This is how you&#8217;d include your openapi YAML file: [source.openapi,yaml] ---- include::myopeanapi.yaml[] ---- publishToConfluence.gradle //Configureation for publishToConfluence confluence = [:] // 'input' is an array of files to upload to Confluence with the ability // to configure a different parent page for each file. // // Attributes // - 'file': absolute or relative path to the asciidoc generated html file to be exported // - 'url': absolute URL to an asciidoc generated html file to be exported // - 'ancestorName' (optional): the name of the parent page in Confluence as string; // this attribute has priority over ancestorId, but if page with given name doesn't exist, // ancestorId will be used as a fallback // - 'ancestorId' (optional): the id of the parent page in Confluence as string; leave this empty // if a new parent shall be created in the space // - 'preambleTitle' (optional): the title of the page containing the preamble (everything // before the first second level heading). Default is 'arc42' // // The following four keys can also be used in the global section below // - 'spaceKey' (optional): page specific variable for the key of the confluence space to write to // - 'createSubpages' (optional): page specific variable to determine whether .sect2 sections shall be split from the current page into subpages // - 'pagePrefix' (optional): page specific variable, the pagePrefix will be a prefix for the page title and it's sub-pages // use this if you only have access to one confluence space but need to store several // pages with the same title - a different pagePrefix will make them unique // - 'pageSuffix' (optional): same usage as prefix but appended to the title and it's subpages // only 'file' or 'url' is allowed. If both are given, 'url' is ignored confluence.with { input = [ [ file: build/docs/html5/arc42-template-de.html ], ] // endpoint of the confluenceAPI (REST) to be used // verfiy that you got the correct endpoint by browsing to // https://[yourServer]/[context]/rest/api/user/current // you should get a valid json which describes your current user // a working example is https://arc42-template.atlassian.net/wiki/rest/api/user/current api = 'https://[yourServer]/[context]/rest/api/' // Additionally, spaceKey, createSubpages, pagePrefix and pageSuffix can be globally defined here. The assignment in the input array has precedence // the key of the confluence space to write to spaceKey = 'asciidoc' // the title of the page containing the preamble (everything the first second level heading). Default is 'arc42' preambleTitle = '' // variable to determine whether .sect2 sections shall be split from the current page into subpages createSubpages = false // the pagePrefix will be a prefix for each page title // use this if you only have access to one confluence space but need to store several // pages with the same title - a different pagePrefix will make them unique pagePrefix = '' pageSuffix = '' /* WARNING: It is strongly recommended to store credentials securely instead of commiting plain text values to your git repository!!! Tool expects credentials that belong to an account which has the right permissions to to create and edit confluence pages in the given space. Credentials can be used in a form of: - passed parameters when calling script (-PconfluenceUser=myUsername -PconfluencePass=myPassword) which can be fetched as a secrets on CI/CD or - gradle variables set through gradle properties (uses the 'confluenceUser' and 'confluencePass' keys) Often, same credentials are used for Jira &amp; Confluence, in which case it is recommended to pass CLI parameters for both entities as -Pusername=myUser -Ppassword=myPassword */ //optional API-token to be added in case the credentials are needed for user and password exchange. //apikey = [API-token] // HTML Content that will be included with every page published // directly after the TOC. If left empty no additional content will be // added // extraPageContent = '&lt;ac:structured-macro ac:name=warning&gt;&lt;ac:parameter ac:name=title /&gt;&lt;ac:rich-text-body&gt;This is a generated page, do not edit!&lt;/ac:rich-text-body&gt;&lt;/ac:structured-macro&gt; extraPageContent = '' // enable or disable attachment uploads for local file references enableAttachments = false // default attachmentPrefix = attachment - All files to attach will require to be linked inside the document. // attachmentPrefix = attachment // Optional proxy configuration, only used to access Confluence // schema supports http and https // proxy = [host: 'my.proxy.com', port: 1234, schema: 'http'] // Optional: specify which Confluence OpenAPI Macro should be used to render OpenAPI definitions // possible values: [confluence-open-api, open-api, true]. true is the same as confluence-open-api for backward compatibility // useOpenapiMacro = confluence-open-api } CSS Styling Some AsciiDoctor features depend on particular CSS style definitions. Unless these styles are defined, some formatting that is present in the HTML version will not be represented when published to Confluence. To configure Confluence to include additional style definitions: Log in to Confluence as a space admin Go to the desired space Select Space tools, Look and Feel, Stylesheet Click Edit, enter the desired style definitions, and click Save The default style definitions can be found in the AsciiDoc project as asciidoctor-default.css . Note that you likely do NOT want to include the whole thing, as some of the definitions are likely to disrupt Confluence&#8217;s layout. The following style definitions are believed to be Confluence-compatible, and enable use of the built-in roles ( big / small , underline / overline / line-through , COLOR / COLOR -background for the sixteen HTML color names ): .big{font-size:larger} .small{font-size:smaller} .underline{text-decoration:underline} .overline{text-decoration:overline} .line-through{text-decoration:line-through} .aqua{color:#00bfbf} .aqua-background{background-color:#00fafa} .black{color:#000} .black-background{background-color:#000} .blue{color:#0000bf} .blue-background{background-color:#0000fa} .fuchsia{color:#bf00bf} .fuchsia-background{background-color:#fa00fa} .gray{color:#606060} .gray-background{background-color:#7d7d7d} .green{color:#006000} .green-background{background-color:#007d00} .lime{color:#00bf00} .lime-background{background-color:#00fa00} .maroon{color:#600000} .maroon-background{background-color:#7d0000} .navy{color:#000060} .navy-background{background-color:#00007d} .olive{color:#606000} .olive-background{background-color:#7d7d00} .purple{color:#600060} .purple-background{background-color:#7d007d} .red{color:#bf0000} .red-background{background-color:#fa0000} .silver{color:#909090} .silver-background{background-color:#bcbcbc} .teal{color:#006060} .teal-background{background-color:#007d7d} .white{color:#bfbfbf} .white-background{background-color:#fafafa} .yellow{color:#bfbf00} .yellow-background{background-color:#fafa00} Source publishToConfluence.gradle task publishToConfluence( description: 'publishes the HTML rendered output to confluence', group: 'docToolchain' ) { doLast { logger.info(docToolchain&gt; docDir: +docDir) binding.setProperty('config',config) binding.setProperty('docDir',docDir) evaluate(new File(projectDir, 'scripts/asciidoc2confluence.groovy')) } } scripts/asciidoc2confluence.groovy /** * Created by Ralf D. Mueller and Alexander Heusingfeld * https://github.com/rdmueller/asciidoc2confluence * * this script expects an HTML document created with AsciiDoctor * in the following style (default AsciiDoctor output) * &lt;div class=sect1&gt; * &lt;h2&gt;Page Title&lt;/h2&gt; * &lt;div class=sectionbody&gt; * &lt;div class=sect2&gt; * &lt;h3&gt;Sub-Page Title&lt;/h3&gt; * &lt;/div&gt; * &lt;div class=sect2&gt; * &lt;h3&gt;Sub-Page Title&lt;/h3&gt; * &lt;/div&gt; * &lt;/div&gt; * &lt;/div&gt; * &lt;div class=sect1&gt; * &lt;h2&gt;Page Title&lt;/h2&gt; * ... * &lt;/div&gt; * */ /* Additions for issue #342 marked as #342-dierk42 ;-) */ // some dependencies /** @Grapes( [@Grab('org.jsoup:jsoup:1.8.2'), @Grab('org.codehaus.groovy.modules.http-builder:http-builder:0.6' ), @Grab('org.apache.httpcomponents:httpmime:4.5.1')] ) **/ import org.jsoup.Jsoup import org.jsoup.parser.Parser import org.jsoup.nodes.Entities.EscapeMode import org.jsoup.nodes.Document import org.jsoup.nodes.Document.OutputSettings import org.jsoup.nodes.Element import org.jsoup.nodes.TextNode import org.jsoup.select.Elements import groovyx.net.http.RESTClient import groovyx.net.http.HttpResponseException import groovyx.net.http.HTTPBuilder import groovyx.net.http.EncoderRegistry import groovyx.net.http.ContentType import java.security.MessageDigest //to upload attachments: import org.apache.http.entity.mime.MultipartEntity import org.apache.http.entity.mime.content.StringBody import org.apache.http.entity.mime.content.InputStreamBody import org.apache.http.entity.mime.HttpMultipartMode import groovyx.net.http.Method def CDATA_PLACEHOLDER_START = '&lt;cdata-placeholder&gt;' def CDATA_PLACEHOLDER_END = '&lt;/cdata-placeholder&gt;' def baseUrl def allPages // configuration def confluenceSpaceKey def confluenceCreateSubpages def confluencePagePrefix def baseApiPath = new URI(config.confluence.api).path // helper functions def MD5(String s) { MessageDigest.getInstance(MD5).digest(s.bytes).encodeHex().toString() } // for getting better error message from the REST-API // LuisMuniz: return the action's result, if successful. def trythis(Closure action) { try { action.call() } catch (HttpResponseException error) { println something went wrong - got an http response code +error.response.status+: switch (error.response.status) { case '401': println (error.response.data.toString().replaceAll(^.*Reason,Reason)) println please check your confluence credentials in config file or passed parameters throw new Exception(missing authentication credentials) break case '400': println error.response.data.message println please check the ancestorId in your config file throw new Exception(Parent does not exist) break default: println error.response.data } null } } def parseAdmonitionBlock(block, String type) { content = block.select(.content).first() titleElement = content.select(.title) titleText = '' if(titleElement != null) { titleText = &lt;ac:parameter ac:name=\title\&gt;${titleElement.text()}&lt;/ac:parameter&gt; titleElement.remove() } block.after(&lt;ac:structured-macro ac:name=\${type}\&gt;${titleText}&lt;ac:rich-text-body&gt;${content}&lt;/ac:rich-text-body&gt;&lt;/ac:structured-macro&gt;) block.remove() } /* #342-dierk42 add labels to a Confluence page. Labels are taken from :keywords: which are converted as meta tags in HTML. Building the array: see below Confluence allows adding labels only after creation of a page. Therefore we need extra API calls. Currently the labels are added one by one. Suggestion for improvement: Build a label structure of all labels an place them with one call. Replaces exisiting labels. No harm Does not check for deleted labels when keywords are deleted from source document! */ def addLabels = { def pageId, def labelsArray -&gt; //https://docs.atlassian.com/confluence/REST/latest/ def api = new RESTClient(config.confluence.api) //this fixes the encoding (dierk42: Is this needed here? Don't know) api.encoderRegistry = new EncoderRegistry( charset: 'utf-8' ) def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'X-Atlassian-Token':'no-check' ] // Attach each label in a API call of its own. The only prefix possible // in our own Confluence is 'global' labelsArray.each { label -&gt; label_data = [ prefix : 'global', name : label ] trythis { // attach label to page pageId // https://developer.atlassian.com/display/CONFDEV/Confluence+REST+API+Examples#ConfluenceRESTAPIExamples-Updatingapage def res = api.post(contentType: ContentType.JSON, path: 'content/' + pageId + /label, body: label_data, headers: headers) } println added label  + label +  to page ID  + pageId } } def uploadAttachment = { def pageId, String url, String fileName, String note -&gt; def is def localHash if (url.startsWith('http')) { is = new URL(url).openStream() //build a hash of the attachment localHash = MD5(new URL(url).openStream().text) } else { is = new File(url).newDataInputStream() //build a hash of the attachment localHash = MD5(new File(url).newDataInputStream().text) } //https://docs.atlassian.com/confluence/REST/latest/ def api = new RESTClient(config.confluence.api) //this fixes the encoding api.encoderRegistry = new EncoderRegistry( charset: 'utf-8' ) if (config.confluence.proxy) { api.setProxy(config.confluence.proxy.host, config.confluence.proxy.port, config.confluence.proxy.schema ?: 'http') } def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'X-Atlassian-Token':'no-check' ] //Add api key and value to REST API request header if configured - required for authentification. if (config.confluence.apikey) { headers.keyid = config.confluence.apikey } //check if attachment already exists def result = nothing def attachment = api.get(path: 'content/' + pageId + '/child/attachment', query: [ 'filename': fileName, ], headers: headers).data def http if (attachment.size==1) { // attachment exists. need an update? def remoteHash = 0 if (attachment.results[0].extensions.comment != null) { remoteHash = attachment.results[0].extensions.comment.replaceAll((?sm).*#([^#]+)#.*,'$1') } if (remoteHash!=localHash) { //hash is different -&gt; attachment needs to be updated http = new HTTPBuilder(config.confluence.api + 'content/' + pageId + '/child/attachment/' + attachment.results[0].id + '/data') println  updated attachment } } else { http = new HTTPBuilder(config.confluence.api + 'content/' + pageId + '/child/attachment') } if (http) { if (config.confluence.proxy) { http.setProxy(config.confluence.proxy.host, config.confluence.proxy.port, config.confluence.proxy.schema ?: 'http') } http.request(Method.POST) { req -&gt; requestContentType: multipart/form-data MultipartEntity multiPartContent = new MultipartEntity(HttpMultipartMode.BROWSER_COMPATIBLE) // Adding Multi-part file parameter file multiPartContent.addPart(file, new InputStreamBody(is, fileName)) // Adding another string parameter comment multiPartContent.addPart(comment, new StringBody(note + \r\n# + localHash + #)) req.setEntity(multiPartContent) headers.each { key, value -&gt; req.addHeader(key, value) } } } } def realTitle = { pageTitle -&gt; confluencePagePrefix + pageTitle + confluencePageSuffix } def rewriteMarks = { body -&gt; // Confluence strips out mark elements. Replace them with default formatting. body.select('mark').wrap('&lt;span style=background:#ff0;color:#000&gt;&lt;/style&gt;').unwrap() } // #352-LuisMuniz: Helper methods // Fetch all pages of the space. Only keep relevant info in the pages Map // The map is indexed by lower-case title def retrieveAllPages = { RESTClient api, Map headers, String spaceKey -&gt; if (allPages != null) { println allPages already retrieved allPages } else { boolean morePages=true int start=0 def request = [ 'type' : 'page', 'spaceKey': spaceKey, 'expand' : 'ancestors', 'limit' : 100 ] allPages =[:] while(morePages) { def results = trythis { request.start=start def args = [ 'headers': headers, 'path' : ${baseApiPath}content, 'query' : request, ] api.get(args).data.results } ?: [] if (results.empty) { morePages=false } else { start += results.size } results.inject(allPages) { Map acc, Map match -&gt; //unique page names in confluence, so we can get away with indexing by title def ancestors = match.ancestors.collect { it.id } acc[match.title.toLowerCase()] = [ title : match.title, id : match.id, parentId: ancestors.isEmpty() ? null : ancestors.last() ] acc } } allPages } } // Retrieve a page by id with contents and version def retrieveFullPage = { RESTClient api, Map headers, String id -&gt; trythis { api.get( [ 'headers': headers, 'path' : ${baseApiPath}content/${id}, 'query' : ['expand': 'body.storage,version'], ] ).data } ?: [:] } //if a parent has been specified, check whether a page has the same parent. boolean hasRequestedParent(Map existingPage, String requestedParentId) { if (requestedParentId) { existingPage.parentId == requestedParentId } else { true } } def rewriteDescriptionLists = { body -&gt; def TAGS = [ dt: 'th', dd: 'td' ] body.select('dl').each { dl -&gt; // WHATWG allows wrapping dt/dd in divs, simply unwrap them dl.select('div').each { it.unwrap() } // group dts and dds that belong together, usually it will be a 1:1 relation // but HTML allows for different constellations def rows = [] def current = [dt: [], dd: []] rows &lt;&lt; current dl.select('dt, dd').each { child -&gt; def tagName = child.tagName() if (tagName == 'dt' &amp;&amp; current.dd.size() &gt; 0) { // dt follows dd, start a new group current = [dt: [], dd: []] rows &lt;&lt; current } current[tagName] &lt;&lt; child.tagName(TAGS[tagName]) child.remove() } rows.each { row -&gt; def sizes = [dt: row.dt.size(), dd: row.dd.size()] def rowspanIdx = [dt: -1, dd: sizes.dd - 1] def rowspan = Math.abs(sizes.dt - sizes.dd) + 1 def max = sizes.dt if (sizes.dt &lt; sizes.dd) { max = sizes.dd rowspanIdx = [dt: sizes.dt - 1, dd: -1] } (0..&lt;max).each { idx -&gt; def tr = dl.appendElement('tr') ['dt', 'dd'].each { type -&gt; if (sizes[type] &gt; idx) { tr.appendChild(row[type][idx]) if (idx == rowspanIdx[type] &amp;&amp; rowspan &gt; 1) { row[type][idx].attr('rowspan', ${rowspan}) } } else if (idx == 0) { tr.appendElement(TAGS[type]).attr('rowspan', ${rowspan}) } } } } dl.wrap('&lt;table&gt;&lt;/table&gt;') .unwrap() } } def rewriteInternalLinks = { body, anchors, pageAnchors -&gt; // find internal cross-references and replace them with link macros body.select('a[href]').each { a -&gt; def href = a.attr('href') if (href.startsWith('#')) { def anchor = href.substring(1) def pageTitle = anchors[anchor] ?: pageAnchors[anchor] if (pageTitle &amp;&amp; a.text()) { // as Confluence insists on link texts to be contained // inside CDATA, we have to strip all HTML and // potentially loose styling that way. a.html(a.text()) a.wrap(&lt;ac:link${anchors.containsKey(anchor) ? ' ac:anchor=' + anchor + '' : ''}&gt;&lt;/ac:link&gt;) .before(&lt;ri:page ri:content-title=\${realTitle pageTitle}\/&gt;) .wrap(&lt;ac:plain-text-link-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-link-body&gt;) .unwrap() } } } } def rewriteJiraLinks = { body -&gt; // find links to jira tickets and replace them with jira macros body.select('a[href]').each { a -&gt; def href = a.attr('href') if (href.startsWith(config.jira.api + /browse/)) { def ticketId = a.text() a.before(&lt;ac:structured-macro ac:name=\jira\ ac:schema-version=\1\&gt; &lt;ac:parameter ac:name=\key\&gt;${ticketId}&lt;/ac:parameter&gt; &lt;ac:parameter ac:name=\serverId\&gt;${config.confluence.jiraServerId}&lt;/ac:parameter&gt; &lt;/ac:structured-macro&gt;) a.remove() } } } def rewriteCodeblocks = { body -&gt; body.select('pre &gt; code').each { code -&gt; if (code.attr('data-lang')) { code.select('span[class]').each { span -&gt; span.unwrap() } code.select('i[class]').each { i -&gt; i.unwrap() } code.select('b').each { b -&gt; b.before( // ) b.unwrap() } code.before(&lt;ac:parameter ac:name=\language\&gt;${code.attr('data-lang')}&lt;/ac:parameter&gt;) } code.parent() // pre now .wrap('&lt;ac:structured-macro ac:name=code&gt;&lt;/ac:structured-macro&gt;') .unwrap() code.wrap(&lt;ac:plain-text-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-body&gt;) .unwrap() } } def rewriteOpenAPI = { org.jsoup.nodes.Element body -&gt; if (config.confluence.useOpenapiMacro == true || config.confluence.useOpenapiMacro == 'confluence-open-api') { body.select('div.openapi pre &gt; code').each { code -&gt; def parent=code.parent() def rawYaml=code.wholeText() code.parent() .wrap('&lt;ac:structured-macro ac:name=confluence-open-api ac:schema-version=1 ac:macro-id=1dfde21b-6111-4535-928a-470fa8ae3e7d&gt;&lt;/ac:structured-macro&gt;') .unwrap() code.wrap(&lt;ac:plain-text-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-body&gt;) .replaceWith(new TextNode(rawYaml)) } } else if (config.confluence.useOpenapiMacro == 'open-api') { body.select('div.openapi pre &gt; code').each { code -&gt; def parent=code.parent() def rawYaml=code.wholeText() code.parent() .wrap('&lt;ac:structured-macro ac:name=open-api ac:schema-version=1 data-layout=default ac:macro-id=4302c9d8-fca4-4f14-99a9-9885128870fa&gt;&lt;/ac:structured-macro&gt;') .unwrap() // default: show download button code.before('&lt;ac:parameter ac:name=showDownloadButton&gt;true&lt;/ac:parameter&gt;') code.wrap(&lt;ac:plain-text-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-body&gt;) .replaceWith(new TextNode(rawYaml)) } } } def unescapeCDATASections = { html -&gt; def start = html.indexOf(CDATA_PLACEHOLDER_START) while (start &gt; -1) { def end = html.indexOf(CDATA_PLACEHOLDER_END, start) if (end &gt; -1) { def prefix = html.substring(0, start) + CDATA_PLACEHOLDER_START def suffix = html.substring(end) def unescaped = html.substring(start + CDATA_PLACEHOLDER_START.length(), end) .replaceAll('&amp;lt;', '&lt;').replaceAll('&amp;gt;', '&gt;') .replaceAll('&amp;amp;', '&amp;') html = prefix + unescaped + suffix } start = html.indexOf(CDATA_PLACEHOLDER_START, start + 1) } html } //modify local page in order to match the internal confluence storage representation a bit better //definition lists are not displayed by confluence, so turn them into tables //body can be of type Element or Elements def deferredUpload = [] def parseBody = { body, anchors, pageAnchors -&gt; rewriteOpenAPI body body.select('div.paragraph').unwrap() body.select('div.ulist').unwrap() body.select('div.sect3').unwrap() [ 'note':'info', 'warning':'warning', 'important':'warning', 'caution':'note', 'tip':'tip' ].each { adType, cType -&gt; body.select('.admonitionblock.'+adType).each { block -&gt; parseAdmonitionBlock(block, cType) } } //special for the arc42-template body.select('div.arc42help').select('.content') .wrap('&lt;ac:structured-macro ac:name=expand&gt;&lt;/ac:structured-macro&gt;') .wrap('&lt;ac:rich-text-body&gt;&lt;/ac:rich-text-body&gt;') .wrap('&lt;ac:structured-macro ac:name=info&gt;&lt;/ac:structured-macro&gt;') .before('&lt;ac:parameter ac:name=title&gt;arc42&lt;/ac:parameter&gt;') .wrap('&lt;ac:rich-text-body&gt;&lt;p&gt;&lt;/p&gt;&lt;/ac:rich-text-body&gt;') body.select('div.arc42help').unwrap() body.select('div.title').wrap(&lt;strong&gt;&lt;/strong&gt;).before(&lt;br /&gt;).wrap(&lt;div&gt;&lt;/div&gt;) body.select('div.listingblock').wrap(&lt;p&gt;&lt;/p&gt;).unwrap() // see if we can find referenced images and fetch them new File(tmp/images/.).mkdirs() // find images, extract their URLs for later uploading (after we know the pageId) and replace them with this macro: // &lt;ac:image ac:align=center ac:width=500&gt; // &lt;ri:attachment ri:filename=deployment-context.png/&gt; // &lt;/ac:image&gt; body.select('img').each { img -&gt; img.attributes().each { attribute -&gt; //println attribute.dump() } def src = img.attr('src') def imgWidth = img.attr('width')?:500 def imgAlign = img.attr('align')?:center println  image: +src //it is not an online image, so upload it to confluence and use the ri:attachment tag if(!src.startsWith(http)) { def newUrl = baseUrl.toString().replaceAll('\\\\','/').replaceAll('/[^/]*$','/')+src def fileName = java.net.URLDecoder.decode((src.tokenize('/')[-1]),UTF-8) newUrl = java.net.URLDecoder.decode(newUrl,UTF-8) trythis { deferredUpload &lt;&lt; [0,newUrl,fileName,automatically uploaded] } img.after(&lt;ac:image ac:align=\${imgAlign}\ ac:width=\${imgWidth}\&gt;&lt;ri:attachment ri:filename=\${fileName}\/&gt;&lt;/ac:image&gt;) } // it is an online image, so we have to use the ri:url tag else { img.after(&lt;ac:image ac:align=\imgAlign\ ac:width=\${imgWidth}\&gt;&lt;ri:url ri:value=\${src}\/&gt;&lt;/ac:image&gt;) } img.remove() } if(config.confluence.enableAttachments){ attachmentPrefix = config.confluence.attachmentPrefix ? config.confluence.attachmentPrefix : 'attachment' body.select('a').each { link -&gt; def src = link.attr('href') println  attachment src: +src //upload it to confluence and use the ri:attachment tag if(src.startsWith(attachmentPrefix)) { def newUrl = baseUrl.toString().replaceAll('\\\\','/').replaceAll('/[^/]*$','/')+src def fileName = java.net.URLDecoder.decode((src.tokenize('/')[-1]),UTF-8) newUrl = java.net.URLDecoder.decode(newUrl,UTF-8) trythis { deferredUpload &lt;&lt; [0,newUrl,fileName,automatically uploaded non-image attachment by docToolchain] } def uriArray=fileName.split(/) def pureFilename = uriArray[uriArray.length-1] def innerhtml = link.html() link.after(&lt;ac:structured-macro ac:name=\view-file\ ac:schema-version=\1\&gt;&lt;ac:parameter ac:name=\name\&gt;&lt;ri:attachment ri:filename=\${pureFilename}\/&gt;&lt;/ac:parameter&gt;&lt;/ac:structured-macro&gt;) link.after(&lt;ac:link&gt;&lt;ri:attachment ri:filename=\${pureFilename}\/&gt;&lt;ac:plain-text-link-body&gt; &lt;![CDATA[\${innerhtml}\]]&gt;&lt;/ac:plain-text-link-body&gt;&lt;/ac:link&gt;) link.remove() } } } if(config.confluence.jiraServerId){ rewriteJiraLinks body } rewriteMarks body rewriteDescriptionLists body rewriteInternalLinks body, anchors, pageAnchors //sanitize code inside code tags rewriteCodeblocks body def pageString = unescapeCDATASections body.html().trim() //change some html elements through simple substitutions pageString = pageString .replaceAll('&lt;br&gt;','&lt;br /&gt;') .replaceAll('&lt;/br&gt;','&lt;br /&gt;') .replaceAll('&lt;a([^&gt;]*)&gt;&lt;/a&gt;','') .replaceAll(CDATA_PLACEHOLDER_START,'&lt;![CDATA[') .replaceAll(CDATA_PLACEHOLDER_END,']]&gt;') return pageString } // the create-or-update functionality for confluence pages // #342-dierk42: added parameter 'keywords' def pushToConfluence = { pageTitle, pageBody, String parentId, anchors, pageAnchors, keywords -&gt; def api = new RESTClient(config.confluence.api) def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'Content-Type':'application/json; charset=utf-8' ] //Add api key and value to REST API request header if configured - required for authentification. if (config.confluence.apikey) { headers.keyid = config.confluence.apikey } String realTitleLC = realTitle(pageTitle).toLowerCase() //this fixes the encoding api.encoderRegistry = new EncoderRegistry( charset: 'utf-8' ) if (config.confluence.proxy) { api.setProxy(config.confluence.proxy.host, config.confluence.proxy.port, config.confluence.proxy.schema ?: 'http') } //try to get an existing page localPage = parseBody(pageBody, anchors, pageAnchors) def localHash = MD5(localPage) if(config.confluence.disableToC){ def prefix = (config.confluence.extraPageContent?:'') localPage = prefix+localPage localHash = MD5(localPage) localPage += '&lt;p style=display:none&gt;hash: #'+localHash+'#&lt;/p&gt;' }else{ def default_toc = '&lt;p&gt;&lt;ac:structured-macro ac:name=toc/&gt;&lt;/p&gt;' def prefix = (config.confluence.tableOfContents?:default_toc)+(config.confluence.extraPageContent?:'') localPage = prefix+localPage def default_children = '&lt;p&gt;&lt;ac:structured-macro ac:name=children&gt;&lt;ac:parameter ac:name=sort&gt;creation&lt;/ac:parameter&gt;&lt;/ac:structured-macro&gt;&lt;/p&gt;' localPage += (config.confluence.tableOfChildren?:default_children) localHash = MD5(localPage) localPage += '&lt;p style=display:none&gt;hash: #'+localHash+'#&lt;/p&gt;' } def request = [ type : 'page', title: realTitle(pageTitle), space: [ key: confluenceSpaceKey ], body : [ storage: [ value : localPage, representation: 'storage' ] ] ] if (parentId) { request.ancestors = [ [ type: 'page', id: parentId] ] } def pages = retrieveAllPages(api, headers, config.confluence.spaceKey) // println Suche nach vorhandener Seite:  + pageTitle Map existingPage = pages[realTitleLC] def page if (existingPage) { if (hasRequestedParent(existingPage, parentId)) { page = retrieveFullPage(api, headers, existingPage.id) } else { page = null } } else { page = null } // println Gefunden:  + page.id +  Titel:  + page.title if (page) { println found existing page:  + page.id + version +page.version.number //extract hash from remote page to see if it is different from local one def remotePage = page.body.storage.value.toString().trim() def remoteHash = remotePage =~ /(?ms)hash: #([^#]+)#/ remoteHash = remoteHash.size()==0?:remoteHash[0][1] // println remoteHash:  + remoteHash // println localHash:  + localHash if (remoteHash == localHash) { println page hasn't changed! deferredUpload.each { uploadAttachment(page?.id, it[1], it[2], it[3]) } deferredUpload = [] // #324-dierk42: Add keywords as labels to page. if (keywords) { addLabels(page.id, keywords) } return page.id } else { trythis { // update page // https://developer.atlassian.com/display/CONFDEV/Confluence+REST+API+Examples#ConfluenceRESTAPIExamples-Updatingapage request.id = page.id request.version = [number: (page.version.number as Integer) + 1] def res = api.put(contentType: ContentType.JSON, requestContentType : ContentType.JSON, path: 'content/' + page.id, body: request, headers: headers) } println &gt; updated page +page.id deferredUpload.each { uploadAttachment(page.id, it[1], it[2], it[3]) } deferredUpload = [] // #324-dierk42: Add keywords as labels to page. if (keywords) { addLabels(page.id, keywords) } return page.id } } else { //#352-LuisMuniz if the existing page's parent does not match the requested parentId, fail if (existingPage &amp;&amp; !hasRequestedParent(existingPage, parentId)) { throw new IllegalArgumentException(Cannot create page, page with the same  + title=${existingPage.title}  + with id=${existingPage.id} already exists in the space.  + A Confluence page title must be unique within a space, consider specifying a 'confluencePagePrefix' in ConfluenceConfig.groovy) } //create a page trythis { page = api.post(contentType: ContentType.JSON, requestContentType: ContentType.JSON, path: 'content', body: request, headers: headers) } println &gt; created page +page?.data?.id deferredUpload.each { uploadAttachment(page?.data?.id, it[1], it[2], it[3]) } deferredUpload = [] // #324-dierk42: Add keywords as labels to page. if (keywords) { addLabels(page?.data?.id, keywords) } return page?.data?.id } } def parseAnchors = { page -&gt; def anchors = [:] page.body.select('[id]').each { anchor -&gt; def name = anchor.attr('id') anchors[name] = page.title anchor.before(&lt;ac:structured-macro ac:name=\anchor\&gt;&lt;ac:parameter ac:name=\\&gt;${name}&lt;/ac:parameter&gt;&lt;/ac:structured-macro&gt;) } anchors } def pushPages pushPages = { pages, anchors, pageAnchors, labels -&gt; pages.each { page -&gt; println page.title def id = pushToConfluence page.title, page.body, page.parent, anchors, pageAnchors, labels page.children*.parent = id // println Push children von id  + id pushPages page.children, anchors, pageAnchors, labels // println Ende Push children von id  + id } } def recordPageAnchor = { head -&gt; def a = [:] if (head.attr('id')) { a[head.attr('id')] = head.text() } a } def promoteHeaders = { tree, start, offset -&gt; (start..7).each { i -&gt; tree.select(h${i}).tagName(h${i-offset}).before('&lt;br /&gt;') } } def retrievePageIdByName = { String name -&gt; def api = new RESTClient(config.confluence.api) def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'Content-Type':'application/json; charset=utf-8' ] trythis { def request = [ 'title' : name, 'spaceKey' : confluenceSpaceKey ] api.get( [ 'headers': headers, 'path' : ${baseApiPath}content, 'query' : request, ] ).data.results?.getAt(0)?.id } ?: null } config.confluence.input.each { input -&gt; input.file = ${docDir}/${input.file} println publish ${input.file} if (input.file ==~ /.*[.](ad|adoc|asciidoc)$/) { println convert ${input.file} groovy asciidoc2html.groovy ${input.file}.execute() input.file = input.file.replaceAll(/[.](ad|adoc|asciidoc)$/, '.html') println to ${input.file} } // assignend, but never used in pushToConfluence(...) (fixed here) confluenceSpaceKey = input.spaceKey ?: config.confluence.spaceKey confluenceCreateSubpages = (input.createSubpages != null) ? input.createSubpages : config.confluence.createSubpages // hard to read in case of using :sectnums: -&gt; so we add a suffix confluencePagePrefix = input.pagePrefix ?: config.confluence.pagePrefix // added confluencePageSuffix = input.pageSuffix ?: config.confluence.pageSuffix confluencePreambleTitle = input.preambleTitle ?: config.confluence.preambleTitle def html = input.file ? new File(input.file).getText('utf-8') : new URL(input.url).getText() baseUrl = input.file ? new File(input.file) : new URL(input.url) Document dom = Jsoup.parse(html, 'utf-8', Parser.xmlParser()) dom.outputSettings().prettyPrint(false);//makes html() preserve linebreaks and spacing dom.outputSettings().escapeMode(org.jsoup.nodes.Entities.EscapeMode.xhtml); //This will ensure xhtml validity regarding entities dom.outputSettings().charset(UTF-8); //does no harm :-) // if ancestorName is defined try to find machingAncestorId in confluence def retrievedAncestorId if (input.ancestorName) { // Retrieve a page id by name retrievedAncestorId = retrievePageIdByName(input.ancestorName) println(Retrieved pageId for given ancestorName '${input.ancestorName}' is ${retrievedAncestorId}) } // if input does not contain an ancestorName, check if there is ancestorId, otherwise check if there is a global one def parentId = retrievedAncestorId ?: input.ancestorId ?: config.confluence.ancestorId // if parentId is still not set, create a new parent page (parentId = null) parentId = parentId ?: null //println(ancestorName: '${input.ancestorName}', ancestorId: ${input.ancestorId} ---&gt; final parentId: ${parentId}) def anchors = [:] def pageAnchors = [:] def sections = pages = [] // #342-dierk42: get the keywords from the meta tags def keywords = [] dom.select('meta[name=keywords]').each { kw -&gt; kws = kw.attr('content').split(',') kws.each { skw -&gt; keywords &lt;&lt; skw.trim() } println Keywords: + keywords } // let's try to select the first page and push it to confluence dom.select('div#preamble div.sectionbody').each { pageBody -&gt; pageBody.select('div.sect2').unwrap() def preamble = [ title: confluencePreambleTitle ?: arc42, body: pageBody, children: [], parent: parentId ] pages &lt;&lt; preamble sections = preamble.children parentId = null anchors.putAll(parseAnchors(preamble)) } // &lt;div class=sect1&gt; are the main headings // let's extract these dom.select('div.sect1').each { sect1 -&gt; Elements pageBody = sect1.select('div.sectionbody') def currentPage = [ title: sect1.select('h2').text(), body: pageBody, children: [], parent: parentId ] pageAnchors.putAll(recordPageAnchor(sect1.select('h2'))) if (confluenceCreateSubpages) { pageBody.select('div.sect2').each { sect2 -&gt; def title = sect2.select('h3').text() pageAnchors.putAll(recordPageAnchor(sect2.select('h3'))) sect2.select('h3').remove() def body = Jsoup.parse(sect2.toString(),'utf-8', Parser.xmlParser()) body.outputSettings(new Document.OutputSettings().prettyPrint(false)) def subPage = [ title: title, body: body ] currentPage.children &lt;&lt; subPage promoteHeaders sect2, 4, 3 anchors.putAll(parseAnchors(subPage)) } pageBody.select('div.sect2').remove() } else { pageBody.select('div.sect2').unwrap() promoteHeaders sect1, 3, 2 } sections &lt;&lt; currentPage anchors.putAll(parseAnchors(currentPage)) } pushPages pages, anchors, pageAnchors, keywords if (parentId) { println published to ${config.confluence.api - rest/api/}spaces/${confluenceSpaceKey}/pages/${parentId} } else { println published to ${config.confluence.api - rest/api/}spaces/${confluenceSpaceKey} } }  "
},

{
    "id": 9,
    "uri": "015_tasks/03_task_exportContributors.html",
    "menu": "tasks",
    "title": "exportContributors",
    "text": " Table of Contents exportContributors About This Task How to Use This Task About the Avatar-Icons File Attributes .gravatar img { margin-left: 3px; border-radius: 4px; } exportContributors 3 minutes to read About This Task This task crawls through all Asciidoctor source files and extracts a list of contributors, which is then rendered as AsciiDoc images of each contributor&#8217;s gravatar picture. The extracted list is stored in /home/runner/work/docToolchain/docToolchain/build/contributors/015_tasks/03_task_exportContributors.adoc so it can be easily included in your documents. How to Use This Task The best way to use this task is to create a feedback.adoc file similar to this: feedback.adoc ifndef::backend-pdf[] // (1) image::https://img.shields.io/badge/improve-this%20doc-orange.svg[link={manualdir}{filename}, float=right] // (2) image::https://img.shields.io/badge/create-an%20issue-blue.svg[link=https://github.com/docToolchain/documentation/issues/new?title=&amp;body=%0A%0A%5BEnter%20feedback%20here%5D%0A%0A%0A---%0A%23page:{filename}, float=right] // (3) endif::[] include::{targetDir}/contributors/{filename}[] // (4) Key: Do not show this section when docs are rendered as PDF. Create an Improve This Doc button which links to your GitHub sources. Create a Create an Issue button which links to your issue tracker. Include the list of contributors created by this task. (The task automatically adds the estimated reading time to the list of contributors.) About the Avatar-Icons It seems not to be possible to extract a link to the github avatar icons from the log. So, the solution is to use Gravatar icons. For this to work, the contributors email address is hashed and an icon link is generated from that hash. http://www.gravatar.com/avatar/cc5f3bf8b3cb91c985ed4fd046aa451d?d=identicon This result at least in an icon which has a distinct color. Contributors can setup their own image through Gravatar.com . For this to work, the git commits need to use an email address which can be verified by Gravatar.com. Infortunately, this is not the case if a contributor decided to make his email address private in the email settions of her github account. File Attributes This task also exports some GitHub file attributes. The extracted attributes are stored in /home/runner/work/docToolchain/docToolchain/build/fileattribs/015_tasks/03_task_exportContributors.adoc . :lastUpdated: 16.05.2019 06:22 :lastAuthorName: Ralf D. Müller :lastAuthorEmail: ralf.d.mueller@gmail.com :lastAuthorAvatar: http://www.gravatar.com/avatar/cc5f3bf8b3cb91c985ed4fd046aa451d?d=identicon[32,32,role='gravatar',alt='Ralf D. Müller',title='Ralf D. Müller'] :lastMessage: #310 started to document config options You can import and use these attributes in the same way as you import the contributors list. Important please make sure that you do not accidentally publish the email address if your contributors do not want it. For example: feedback.adoc include::{targetDir}/fileattribs/{filename}[] Last updated {lastUpdated} by {lastAuthorName} "
},

{
    "id": 10,
    "uri": "015_tasks/03_task_copy_themes.html",
    "menu": "tasks",
    "title": "copyThemes",
    "text": " Table of Contents copyThemes About This Task Source .gravatar img { margin-left: 3px; border-radius: 4px; } copyThemes 1 minute to read About This Task docToolchain provides you with a simple Twitter bootstrap default theme to get you started. You can use the copyThemes task to apply a different theme (either jBakeTheme or pdfTheme) to your project. Feel free to remove all files which should remain as the default and change all others. When you next run docToolchain, your theme files will be laid over the default theme in order to generate the PDF or site. Source scripts/copyThemes.gradle //tag::copyThemes[] task copyThemes( description: 'copy some default files to your project for you to modify', group: 'docToolchain helper' ) { doFirst { } doLast { def color = { color, text -&gt; def colors = [black: 30, red: 31, green: 32, yellow: 33, blue: 34, magenta: 35, cyan: 36, white: 37] return new String((char) 27) + [${colors[color]}m${text} + new String((char) 27) + [0m } def lang = ant.input(message: ${color 'green', 'What do you want me to copy?'}, validargs: 'pdfTheme,jBakeTheme', addproperty: 'what') switch (ant.what) { case 'pdfTheme': def targetDir = new File(pdfThemeDir) /** if (targetDir.exists()) { println ${targetDir.canonicalPath} already exists println in order to re-install the theme, please remove the folder first and re-run the script throw new RuntimeException(pdfTheme folder already exists) } **/ targetDir.mkdirs() def source = new File(projectDir, 'template_config/pdfTheme') println source.canonicalPath println targetDir.canonicalPath copy { from new File(projectDir, 'template_config/pdfTheme') into targetDir } println pdfTheme copied into ${targetDir} break case 'jBakeTheme': def targetDir = new File(new File(docDir, inputPath), config.microsite.siteFolder?:'../site') /** if (targetDir.exists()) { println ${targetDir.canonicalPath} already exists println in order to re-install the theme, please remove the folder first and re-run the script throw new RuntimeException(jBakeTheme folder already exists) } **/ targetDir.mkdirs() copy { from new File(projectDir, 'src/site') into targetDir } def siteTheme = System.getenv('DTC_SITETHEME')?: def themeFolder = new File(projectDir, ../themes/ + siteTheme.md5()) copy { from(themeFolder) {} into targetDir } println jBakeTheme copied into ${targetDir.canonicalPath} break } } } //end::copyThemes[] "
},

{
    "id": 11,
    "uri": "015_tasks/03_task_exportMetrics.html",
    "menu": "tasks",
    "title": "exportMetrics",
    "text": " Table of Contents exportMetrics .gravatar img { margin-left: 3px; border-radius: 4px; } exportMetrics 1 minute to read This task crawls through all Asciidoctor source files and extracts the total number of words in each file (word count) so you can check your writing progress. The output is displayed on the command line. "
},

{
    "id": 12,
    "uri": "015_tasks/03_task_exportChangeLog.html",
    "menu": "tasks",
    "title": "exportChangeLog",
    "text": " Table of Contents exportChangeLog About This Task Further Reading Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportChangeLog 2 minutes to read About This Task As the name suggests, this task exports the changelog to be referenced from within your documentation, if needed. The changelog is written to build/docs/changelog.adoc . This task can be configured to use a different source control system or a different directory. To configure this task, copy template_config/scripts/ChangelogConfig.groovy to your directory and modify to suit your needs. Then use -PchangelogConfigFile=&lt;your config file&gt; to add the path to your configuration file to the task. See the description inside the template for more details. By default, the source is the Git changelog for the path src/docs and only contains the commit messages for changes made to the documentation. All changes to the build or other sources in the repository will not show up. By default, the changelog contains changes made to date , author and commit message already formatted as AsciiDoc table content: | 09.04.2017 | Ralf D. Mueller | fix #24 template updated to V7.0 | 08.04.2017 | Ralf D. Mueller | fixed typo You simply include it like this: .Changes [options=header,cols=1,2,6] |==== | Date | Author | Comment include::../../build/docs/changelog.adoc[] |==== By excluding the table definition, you can easily translate the table headings through different text snippets. Note In a future docToolchain release, you will have the ability to include only certain commit messages from the changelog and exclude others (starting with # or // ?). This feature is not available just yet. Further Reading The only constant in life is change blog post. Source exportChangelog.gradle task exportChangeLog( description: 'exports the change log from a git subpath', group: 'docToolchain' ) { doFirst { new File(targetDir).mkdirs() } doLast { logger.info(docToolchain&gt; docDir: +docDir) logger.info(docToolchain&gt; mainConfigFile: +mainConfigFile) def config = new ConfigSlurper().parse(new File(docDir, mainConfigFile).text) def cmd = ${config.changelog.cmd} . def changes = cmd.execute(null, new File(docDir, config.changelog.dir)).text def changelog = new File(targetDir, 'changelog.adoc') logger.info &gt; changelog exported ${changelog.canonicalPath} changelog.write(changes) } } "
},

{
    "id": 13,
    "uri": "015_tasks/03_tasks.html",
    "menu": "tasks",
    "title": "What Is a Task?",
    "text": " Table of Contents What Is a Task? How Tasks Are Named generateX exportX convertToX publishToX .gravatar img { margin-left: 3px; border-radius: 4px; } What Is a Task? 2 minutes to read A task is another name for a script which triggers the build actions which compile and publish your docs. This diagram gives you an overview of the entire build process: Figure 1. docToolchain How Tasks Are Named Tasks are given a naming prefix which indicates their role in the build process. There are currently 4 groups. generateX These tasks use plain old AsciiDoctor functionality to render the source to a given format. exportX These tasks export images and AsciiDoc snippets from other systems or file formats. The resulting artifacts can then be included from your main sources. export tasks differ from generate tasks because with export tasks, you don&#8217;t have to export with each build. Also, with export tasks, it&#8217;s likely that you will already store the resulting artifacts under version control because the tools needed for the export (such as Sparx Enterprise Architect or MS PowerPoint) are typically not available on your build server or another contributor&#8217;s machine. convertToX These tasks take the output from AsciiDoctor and convert it (through other tools) to the target format. This results in a dependency on a generateX task and another external tool (currently pandoc ). publishToX These tasks not only convert your documents but also deploy, publish and move them to a remote system (currently Confluence), meaning the result is immediately visible to others. "
},

{
    "id": 14,
    "uri": "015_tasks/03_task_generateDeck.html",
    "menu": "tasks",
    "title": "generateDeck",
    "text": " Table of Contents generateDeck About This Task Source .gravatar img { margin-left: 3px; border-radius: 4px; } generateDeck 1 minute to read About This Task This task makes use of the asciidoctor-reveal.js backend to render your documents into a HTML-based presentation. It creates a PowerPoint presentation, then enriches it by adding reveal.js slide definitions in AsciiDoc to the speaker notes. For best results, use this task with the exportPPT task. Source AsciiDocBasics.gradle task generateDeck ( type: AsciidoctorTask, group: 'docToolchain', description: 'use revealJs as asciidoc backend to create a presentation') { attributes ( 'idprefix': 'slide-', 'idseparator': '-', 'docinfo1': '', 'revealjs_theme': 'black', 'revealjs_progress': 'true', 'revealjs_touch': 'true', 'revealjs_hideAddressBar': 'true', 'revealjs_transition': 'linear', 'revealjs_history': 'true', 'revealjs_slideNumber': 'true' ) options template_dirs : [new File(new File (projectDir,'/resources/asciidoctor-reveal.js'),'templates').absolutePath ] def sourceFilesREVEAL = sourceFiles.findAll { 'revealjs' in it.formats } // onlyIf { // sourceFilesREVEAL // } sources { sourceFilesREVEAL.each { include it.file logger.info it.file } } outputDir = file(targetDir+'/decks/') resources { from('resources') { include 'reveal.js/**' } from(sourceDir) { include 'images/**' } into() logger.info ${docDir}/${config.outputPath}/images } doFirst { if (sourceFilesREVEAL.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type 'revealjs'. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } "
},

{
    "id": 15,
    "uri": "015_tasks/03_task_exportMarkdown.html",
    "menu": "tasks",
    "title": "exportMarkdown",
    "text": " Table of Contents exportMarkdown Source .gravatar img { margin-left: 3px; border-radius: 4px; } 1 minute to read exportMarkdown The exportMarkdown task can be used to include markdown files into the documentation. The task is scanning the directory /src/docs for markdown files ( *.md ) and converts them into AsciiDoc files. The converted files can then be referenced from within the /build -folder. Source exportMarkdown.gradle task exportMarkdown( description: 'exports all markdown files to AsciiDoc', group: 'docToolchain', type: Copy ) { from srcDir include(**/*.md) //include only markdown files includeEmptyDirs = false rename(/(.+).md/, '$1.adoc') //rename all files from *.md to *.adoc filter(Markdown2AdocFilter) // convert the content of the files into targetDir } class Markdown2AdocFilter extends FilterReader { Markdown2AdocFilter(Reader input) { super(new StringReader(nl.jworks.markdown_to_asciidoc.Converter.convertMarkdownToAsciiDoc(input.text))) } } "
},

{
    "id": 16,
    "uri": "015_tasks/03_task_exportJiraSprintChangelog.html",
    "menu": "tasks",
    "title": "exportJiraSprintChangelogIssues",
    "text": " Table of Contents exportJiraSprintChangelogIssues About This Task Configuration Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportJiraSprintChangelogIssues 1 minute to read About This Task This task exports a simplified (key and summary) list of Jira issues for a specific sprint defined in the task configuration. Only a few additional fields (such as assignee) can be switched using configuration flags. Once you define the sprint, the relevant AsciiDoc and Excel files will be generated. If a sprint is not defined in the configuration, changelogs for all sprints that match the configuration will be saved in separate AsciiDoc files and in different tabs within an Excel file. The task configuration can be found within Config.gradle . In addition to the configuration snippet below, it is important to configure the Jira API and credentials in the Jira section of the configuration inside the same file. Configuration Config.groovy // Sprint changelog configuration generate changelog lists based on tickets in sprints of an Jira instance. // This feature requires at least Jira API &amp; credentials to be properly set in Jira section of this configuration sprintChangelog = [:] sprintChangelog.with { sprintState = 'closed' // it is possible to define multiple states, i.e. 'closed, active, future' ticketStatus = Done, Closed // it is possible to define multiple ticket statuses, i.e. Done, Closed, 'in Progress' showAssignee = false showTicketStatus = false showTicketType = true sprintBoardId = 12345 // Jira instance probably have multiple boards; here it can be defined which board should be used // Output folder for this task inside main outputPath resultsFolder = 'Sprints' // if sprintName is not defined or sprint with that name isn't found, release notes will be created on for all sprints that match sprint state configuration sprintName = 'PRJ Sprint 1' // if sprint with a given sprintName is found, release notes will be created just for that sprint allSprintsFilename = 'Sprints_Changelogs' // Extension will be automatically added. } Source exportJiraSprintChangelog.gradle task exportJiraSprintChangelog( description: 'exports all jira issues from Sprint for release notes', group: 'docToolchain' ) { doLast { // Pre defined ticket fields for Changelog based on Jira Sprints def defaultTicketFields = 'summary,status,assignee,issuetype' // retrieving sprints for a given board def sprints = { apiSprints, headers, boardId, sprintState -&gt; apiSprints.get(path: agile/latest/board/${boardId}/sprint, query:[state: ${sprintState}], headers: headers ).data } // retrieving issues for given sprint def issues = { apiIssues, headers, boardId, sprintId, status -&gt; apiIssues.get(path: agile/latest/board/${boardId}/sprint/${sprintId}/issue, query: ['jql' : status in (${status}) ORDER BY type DESC, status ASC, 'maxResults': 1000, fields: defaultTicketFields ], headers: headers ).data } // preparing target folder for generated files final String taskSubfolderName = config.sprintChangelog.resultsFolder final File targetFolder = new File(targetDir + File.separator + taskSubfolderName) if (!targetFolder.exists()) targetFolder.mkdirs() logger.debug(Output folder for 'exportJiraSprintChangelog' task is: '${targetFolder}') // Getting configuration def jiraRoot = config.jira.api def jiraProject = config.jira.project def sprintState = config.sprintChangelog.sprintState def ticketStatusForReleaseNotes = config.sprintChangelog.ticketStatus def sprintBoardId = config.sprintChangelog.sprintBoardId def showAssignee = config.sprintChangelog.showAssignee def showTicketStatus = config.sprintChangelog.showTicketStatus def showTicketType = config.sprintChangelog.showTicketType def sprintName = config.sprintChangelog.sprintName def allSprintsFilename = config.sprintChangelog.allSprintsFilename logger.info(\n==========================\nJira Release notes config\n==========================) logger.info(Spring Board ID: ${sprintBoardId}) logger.info(Show assignees: ${showAssignee}. Show ticket status: ${showTicketStatus}. Show ticket type: ${showTicketType}) logger.info(Filtering for sprints with configured state: '${sprintState}') logger.info(Filtering for issues with configured statuses: ${ticketStatusForReleaseNotes}) logger.info(Attempt to generate release notes for sprint with a name: '${sprintName}') logger.info(Filename used for all sprints: '${allSprintsFilename}') def api = new groovyx.net.http.RESTClient(jiraRoot + '/rest/') api.encoderRegistry = new groovyx.net.http.EncoderRegistry(charset: 'utf-8') def headers = [ 'Authorization': Basic  + config.jira.credentials, 'Content-Type' : 'application/json; charset=utf-8' ] def allChangelogsFilename = ${allSprintsFilename}.xlsx logger.quiet(Changelogs of all sprints will be saved in '${allChangelogsFilename}' file) def changelogsXls = new File(targetFolder, allChangelogsFilename) def changelogsXlsFos = new FileOutputStream(changelogsXls) Workbook wb = new XSSFWorkbook(); CreationHelper hyperlinkHelper = wb.getCreationHelper(); String rgbS = A7A7A7 byte[] rgbB = Hex.decodeHex(rgbS) // get byte array from hex string XSSFColor color = new XSSFColor(rgbB, null) //IndexedColorMap has no usage until now. So it can be set null. XSSFCellStyle headerCellStyle = (XSSFCellStyle) wb.createCellStyle() headerCellStyle.setFillForegroundColor(color) headerCellStyle.setFillPattern(FillPatternType.SOLID_FOREGROUND) // prepare tickets according to configuration def columns = ['key'].plus(defaultTicketFields.split(',').collect()) if (!showAssignee) { columns = columns.minus('assignee')} if (!showTicketStatus) { columns = columns.minus('status')} if (!showTicketType) { columns = columns.minus('issuetype')} logger.info(Release notes will contain following info: ${columns}) logger.info(\n=====================\n Sprints\n=====================) // def allMatchedSprints = sprints(api, headers, sprintBoardId, sprintState).values def foundExactSprint = allMatchedSprints.any {it.name == sprintName} logger.info(All sprints that matched configuration: ${allMatchedSprints.size()}) def sprintsForChangelog = foundExactSprint ? allMatchedSprints.stream().filter() {it.name == sprintName} : allMatchedSprints logger.info(Found exact Sprint with name '${sprintName}': ${foundExactSprint}.) sprintsForChangelog.each { sprint -&gt; logger.quiet(\nSprint: $sprint.name [id: $sprint.id] state &lt;$sprint.state&gt;) /* ================================================ Create new worksheet inside existing excel file ================================================ */ String safeSprintName = WorkbookUtil.createSafeSheetName(${sprint.name}) def ws = wb.createSheet(safeSprintName) // Add titles (typically key &amp; summary, but assignee, ticket status, ticket type can be configured in Config.groovy too) def titleRow = ws.createRow(0); int cellNumber = 0; columns.each {columnTitle -&gt; titleRow.createCell(cellNumber++).setCellValue(${columnTitle.capitalize()})} def lastRow = titleRow.getRowNum() titleRow.setRowStyle(headerCellStyle) // set summary (at position 1) column wider than other columns ws.setColumnWidth(1, 35*256) /* ========================================= AsciiDoc file for each sprint ========================================= */ def asciidocFilename = ${sprint.name.replaceAll( , _)}.adoc logger.info(Results will be saved in '${asciidocFilename}' file) def changeLogAdoc = new File(targetFolder, ${asciidocFilename}) changeLogAdoc.write(.Table ${sprint.name} Changelog\n, 'utf-8') changeLogAdoc.append(|=== \n) // AsciiDoc table columns columns.each {columnTitle -&gt; changeLogAdoc.append(|${columnTitle} , 'utf-8')} /* ========================================= Add tickets for the sprint ========================================= */ issues(api, headers, sprintBoardId, sprint.id, ticketStatusForReleaseNotes).issues.each {issue -&gt; def assignee = ${issue.fields.assignee ? issue.fields.assignee.displayName : 'unassigned'}  def message = showAssignee ? by ${assignee} :  logger.quiet(Issue: [$issue.key] '$issue.fields.summary' ${message}&lt;$issue.fields.status.name&gt;) /* =========================== Write ticket to Excel =========================== */ int cellPosition = 0 def row = ws.createRow(++lastRow) Hyperlink link = hyperlinkHelper.createHyperlink(HyperlinkType.URL) link.setAddress(${jiraRoot}/browse/${issue.key}) Cell cellWithUrl = row.createCell(cellPosition) cellWithUrl.setCellValue(${issue.key}) cellWithUrl.setHyperlink(link) row.createCell(++cellPosition).setCellValue(${issue.fields.summary}) /* ============================= Write ticket to Asciidoc ============================= */ changeLogAdoc.append(\n, 'utf-8') changeLogAdoc.append(| ${jiraRoot}/browse/${issue.key}[${issue.key}] , 'utf-8') changeLogAdoc.append(| ${issue.fields.summary} , 'utf-8') /* === Write ticket status, assignee, ticket typee if configured to both Asciidoc &amp; Excel files === */ if (showTicketStatus) { row.createCell(++cellPosition).setCellValue(${issue.fields.status.name}) changeLogAdoc.append(| ${issue.fields.status.name} , 'utf-8') } if (showAssignee) { row.createCell(++cellPosition).setCellValue(${assignee}) changeLogAdoc.append(| ${assignee}, 'utf-8') } if (showTicketType) { row.createCell(++cellPosition).setCellValue(${issue.fields.issuetype.name}) changeLogAdoc.append(| ${issue.fields.issuetype.name} , 'utf-8') } } // Close the asciidoc table changeLogAdoc.append(\n|=== \n,'utf-8') // Set auto-width to KEY column ws.autoSizeColumn(0); } // Write to Excel file wb.write(changelogsXlsFos) } } "
},

{
    "id": 17,
    "uri": "015_tasks/03_task_dependencyUpdates.html",
    "menu": "tasks",
    "title": "dependencyUpdates",
    "text": " Table of Contents dependencyUpdates .gravatar img { margin-left: 3px; border-radius: 4px; } dependencyUpdates 1 minute to read This task uses the Gradle versions plugin created by Ben Manes to check for outdated build dependencies. Use this task to keep all dependencies up to date. Warning If you discover newer version, it doesn&#8217;t mean that versions and dependencies will play nicely together. To ensure that everything works, we recommend the versions selected by docToolchain contributors. Tip For more information, read the Handle Dependency Updates the easy Way blog post. "
},

{
    "id": 18,
    "uri": "015_tasks/03_task_fixencoding.html",
    "menu": "tasks",
    "title": "fixEncoding",
    "text": " Table of Contents fixEncoding About This Task Source .gravatar img { margin-left: 3px; border-radius: 4px; } fixEncoding 1 minute to read About This Task Whenever Asciidoctor has to process a file that is not UTF-8 encoded, Ruby tries to read it, then throws an error similar to this one: asciidoctor: FAILED: /home/demo/test.adoc: Failed to load AsciiDoc document - invalid byte sequence in UTF-8 Unfortunately, finding the incorrectly encoded file is difficult if a lot of includes:: are used, and Asciidoctor will only show the name of the main document. This is not Asciidoctor&#8217;s fault. The fault lies with the Ruby interpreter that sits underneath. The fixEncoding task crawls through all *.ad and *.adoc files and checks their encoding. If it comes across a file which is not UTF-8 encoded, it will rewrite it with the UTF-8 encoding. Source scripts/fixEncoding.gradle import groovy.util.* import static groovy.io.FileType.* task fixEncoding( description: 'finds and converts non UTF-8 adoc files to UTF-8', group: 'docToolchain helper', ) { doLast { File sourceFolder = new File(${docDir}/${inputPath}) println(sourceFolder:  + sourceFolder.canonicalPath) sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^.*(ad|adoc|asciidoc)$') { CharsetToolkit toolkit = new CharsetToolkit(file); // guess the encoding def guessedCharset = toolkit.getCharset().toString().toUpperCase(); if (guessedCharset!='UTF-8') { def text = file.text file.write(text, utf-8) println( converted ${file.name} from '${guessedCharset}' to 'UFT-8') } } } } } "
},

{
    "id": 19,
    "uri": "015_tasks/03_task_exportPPT.html",
    "menu": "tasks",
    "title": "exportPPT",
    "text": " Table of Contents exportPPT About This Task Further Reading and Resources Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportPPT About This Task This task lets you export AsciiDoc content to a series of PowerPoint slides. It is currently a Windows-only task. 1 minute to read Further Reading and Resources Read the Do More with Slides blog post. Find more information about the Windows-only aspect of this task in this issue . Check out asciidoctorj-office-extension for another way to use PPT slides in your docs. Source exportPPT.gradle task exportPPT( dependsOn: [streamingExecute], description: 'exports all slides and some texts from PPT files', group: 'docToolchain' ) { doLast { //make sure path for notes exists //and remove old notes new File(projectDir, 'src/docs/ppt').deleteDir() //also remove old diagrams new File(projectDir, 'src/docs/images/ppt').deleteDir() //create a readme to clarify things def readme = This folder contains exported slides or notes from .ppt presentations. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportPPT` to re-export files  new File(projectDir, 'src/docs/images/ppt/.').mkdirs() new File(projectDir, 'src/docs/images/ppt/readme.ad').write(readme) new File(projectDir, 'src/docs/ppt/.').mkdirs() new File(projectDir, 'src/docs/ppt/readme.ad').write(readme) //execute through cscript in order to make sure that we get WScript.echo right %SystemRoot%\\System32\\cscript.exe //nologo ${projectDir}/scripts/exportPPT.vbs.executeCmd() } } exportPPT.vbs Const ForAppending = 8 Const ppPlaceholderBody = 2 ' Helper ' http://windowsitpro.com/windows/jsi-tip-10441-how-can-vbscript-create-multiple-folders-path-mkdir-command Function MakeDir (strPath) Dim strParentPath, objFSO Set objFSO = CreateObject(Scripting.FileSystemObject) On Error Resume Next strParentPath = objFSO.GetParentFolderName(strPath) If Not objFSO.FolderExists(strParentPath) Then MakeDir strParentPath If Not objFSO.FolderExists(strPath) Then objFSO.CreateFolder strPath On Error Goto 0 MakeDir = objFSO.FolderExists(strPath) End Function Function SearchPresentations(path) For Each folder In path.SubFolders SearchPresentations folder Next For Each file In path.Files If (Left(fso.GetExtensionName (file.Path), 3) = ppt) OR (Left(fso.GetExtensionName (file.Path), 3) = pps) Then WScript.echo found &amp;file.path ExportSlides(file.Path) End If Next End Function Sub ExportSlides(sFile) Set objRegEx = CreateObject(VBScript.RegExp) objRegEx.Global = True objRegEx.IgnoreCase = True objRegEx.MultiLine = True ' . doesn't work for multiline in vbs, [\s,\S] does... objRegEx.Pattern = [\s,\S]*{adoc} ' http://www.pptfaq.com/FAQ00481_Export_the_notes_text_of_a_presentation.htm strFileName = fso.GetFIle(sFile).Name Set oPPT = CreateObject(PowerPoint.Application) Set oPres = oPPT.Presentations.Open(sFile, True, False, False) ' Read Only, No Title, No Window Set oSlides = oPres.Slides strNotesText =  strImagePath = /src/docs/images/ppt/ &amp; strFileName &amp; / MakeDir(. &amp; strImagePath) strNotesPath = /src/docs/ppt/ MakeDir(. &amp; strNotesPath) For Each oSl In oSlides strSlideName = oSl.Name ' WScript.echo fso.GetAbsolutePathName(.) &amp; strImagePath &amp; strSlideName &amp; .jpg oSl.Export fso.GetAbsolutePathName(.) &amp; strImagePath &amp; strSlideName &amp; .jpg, .jpg For Each oSh In oSl.NotesPage.Shapes If oSh.PlaceholderFormat.Type = ppPlaceholderBody Then If oSh.HasTextFrame Then If oSh.TextFrame.HasText Then strCurrentNotes = oSh.TextFrame.TextRange.Text strCurrentNotes = Replace(strCurrentNotes,vbVerticalTab, vbCrLf) strCurrentNotes = Replace(strCurrentNotes,{slide},image::ppt/&amp;strFileName&amp;/&amp;strSlideName&amp;.jpg[]) ' remove speaker notes before marker {adoc} strCurrentNotes = objRegEx.Replace(strCurrentNotes,) strNotesText = strNotesText &amp; vbCrLf &amp; strCurrentNotes &amp; vbCrLf &amp; vbCrLf End If End If End If Next Next ' WScript.echo fso.GetAbsolutePathName(.) &amp; strNotesPath&amp;&amp;strFileName&amp;.ad ' http://stackoverflow.com/questions/2524703/save-text-file-utf-8-encoded-with-vba Set fsT = CreateObject(ADODB.Stream) fsT.Type = 2 'Specify stream type - we want To save text/string data. fsT.Charset = utf-8 'Specify charset For the source text data. fsT.Open 'Open the stream And write binary data To the object fsT.WriteText ifndef::imagesdir[:imagesdir: ../../images]&amp;vbCrLf&amp;CStr(strNotesText) fsT.SaveToFile fso.GetAbsolutePathName(.) &amp; strNotesPath&amp;&amp;strFileName&amp;.ad, 2 'Save binary data To disk oPres.Close() oPPT.Quit() End Sub set fso = CreateObject(Scripting.fileSystemObject) WScript.echo Slide extractor WScript.echo looking for .ppt files in  &amp; fso.GetAbsolutePathName(.) &amp; /src SearchPresentations fso.GetFolder(./src) WScript.echo finished exporting slides "
},

{
    "id": 20,
    "uri": "015_tasks/03_task_exportExcel.html",
    "menu": "tasks",
    "title": "exportExcel",
    "text": " Table of Contents exportExcel About This Task Further Reading Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportExcel 2 minutes to read About This Task Sometimes you need to include tabular data in your documentation. Most likely, this data will be stored as a MS Excel spreadsheet, or you may like to use Excel to create and edit it. Either way, this task lets you export an Excel spreadsheet and include it directly in your docs. It searches for .xlsx files and exports each contained worksheet as .csv and as .adoc . Note that formulas contained in your spreadsheet are evaluated and exported statically. The generated files are written to src/excel/[filename]/[worksheet].(adoc|cvs) . The src folder is used instead of the build folder because a better history of worksheet changes is captured. The files can be included either as AsciiDoc: include::excel/Sample.xlsx/Numerisch.adoc[] &#8230;&#8203;or as a CSV file: [options=header,format=csv] |=== include::excel/Sample.xlsx/Numerisch.csv[] |=== The AsciiDoc version gives you a bit more control because the following are preserved: Horizontal and vertical alignment. col-span and row-span. Line breaks. Column width relative to other columns. Background colors. Further Reading See asciidoctorj-office-extension to learn another way to use Excel spreadsheets in your docs. Source build.gradle task exportExcel( description: 'exports all excelsheets to csv and AsciiDoc', group: 'docToolchain' ) { doFirst { File sourceDir = file(srcDir) def tree = fileTree(srcDir).include('**/*.xlsx').exclude('**/~*') def exportFileDir = new File(sourceDir, 'excel') //make sure path for notes exists exportFileDir.deleteDir() //create a readme to clarify things def readme = This folder contains exported workbooks from Excel. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportExcel` to re-export files  exportFileDir.mkdirs() new File(exportFileDir, '/readme.ad').write(readme) } doLast { File sourceDir = file(srcDir) def exportFileDir = new File(sourceDir, 'excel') def tree = fileTree(srcDir).include('**/*.xlsx').exclude('**/~*') def nl = System.getProperty(line.separator) def export = { sheet, evaluator, targetFileName -&gt; def targetFileCSV = new File(targetFileName + '.csv') def targetFileAD = new File(targetFileName + '.adoc') def df = new org.apache.poi.ss.usermodel.DataFormatter(); def regions = [] sheet.numMergedRegions.times { regions &lt;&lt; sheet.getMergedRegion(it) } logger.debug sheet contains ${regions.size()} regions def color = '' def resetColor = false def numCols = 0 def headerCreated = false def emptyRows = 0 for (int rowNum=0; rowNum&lt;=sheet.lastRowNum; rowNum++) { def row = sheet.getRow(rowNum) if (row &amp;&amp; !headerCreated) { headerCreated = true // create AsciiDoc table header def width = [] numCols = row.lastCellNum numCols.times { columnIndex -&gt; width &lt;&lt; sheet.getColumnWidth((int) columnIndex) } //lets make those numbers nicer: width = width.collect { Math.round(100 * it / width.sum()) } targetFileAD.append('[options=header,cols=' + width.join(',') + ']' + nl) targetFileAD.append('|===' + nl) } def data = [] def style = [] def colors = [] // For each row, iterate through each columns if (row &amp;&amp; (row?.lastCellNum!=-1)) { numCols.times { columnIndex -&gt; def cell = row.getCell(columnIndex) if (cell) { def cellValue = df.formatCellValue(cell, evaluator) if (cellValue.startsWith('*') &amp;&amp; cellValue.endsWith('\u20AC')) { // Remove special characters at currency cellValue = cellValue.substring(1).trim(); } def cellStyle = '' def region = regions.find { it.isInRange(cell.rowIndex, cell.columnIndex) } def skipCell = false if (region) { //check if we are in the upper left corner of the region if (region.firstRow == cell.rowIndex &amp;&amp; region.firstColumn == cell.columnIndex) { def colspan = 1 + region.lastRow - region.firstRow def rowspan = 1 + region.lastColumn - region.firstColumn if (rowspan &gt; 1) { cellStyle += ${rowspan} } if (colspan &gt; 1) { cellStyle += .${colspan} } cellStyle += + } else { skipCell = true } } if (!skipCell) { switch (cell.cellStyle.alignmentEnum.toString()) { case 'RIGHT': cellStyle += '&gt;' break case 'CENTER': cellStyle += '^' break } switch (cell.cellStyle.verticalAlignmentEnum.toString()) { case 'BOTTOM': cellStyle += '.&gt;' break case 'CENTER': cellStyle += '.^' break } color = cell.cellStyle.fillForegroundXSSFColor?.RGB?.encodeHex() color = color != null ? nl + {set:cellbgcolor:#${color}} : '' data &lt;&lt; cellValue if (color == '' &amp;&amp; resetColor) { colors &lt;&lt; nl + {set:cellbgcolor!} resetColor = false } else { colors &lt;&lt; color } if (color != '') { resetColor = true } style &lt;&lt; cellStyle } else { data &lt;&lt;  colors &lt;&lt;  style &lt;&lt; skip } } else { data &lt;&lt;  colors &lt;&lt;  style &lt;&lt;  } } emptyRows = 0 } else { if (emptyRows&lt;3) { //insert empty row numCols.times { data &lt;&lt;  colors &lt;&lt;  style &lt;&lt;  } emptyRows++ } else { break } } targetFileCSV.append(data .collect { \${it.replaceAll('', '')}\ } .join(',') + nl, 'UTF-8') targetFileAD.append(data .withIndex() .collect { value, index -&gt; if (style[index] == skip) {  } else { style[index] + | ${value.replaceAll('[|]', '{vbar}').replaceAll(\n, ' +$0') + colors[index]} } } .join(nl) + nl * 2, 'UTF-8') } targetFileAD.append('|===' + nl) } tree.each { File excel -&gt; println file:  + excel def excelDir = new File(exportFileDir, excel.getName()) excelDir.mkdirs() InputStream inp inp = new FileInputStream(excel) def wb = org.apache.poi.ss.usermodel.WorkbookFactory.create(inp); def evaluator = wb.getCreationHelper().createFormulaEvaluator(); for (int wbi = 0; wbi &lt; wb.getNumberOfSheets(); wbi++) { def sheetName = wb.getSheetAt(wbi).getSheetName() println  -- sheet:  + sheetName def targetFile = new File(excelDir, sheetName) export(wb.getSheetAt(wbi), evaluator, targetFile.getAbsolutePath()) } inp.close(); } } } "
},

{
    "id": 21,
    "uri": "015_tasks/03_task_previewSite.html",
    "menu": "tasks",
    "title": "previewSite",
    "text": " Table of Contents previewSite .gravatar img { margin-left: 3px; border-radius: 4px; } previewSite 1 minute to read When you use the build in static site generator through generateSite , you sometimes need a real static server locally running to preview the results. This task will start a jetty with your generated site. Note When you use WSL to execute this command, you will need the IP V6 address of localhost [::1] to access the server. 127.0.0.1 or localhost will not work. Note Most site themes don&#8217;t need the static site server for general content. You can just preview the site by opening it right from the filesystem in your browser. But some javascript features will not work because of CORS restrictions. "
},

{
    "id": 22,
    "uri": "015_tasks/03_task_htmlSanityCheck.html",
    "menu": "tasks",
    "title": "htmlSanityCheck",
    "text": " Table of Contents htmlSanityCheck Source .gravatar img { margin-left: 3px; border-radius: 4px; } htmlSanityCheck 1 minute to read This task invokes the htmlSanityCheck gradle plugin. It is a standalone (batch- and command-line) html sanity checker - it detects missing images, dead links, and duplicate bookmarks. In docToolchain, this task is used to ensure that the generated HTML contains no missing links or other problems. This task is the last default task and creates a report in build/report/htmlchecks/index.html Figure 1. sample report Further information can be found on GitHub: https://github.com/aim42/htmlSanityCheck Tip Blog-Post: Automated Quality-Checks Source htmlSanityCheck.gradle htmlSanityCheck { sourceDir = new File(config.htmlSanityCheck.sourceDir?targetDir+/+config.htmlSanityCheck.sourceDir:$targetDir/html5) // files to check - in Set-notation //sourceDocuments = [ one-file.html, another-file.html, index.html] // where to put results of sanityChecks... checkingResultsDir = new File(config.htmlSanityCheck.checkingResultsDir?:checkingResultsPath) // directory where the results written to in JUnit XML format junitResultsDir = new File(config.htmlSanityCheck.junitResultsDir?:$targetDir/test-results/htmlchecks) // which statuscodes shall be interpreted as warning, error or success defaults to standard httpSuccessCodes = config.htmlSanityCheck.httpSuccessCodes?:[] httpWarningCodes = config.htmlSanityCheck.httpWarningCodes?:[] httpErrorCodes = config.htmlSanityCheck.httpErrorCodes?:[] // fail build on errors? failOnErrors = config.htmlSanityCheck.failOnErrors?:false logger.info docToolchain&gt; HSC sourceDir: ${sourceDir} logger.info docToolchain&gt; HSC checkingResultsDir: ${checkingResultsDir} } "
},

{
    "id": 23,
    "uri": "015_tasks/03_task_collectIncludes.html",
    "menu": "tasks",
    "title": "collectIncludes",
    "text": " Table of Contents collectIncludes About This Task Example .gravatar img { margin-left: 3px; border-radius: 4px; } collectIncludes 2 minutes to read About This Task This task crawls through your entire project looking for AsciiDoc files with a specific name pattern, then creates a single AsciiDoc file which includes only those files. When you create modular documentation, most includes are static. For example, the arc42-template has 12 chapters and a master template that includes those 12 chapters. Normally when you work with dynamic modules like ADRs (Architecture Decision Records) you create those files on the fly. Maybe not within your /src/docs folder, but alongside the code file for which you wrote the ADR. In order to include these files in your documentation, you have to add the file with its whole relative path to one of your AsciiDoc files. This task will handle it for you! Just stick to this file-naming pattern ^[A-Z]{3,}[-_].* (begin with at least three uppercase letters and a dash/underscore) and this task will collect the file and write it to your build folder. You only have to include this generated file from within your documentation. If you provide templates for the documents, those templates are skipped if the name matches the pattern ^.\*[-\_][tT]emplate[-\_].* . Example You have a file called: /src/java/yourCompany/domain/books/ADR-1-whyWeUseTheAISINInsteadOFISBN.adoc The task will collect this file and write another file called: /build/docs/_includes/ADR_includes.adoc &#8230;&#8203;which will look like this: include::../../../src/java/yourCompany/domain/books/ADR-1-whyWeUseTheAISINInsteadOFISBN.adoc[] Obviously, you&#8217;ll reap the most benefits if the task has several ADR files to collect. 😎 You can then include these files in your main documentation by using a single include: include::{targetDir}/docs/_includes/ADR_includes.adoc[] scripts/collectIncludes.gradle import static groovy.io.FileType.* import java.security.MessageDigest task collectIncludes( description: 'collect all ADRs as includes in one file', group: 'docToolchain' ) { doFirst { new File(targetDir, '_includes').mkdirs() } doLast { //let's search the whole project for files, not only the docs folder //could be a problem with node projects :-) //running as subproject? set scandir to main project if (project.name!=rootProject.name &amp;&amp; scanDir=='.') { scanDir = project(':').projectDir.path } if (docDir.startsWith('.')) { docDir = file(new File(projectDir, docDir).canonicalPath) } logger.info docToolchain&gt; docDir: ${docDir} logger.info docToolchain&gt; scanDir: ${scanDir} if (scanDir.startsWith('.')) { scanDir = file(new File(docDir, scanDir).canonicalPath) } else { scanDir = file(new File(scanDir, ).canonicalPath) } logger.info docToolchain&gt; scanDir: ${scanDir} logger.info docToolchain&gt; includeRoot: ${includeRoot} if (includeRoot.startsWith('.')) { includeRoot = file(new File(docDir, includeRoot).canonicalPath) } logger.info docToolchain&gt; includeRoot: ${includeRoot} File sourceFolder = scanDir println sourceFolder:  + sourceFolder.canonicalPath def collections = [:] sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^[A-Z]{3,}[-_].*[.](ad|adoc|asciidoc)$') { def type = file.name.replaceAll('^([A-Z]{3,})[-_].*$','\$1') if (!collections[type]) { collections[type] = [] } logger.info file:  + file.canonicalPath def fileName = (file.canonicalPath - scanDir.canonicalPath)[1..-1] if (file.name ==~ '^.*[Tt]emplate.*$') { logger.info ignore template file:  + fileName } else { if (file.name ==~ '^.*[A-Z]{3,}_includes.adoc$') { logger.info ignore generated _includes files:  + fileName } else { if ( fileName.startsWith('docToolchain') || fileName.replace(\\, /).matches('^.*/docToolchain/.*$')) { //ignore docToolchain as submodule } else { logger.info include corrected file:  + fileName collections[type] &lt;&lt; fileName } } } } } println targetFolder:  + (targetDir - docDir) logger.info targetDir - includeRoot:  + (targetDir - includeRoot) def pathDiff = '../' * ((targetDir - docDir) .replaceAll('^/','') .replaceAll('/$','') .replaceAll([^/],'').size()+1) logger.info pathDiff:  + pathDiff collections.each { type, fileNames -&gt; if (fileNames) { def outFile = new File(targetDir+'/_includes', type + '_includes.adoc') logger.info outFile.canonicalPath-sourceFolder.canonicalPath outFile.write(// this is autogenerated\n) fileNames.sort().each { fileName -&gt; outFile.append (include::../+pathDiff+fileName.replace(\\, /)+[]\n\n) } } } } } "
},

{
    "id": 24,
    "uri": "015_tasks/03_task_autobuildSite.html",
    "menu": "tasks",
    "title": "autobuildSite",
    "text": " Table of Contents autobuildSite .gravatar img { margin-left: 3px; border-radius: 4px; } autobuildSite 1 minute to read This script starts an endless loop which checks for changes to your docs source then re-runs the generateSite -task whenever it detects changes. The output will be logged to build/generateSite.log . bin/autobuildSite.bash #!/bin/bash DIR_TO_WATCH='src/' #COMMAND='rm -r build || true &amp;&amp; mkdir -p build/microsite/output/images/ &amp;&amp; ./dtcw generateSite 2&gt;&amp;1 | tee build/generateSite.log' COMMAND='mkdir -p build/microsite/output/images/ &amp;&amp; ./dtcw generateSite 2&gt;&amp;1 | tee build/generateSite.log' #execute first time cp src/docs/images/ready.png build/microsite/output/images/status.png #eval $COMMAND #wait for changes and execute while true ; do watch --no-title --chgexit ls -lR ${DIR_TO_WATCH} | sha1sum cp src/docs/images/building.png build/microsite/output/images/status.png eval $COMMAND cp src/docs/images/ready.png build/microsite/output/images/status.png sleep 6 done "
},

{
    "id": 25,
    "uri": "015_tasks/03_task_generateSite.html",
    "menu": "tasks",
    "title": "generateSite",
    "text": " Table of Contents generateSite Pages Templates and Style Landing Page Blog Search CI/CD See also Source .gravatar img { margin-left: 3px; border-radius: 4px; } generateSite 4 minutes to read When you have one document, the output of generateHTML might fit your needs. When your documentation grows, and you have several documents, you need a microsite which bundles all the information. The generateSite task uses jBake to create a static site with a Landingpage, Blog and Search. Pages The microsite is page oriented instead of document oriented. It is likely that you already organized your documents by chapter. Use these chapters as pages to create a great user experience. The arc42-template sources are a good example. In order to include a page in the microsite, add a meta-data header to it. page meta-data :jbake-menu: arc42 :jbake-title: Solution Strategy :jbake-order: 4 :jbake-type: page_toc :jbake-status: published :filename: 015_tasks/03_task_generateSite.adoc :imagesdir: ../../images :toc: [[section-solution-strategy]] === Solution Strategy jbake-menu the top-level menu for this page. Defaults to the top-level folder name of the adoc file within the docDir . jbake-title the title to be displayed in the drop-down top-level menu. Defaults to the first headline of the file. jbake-order give the drop-down entries a sort order. Defaults to a prefixed number of the file like 04 _filename.adoc or to the prefixed number of the second level folder name. jbake-type the page type. This controls which template is used to render the page. You will mostly use page for a full-width page or page_toc for a page with a table of contents (toc) rendered on the left. Defaults to page_toc . jbake-status draft or published . Only published pages will be rendered. Defaults to published for files with a jbake-order and draft for files without order or files prefixed with _ . filename needed for edit and feedback-links (coming soon). Defaults to the filename :-) ifndef this fixes the imagesdir according to the nesting level of your docs folder Defaults to the main docDir/images toc for :jbake-type: page_toc , you need this line to generate the toc. Note Start your pages with a == level headline. You can fix the level offset when you include the page in a larger document with include::chapter.adoc[leveloffset=+1] Templates and Style The jBake templates and CSS etc. is hidden away from you for convenience. The basic template use twitter Bootstrap 5 as CSS framework. Use the copyThemes task to copy all hidden jBake resources to your project. You can then remove the ones you don&#8217;t need and change those you want to change. Note copyThemes overwrites existing files, but your code is under version control, isn&#8217;t it? Landing Page Put a index.gsp page as landing page in src/site/templates . This landing page is plain HTML5 styled with twitter bootstrap. Header and footer is added by docToolchain. An example can be found through copyThemes or on github . Blog The microsite also contains a simple but powerful blog. Use it to inform your team about changes and architecture decision records (ADRs). To create a new blog post, create a new file in src/docs/blog/&lt;year&gt;/&lt;post-name&gt;.adoc with the following template: blog post template :jbake-title: &lt;title-of your post&gt; :jbake-date: &lt;date formatted as 2021-02-28&gt; :jbake-type: post :jbake-tags: &lt;blog, asciidoc&gt; :jbake-status: published :imagesdir: ../../images == {jbake-title} {jbake-author} {jbake-date} &lt;insert your text here&gt; Search The microsite does not bring its own local search. Instead, it only has a search input-field which can be used to link to another search engine. CI/CD When run in an automated build, you should set the environment variable DTC_HEADLESS to true or 1 to ensure that docToolchain will not ask to install the configured theme. It will simply assume that you do want to install it. To avoid the download of the theme with every build, you can copy the themes folder from $HOME/.doctoolchain/themes to the corresponding folder in your build container. See also previewSite Source scripts/generateSite.gradle import groovy.util.* import static groovy.io.FileType.* buildscript { repositories { maven { url mavenRepository } } dependencies { classpath 'org.asciidoctor:asciidoctorj-diagram:2.0.2' } } dependencies { jbake 'org.asciidoctor:asciidoctorj-diagram:2.0.2' jbake 'io.pebbletemplates:pebble:3.1.2' } apply plugin: 'org.jbake.site' apply plugin: 'org.gretty' def color = { color, text -&gt; def colors = [black: 30, red: 31, green: 32, yellow: 33, blue: 34, magenta: 35, cyan: 36, white: 37] return new String((char) 27) + [${colors[color]}m${text} + new String((char) 27) + [0m } jbake { version = '2.7.0-rc.2' srcDirName = ${targetDir}/microsite/tmp/site destDirName = ${targetDir}/microsite/output configuration['asciidoctor.option.requires'] = asciidoctor-diagram config.microsite.each { key, value -&gt; configuration['site.'+key-'config.microsite.'] = value //println 'site.'+key-'config.microsite.' + = + value } configuration['asciidoctor.attributes'] = [ sourceDir=${targetDir}, 'source-highlighter=prettify@', //'imagesDir=../images@', imagesoutDir=${targetDir}/microsite/output/images@, imagesDir=${config.microsite.contextPath}/images@, targetDir=${targetDir}, docDir=${docDir}, projectRootDir=${new File(docDir).canonicalPath}@, ] } bakePreview { port = '8046' } gretty { httpPort = ${config.microsite.previewPort?:8042} as Integer contextPath = ${config.microsite.contextPath} extraResourceBases = [${targetDir}/microsite/output] } task generateSite( group: 'docToolchain', description: 'generate a microsite using jBake.') { doLast { new File(${targetDir}/microsite/tmp).mkdirs() println created println new File(${targetDir}/microsite/tmp/).canonicalPath //copy internal theme println copy internal theme ${new File(projectDir, 'src/site').canonicalPath} copy { from('src/site') into(${targetDir}/microsite/tmp/site) } //check if a remote pdfTheme is defined def siteTheme = System.getenv('DTC_SITETHEME')?: def themeFolder = new File(projectDir, ../themes/ + siteTheme.md5()) try { if (siteTheme) { println use siteTheme $siteTheme //check if it is already installed if (!themeFolder.exists()) { if (System.getenv('DTC_HEADLESS')) { ant.yesno = y } else { println ${color 'green', \nTheme '$siteTheme' is not installed yet. } def input = ant.input(message:  ${color 'green', 'do you want me to download and install it to '} ${color 'green', ' ' + themeFolder.canonicalPath} ${color 'green', 'for you?'}\n, validargs: 'y,n', addproperty: 'yesno') } if (ant.yesno == y) { themeFolder.mkdirs() download { src siteTheme dest new File(themeFolder, 'siteTheme.zip') } copy { from zipTree(new File(themeFolder, 'siteTheme.zip')) into themeFolder } delete { delete new File(themeFolder, 'siteTheme.zip') } } else { println ${color 'green', \nI will continue without the theme for now... } siteTheme =  } } //copy external theme if (siteTheme) { copy { from(themeFolder) {} into(${targetDir}/microsite/tmp/) } //check if the config has to be updated // check if config still contains /** microsite **/ def configFile = new File(docDir, mainConfigFile) def configFileText = configFile.text if (configFileText.contains(/** start:microsite **/)) { def configFragment = new File(targetDir,'/microsite/tmp/site/configFragment.groovy') if (configFragment.exists()) { println ${color 'green',  It seems that this theme is used for the first time in this project. Let's configure it! If you are unsure, change these settings later in your config file $configFile.canonicalPath } def comment =  def conf =  def example =  def i = 0 configFragment.eachLine { line -&gt; if (line.trim()) { if (line.startsWith(//)) { conf +=   + line + \n def tmp = line[2..-1].trim() comment += color('green', tmp) + \n if (tmp.toLowerCase().startsWith(example)) { example = tmp.replaceAll([^ ]* , ) } } else { //only prompt if there is something to prompt if (line.contains(##)) { def property = line.replaceAll([ =].*, ) if (!example) { example = config.microsite[property] } comment = color('blue', $property) + \n + comment if (example) { ant.input(message: comment, addproperty: 'res' + i, defaultvalue: example) } else { ant.input(message: comment, addproperty: 'res' + i) } (comment, example) = [, ] line = line.replaceAll(##.+##, ant['res' + i]) conf +=   + line + \n i++ } else { conf +=   + line + \n } } } else { conf += \n } } configFile.write(configFileText.replaceAll((?sm)/[*][*] start:microsite [*][*]/.*/[*][*] end:microsite [*][*]/, %%marker%%).replace(%%marker%%, conf)) println color('green', config written\ntry\n ./dtcw generateSite previewSite\nto see your microsite!) } //copy the dummy docs (blog, landing page) to the project repository copy { from(new File(themeFolder, 'site/doc')) {} into(new File(docDir, inputPath)) } } } } } catch (Exception e) { println color('red', e.message) if (e.message.startsWith(Not Found)) { themeFolder.deleteDir() throw new GradleException(Couldn't find theme. Did you specify the right URL?\n+e.message) } else { throw new GradleException(e.message) } } //copy project theme if (config.microsite.siteFolder) { def projectTheme = new File(new File(docDir, inputPath), config.microsite.siteFolder) println copy project theme ${projectTheme.canonicalPath} copy { from(projectTheme) {} into(${targetDir}/microsite/tmp/site) } } //copy docs copy { from(new File(docDir, inputPath)) {} into(${targetDir}/microsite/tmp/site/doc) } //fix MetaData-Header File sourceFolder = new File(targetDir, '/microsite/tmp/site/doc') logger.info(sourceFolder:  + sourceFolder.canonicalPath) sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^.*(ad|adoc|asciidoc)$') { if (file.name.startsWith(_)) { //ignore } else { def origText = file.text //parse jbake attributes def text =  def jbake = [ status: published, order: -1, type: 'page_toc' ] def parseAttribs = true def beforeToc =  origText.eachLine { line -&gt; if (parseAttribs &amp;&amp; line.startsWith(:jbake)) { line = (line - :jbake-).split(: +, 2) jbake[line[0]] = line[1] } else { if (line.startsWith([)) { // stop parsing jBake-attribs when a [source] - block starts which might contain those attribs as example parseAttribs = false } text += line+\n //there are some attributes which have to be set before the toc if (line.startsWith(:toc) ) { beforeToc += line+\n } } } def name = file.canonicalPath - (sourceFolder.canonicalPath+File.separator) if (File.separator=='\\') { name = name.split(\\\\) } else { name = name.split(/) } if (name.size()&gt;1) { if (!jbake.menu) { jbake.menu = name[0] if (jbake.menu ==~ /[0-9]+[-_].*/) { jbake.menu = jbake.menu.split([-_], 2)[1] } } def docname = name[-1] if (docname ==~ /[0-9]+[-_].*/) { jbake.order = docname.split([-_],2)[0] docname = docname.split([-_],2)[1] } if (name.size() &gt; 2) { if ((jbake.order as Integer)==0) { // let's take the order from the second level dir or file and not the file def secondLevel = name[1] if (secondLevel ==~ /[0-9]+[-_].*/) { jbake.order = secondLevel.split([-_],2)[0] } } else { if ((jbake.order as Integer) &gt; 0) { // } else { jbake.status = draft } } } if (jbake.order==-1 &amp;&amp; docname.startsWith('index')) { jbake.order = 0 jbake.status = published } // news blog if (jbake.order==-1 &amp;&amp; jbake.type=='post') { jbake.order = 0 try { jbake.order = Date.parse(yyyy-MM-dd, jbake.date).time / 100000 } catch ( Exception e) { System.out.println unparsable date ${jbake.date} in $name } jbake.status = published } def leveloffset = 0 text.eachLine { line -&gt; if (!jbake.title &amp;&amp; line ==~ ^=+ .*) { jbake.title = (line =~ ^=+ (.*))[0][1] def level = (line =~ ^(=+) .*)[0][1] if (level===) { leveloffset = 1 } } } if (!jbake.title) { jbake.title = docname } if (leveloffset==1) { //leveloffset needed // we always start with == not with = text = text.replaceAll((?ms)^(=+) , '$1= ') } def header = '' jbake.each { key, value -&gt; if (key=='order') { header += :jbake-${key}: ${value as Integer}\n } else { header += :jbake-${key}: ${value}\n } } file.write(header + \n$beforeToc\n\n:toc: left\n\n++++\n&lt;!-- endtoc --&gt;\n++++\n + text, utf-8) } } } } /** println =*80 println (new File(${targetDir}/microsite/tmp/site/doc).canonicalPath) new File(${targetDir}/microsite/tmp/site/doc).eachFileRecurse { file -&gt; if (file.name.endsWith('.adoc')) { System.out.println &gt;&gt; +file.name } } **/ } } task previewSite( group: 'docToolchain', dependsOn: [], description: 'start a little webserver to preview your Microsite', ) { if (new File(${targetDir}/microsite/output).exists()) { finalizedBy 'jettyRun' } doLast { if (new File(${targetDir}/microsite/output).exists()) { // everything is fine } else { throw new GradleException(&gt; &gt; Microsite not built yet, please run './dtcw generateSite' first &gt;) } } } task copyImages(type: Copy) { config.imageDirs.each { imageDir -&gt; from(new File (new File(docDir, inputPath),imageDir)) {} logger.info ('imageDir: '+imageDir) into(${targetDir}/microsite/output/images) } config.resourceDirs.each { resource -&gt; from(new File(file(srcDir),resource.source)) logger.info ('resource: '+resource.source) into(${targetDir}/microsite/output/ + resource.target) } } bake.dependsOn copyImages generateSite.finalizedBy bake "
},

{
    "id": 26,
    "uri": "015_tasks/03_task_exportOpenApi.html",
    "menu": "tasks",
    "title": "exportOpenAPI",
    "text": " Table of Contents exportOpenAPI About This Task Configuration Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportOpenAPI 1 minute to read About This Task This task exports an OpenAPI Specification definition yaml file to a AsciiDoc document. Currently this task depends on OpenAPI Generator (v4.3.1) and its gradle plugin . Configuration Config.groovy // Configuration for OpenAPI related task openApi = [:] // 'specFile' is the name of OpenAPI specification yaml file. Tool expects this file inside working dir (as a filename or relative path with filename) // 'infoUrl' and 'infoEmail' are specification metadata about further info related to the API. By default this values would be filled by openapi-generator plugin placeholders // openApi.with { specFile = 'src/docs/petstore-v2.0.yaml' // i.e. 'petstore.yaml', 'src/doc/petstore.yaml' infoUrl = 'https://my-api.company.com' infoEmail = 'info@company.com' } Source exportOpenApi.gradle task exportOpenApi ( type: org.openapitools.generator.gradle.plugin.tasks.GenerateTask, group: 'docToolchain', description: 'exports OpenAPI specification to the asciidoc file') { if (!specFile) { logger.info(\n---&gt; OpenAPI specification file not found in Config.groovy (https://doctoolchain.github.io/docToolchain/#_exportopenapi)) return } else { logger.info(Found OpenAPI specification in Config.groovy) } outputs.upToDateWhen { false } outputs.cacheIf { false } generatorName = 'asciidoc' outputDir = ${targetDir}/OpenAPI.toString() inputSpec = ${docDir}/${specFile} // plugin is not able to find file if inputPath is defined as '.' logger.debug(\n=====================\nProject Config:\n=====================) logger.debug(Docdir: ${docDir}) logger.debug(Target: ${targetDir}) logger.info(\n=====================\nOpenAPI Config:\n=====================) logger.info(Specification file: ${specFile}) logger.info(inputSpec: ${inputSpec}) logger.info(outputDir: ${outputDir}\n) additionalProperties = [ infoEmail:${config.openApi.infoEmail}, infoUrl:${config.openApi.infoUrl} ] } "
},

{
    "id": 27,
    "uri": "015_tasks/03_task_exportJiraIssues.html",
    "menu": "tasks",
    "title": "exportJiraIssues",
    "text": " Table of Contents exportJiraIssues About This Task Configuration Further Reading Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportJiraIssues 2 minutes to read About This Task This task exports all issues for a given query or queries from Jira as either an AsciiDoc table or an Excel file. The configuration for this task can be found within Config.gradle ( gradle.properties can be used as a fallback). Username/password is deprecated, so you need to use username/API-token instead. An API-token can be created through https://id.atlassian.com/manage/api-tokens . We recommend that you keep username and API-token out of your GitHub repository, and instead pass them as environment variables to docToolchain. Configuration Jira configuration support list requests to Jira where results of each requests will be saved in a file with specifies filename. Flags saveAsciidoc &amp; saveExcel allow you to easily configure the format in which results should be saved. Important The old configuration was based on the single Jira query is deprecated (single 'jql' parameter). Support for it will be removed in the near future. Please migrate to the new configuration which allows multiple Jira queries. Config.groovy // Configuration for Jira related tasks jira = [:] jira.with { // endpoint of the JiraAPI (REST) to be used api = 'https://your-jira-instance' /* WARNING: It is strongly recommended to store credentials securely instead of commiting plain text values to your git repository!!! Tool expects credentials that belong to an account which has the right permissions to read the JIRA issues for a given project. Credentials can be used in a form of: - passed parameters when calling script (-PjiraUser=myUsername -PjiraPass=myPassword) which can be fetched as a secrets on CI/CD or - gradle variables set through gradle properties (uses the 'jiraUser' and 'jiraPass' keys) Often, Jira &amp; Confluence credentials are the same, in which case it is recommended to pass CLI parameters for both entities as -Pusername=myUser -Ppassword=myPassword */ // the key of the Jira project project = 'PROJECTKEY' // the format of the received date time values to parse dateTimeFormatParse = yyyy-MM-dd'T'H:m:s.SSSz // i.e. 2020-07-24'T'9:12:40.999 CEST // the format in which the date time should be saved to output dateTimeFormatOutput = dd.MM.yyyy HH:mm:ss z // i.e. 24.07.2020 09:02:40 CEST // the label to restrict search to label = 'label1' // Legacy settings for Jira query. This setting is deprecated &amp; support for it will soon be completely removed. Please use JiraRequests settings jql = project='%jiraProject%' AND labels='%jiraLabel%' ORDER BY priority DESC, duedate ASC // Base filename in which Jira query results should be stored resultsFilename = 'JiraTicketsContent' saveAsciidoc = true // if true, asciidoc file will be created with *.adoc extension saveExcel = true // if true, Excel file will be created with *.xlsx extension // Output folder for this task inside main outputPath resultsFolder = 'JiraRequests' /* List of requests to Jira API: These are basically JQL expressions bundled with a filename in which results will be saved. User can configure custom fields IDs and name those for column header, i.e. customfield_10026:'Story Points' for Jira instance that has custom field with that name and will be saved in a coloumn named Story Points */ requests = [ new JiraRequest( filename:File1_Done_issues, jql:project='%jiraProject%' AND status='Done' ORDER BY duedate ASC, customfields: [customfield_10026:'Story Points'] ), new JiraRequest( filename:'CurrentSprint', jql:project='%jiraProject%' AND Sprint in openSprints() ORDER BY priority DESC, duedate ASC, customfields: [customfield_10026:'Story Points'] ), ] } @groovy.transform.Immutable class JiraRequest { String filename //filename (without extension) of the file in which JQL results will be saved. Extension will be determined automatically for Asciidoc or Excel file String jql // Jira Query Language syntax Map&lt;String,String&gt; customfields // map of customFieldId:displayName values for Jira fields which don't have default names, i.e. customfield_10026:StoryPoints } Further Reading Read the Living Documents for Agile Projects blog post. Source exportJiraIssues.gradle task exportJiraIssues( description: 'exports all jira issues from a given search', group: 'docToolchain' ) { doLast { final String taskSubfolderName = config.jira.resultsFolder final File targetFolder = new File(targetDir + File.separator + taskSubfolderName) if (!targetFolder.exists()) targetFolder.mkdirs() logger.debug(Output folder for 'exportJiraIssues' task is: '${targetFolder}') // map configuration from Config.groovy to existing variables for compatibility with naming of Jira settings in gradle.properties def jiraRoot = config.jira.api def jiraProject = config.jira.project def jiraLabel = config.jira.label def jiraResultsFilename = config.jira.resultsFilename def jiraDateTimeFormatParse = config.jira.dateTimeFormatParse def jiraDateTimeOutput = config.jira.dateTimeFormatOutput def defaultFields = 'priority,created,resolutiondate,summary,assignee,status' def jira = new groovyx.net.http.RESTClient(jiraRoot + '/rest/api/2/') jira.encoderRegistry = new groovyx.net.http.EncoderRegistry(charset: 'utf-8') def headers = [ 'Authorization': Basic  + config.jira.credentials, 'Content-Type' : 'application/json; charset=utf-8' ] def jiraRequests = config.jira.requests if (config.jira.jql) { logger.warn(&gt;&gt;&gt;Found legacy Jira requests. Please migrate to the new Jira configuration ASAP. Old config with jql will be removed soon) writeAsciiDocFileForLegacyConfiguration(targetFolder, jira, headers, config.jira) } jiraRequests.each {rq -&gt; logger.quiet(Request to Jira API for '${rq.filename}' with query: '${rq.jql}') def allHeaders = ${defaultFields},${rq.customfields.values().join(,)} def allFieldIds = ${defaultFields},${rq.customfields.keySet().join(,)} logger.quiet(Preparing headers for default &amp; custom fields: ${allHeaders}) logger.quiet(Preparing field IDs for default &amp; custom fields: ${allFieldIds}) // Save AsciiDoc file if (config.jira.saveAsciidoc) { def extension = 'adoc' jiraResultsFilename = ${rq.filename}.${extension} logger.info(Results will be saved in '${rq.filename}.${extension}' file) def jiraDataAsciidoc = new File(targetFolder, ${rq.filename}.${extension}) jiraDataAsciidoc.write(.${rq.filename}\n, 'utf-8') jiraDataAsciidoc.append(|=== \n) // AsciiDoc table headers (custom fields map needs values here) jiraDataAsciidoc.append(|Key , 'utf-8') allHeaders.split(,).each {field -&gt; jiraDataAsciidoc.append(|${field.capitalize()} , 'utf-8') } jiraDataAsciidoc.append(\n, 'utf-8') jira.get(path: 'search', query: ['jql' : rq.jql.replaceAll('%jiraProject%', jiraProject).replaceAll('%jiraLabel%', jiraLabel), 'maxResults': 1000, fields: ${allFieldIds} ], headers: headers ).data.issues.each { issue -&gt; //logger.quiet(&gt;&gt; Whole issue ${issue.key}:\n ${issue.fields}) jiraDataAsciidoc.append(| ${jiraRoot}/browse/${issue.key}[${issue.key}] , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.priority.name} , 'utf-8') jiraDataAsciidoc.append(| ${Date.parse(jiraDateTimeFormatParse, issue.fields.created).format(jiraDateTimeOutput)} , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.resolutiondate ? Date.parse(jiraDateTimeFormatParse, issue.fields.resolutiondate).format(jiraDateTimeOutput) : ''} , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.summary} , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.assignee ? issue.fields.assignee.displayName : 'not assigned'}, 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.status.name} , 'utf-8') rq.customfields.each { field -&gt; def foundCustom = issue.fields.find {it.key == field.key} //logger.quiet(Examining issue '${issue.key}' for custom field '${field.key}' has found: '${foundCustom}') jiraDataAsciidoc.append(| ${foundCustom ? foundCustom.value : '-'}\n, 'utf-8') } } jiraDataAsciidoc.append(|=== \n) } else { logger.quiet(Set saveAsciidoc=true in '${mainConfigFile}' to save results in AsciiDoc file) } // Save Excel file if (config.jira.saveExcel) { def extension = 'xlsx' jiraResultsFilename = ${rq.filename}.${extension} logger.quiet(&gt;&gt; Results will be saved in '${rq.filename}.${extension}' file) //def jiraDataAsciidoc = new File(targetFolder, ${rq.filename}.${extension}) def jiraDataXls = new File(targetFolder, jiraResultsFilename) def jiraFos = new FileOutputStream(jiraDataXls) Workbook wb = new XSSFWorkbook(); CreationHelper hyperlinkHelper = wb.getCreationHelper(); def sheetName = ${rq.filename} def ws = wb.createSheet(sheetName) String rgbS = A7A7A7 byte[] rgbB = Hex.decodeHex(rgbS) XSSFColor color = new XSSFColor(rgbB, null) //IndexedColorMap has no usage until now. So it can be set null. XSSFCellStyle headerCellStyle = (XSSFCellStyle) wb.createCellStyle() headerCellStyle.setFillForegroundColor(color) headerCellStyle.setFillPattern(FillPatternType.SOLID_FOREGROUND) def titleRow = ws.createRow(0); int cellNumber = 0; titleRow.createCell(cellNumber).setCellValue(Key) allHeaders.split(,).each {field -&gt; titleRow.createCell(++cellNumber).setCellValue(${field.capitalize()}) } def lastRow = titleRow.getRowNum() titleRow.setRowStyle(headerCellStyle) jira.get(path: 'search', query: ['jql' : rq.jql.replaceAll('%jiraProject%', jiraProject).replaceAll('%jiraLabel%', jiraLabel), 'maxResults': 1000, fields: ${allFieldIds} ], headers: headers ).data.issues.each { issue -&gt; int cellPosition = 0 def row = ws.createRow(++lastRow) Hyperlink link = hyperlinkHelper.createHyperlink(HyperlinkType.URL) link.setAddress(${jiraRoot}/browse/${issue.key}) Cell cellWithUrl = row.createCell(cellPosition) cellWithUrl.setCellValue(${issue.key}) cellWithUrl.setHyperlink(link) row.createCell(++cellPosition).setCellValue(${issue.fields.priority.name}) row.createCell(++cellPosition).setCellValue(${Date.parse(jiraDateTimeFormatParse, issue.fields.created).format(jiraDateTimeOutput)}) row.createCell(++cellPosition).setCellValue(${issue.fields.resolutiondate ? Date.parse(jiraDateTimeFormatParse, issue.fields.resolutiondate).format(jiraDateTimeOutput) : ''}) row.createCell(++cellPosition).setCellValue(${issue.fields.summary}) row.createCell(++cellPosition).setCellValue(${issue.fields.assignee ? issue.fields.assignee.displayName : ''}) row.createCell(++cellPosition).setCellValue(${issue.fields.status.name}) // Custom fields rq.customfields.each { field -&gt; def position = ++cellPosition def foundCustom = issue.fields.find {it.key == field.key} row.createCell(position).setCellValue(${foundCustom ? foundCustom.value : '-'}) } } // set jira issue key column fits the content width for(int colNum = 0; colNum&lt;allHeaders.size()+1;colNum++) { ws.autoSizeColumn(colNum) } // Set summary column width slightly wider but fixed size, so it doesn't change with every summary update ws.setColumnWidth(4, 25*384) wb.write(jiraFos) } else { logger.quiet(Set saveExcel=true in '${mainConfigFile}' to save results in Excel file) } } } } // This method can be removed when support for legacy Jira configuration is gone def writeAsciiDocFileForLegacyConfiguration(def targetFolder, def restClient, def headers, def jiraConfig) { def resultsFilename = ${jiraConfig.resultsFilename}_legacy.adoc def openIssues = new File(targetFolder, ${resultsFilename}) openIssues.write(.Table {Title}\n, 'utf-8') openIssues.append(|=== \n) openIssues.append(|Key |Priority |Created | Assignee | Summary\n, 'utf-8') def legacyJql = jiraConfig.jql.replaceAll('%jiraProject%', config.jira.project).replaceAll('%jiraLabel%', config.jira.label) println (Results for legacy query '${legacyJql}' will be saved in '${resultsFilename}' file) restClient.get(path: 'search', query: ['jql' : legacyJql, 'maxResults': 1000, 'fields' : 'created,resolutiondate,priority,summary,timeoriginalestimate, assignee' ], headers: headers ).data.issues.each { issue -&gt; openIssues.append(| ${jiraRoot}/browse/${issue.key}[${issue.key}] , 'utf-8') openIssues.append(| ${issue.fields.priority.name} , 'utf-8') openIssues.append(| ${Date.parse(jiraConfig.dateTimeFormatParse, issue.fields.created).format(jiraConfig.dateTimeFormatOutput)} , 'utf-8') openIssues.append(| ${issue.fields.assignee ? issue.fields.assignee.displayName : 'not assigned'}, 'utf-8') openIssues.append(| ${issue.fields.summary} , 'utf-8') } openIssues.append(|=== \n) } "
},

{
    "id": 28,
    "uri": "015_tasks/03_task_generatePDF.html",
    "menu": "tasks",
    "title": "generatePDF",
    "text": " Table of Contents generatePDF Source .gravatar img { margin-left: 3px; border-radius: 4px; } generatePDF 2 minutes to read This task makes use of the asciidoctor-pdf plugin to render your documents as a pretty PDF file. The file will be written to build/pdf . Note The used plugin is still in alpha status, but the results are already quite good. If you want to use another way to create a PDF, use PhantomJS for instance and script it. The PDF is generated directly from your AsciiDoc sources without the need of an intermediate format or other tools. The result looks more like a nicely rendered book than a print-to-pdf HTML page. It is very likely that you need to theme you PDF - change colors, fonts, page header, and footer. This can be done by creating a custom-theme.yml file. As a starting point, copy the file src/docs/pdfTheme/custom-theme.yml from docToolchain to your project and reference it from your main .adoc`file via setting the `:pdf-stylesdir: . For instance, insert :pdf-stylesdir: ../pdfTheme at the top of your document to reference the custom-theme.yml from the /pdfTheme folder. Documentation on how to modify a theme can be found in the asciidoctor-pdf theming guide . Tip Blog-Post: Beyond HTML Source AsciiDocBasics.gradle task generatePDF ( type: AsciidoctorTask, group: 'docToolchain', description: 'use pdf as asciidoc backend') { attributes ( 'plantUMLDir' : file(${docDir}/${config.outputPath}/pdf/images/plantUML/).path, ) attributes ( 'data-uri': 'true', 'plantUMLDir' : file(${docDir}/${config.outputPath}/images/).path, 'imagesoutdir' : file(${docDir}/${config.outputPath}/images/).path ) def sourceFilesPDF = sourceFiles.findAll { 'pdf' in it.formats } // onlyIf { // sourceFilesPDF // } sources { sourceFilesPDF.each { include it.file logger.info it.file } } backends = ['pdf'] /** //check if a remote pdfTheme is defined def pdfTheme = System.getenv('DTC_PDFTHEME') def themeFolder = pdfTheme.md5() if (pdfTheme) { //check if it is already installed //TODO: finish this... } **/ doFirst { if (sourceFilesPDF.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type PDF. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } "
},

{
    "id": 29,
    "uri": "015_tasks/03_task_exportVisio.html",
    "menu": "tasks",
    "title": "exportVisio",
    "text": " Table of Contents exportVisio About This Task Important Information About This Task Further Reading and Resources Source .gravatar img { margin-left: 3px; border-radius: 4px; } exportVisio 1 minute to read About This Task This task searches for Visio files in the /src/docs folder then exports all diagrams and element notes to /src/docs/images/visio and /src/docs/visio . Images are stored as /images/visio/[filename]-[pagename].png . Notes are stored as /visio/[filename]-[pagename].adoc You can specify a filename to export notes to by starting any comment with {adoc:[filename].adoc} . It will then be written to /visio/[filename].adoc . Important Information About This Task Currently, only Visio files stored directly in /src/docs are supported. All others will export to the wrong location. Before running this task, close any open Visio instance. Further Reading and Resources Issue #112 . Source exportVisio.gradle task exportVisio( dependsOn: [streamingExecute], description: 'exports all diagrams and notes from visio files', group: 'docToolchain' ) { doLast { //make sure path for notes exists //and remove old notes new File(docDir, 'src/docs/visio').deleteDir() //also remove old diagrams new File(docDir, 'src/docs/images/visio').deleteDir() //create a readme to clarify things def readme = This folder contains exported diagrams and notes from visio files. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportVisio` to re-export files  new File(docDir, 'src/docs/images/visio/.').mkdirs() new File(docDir, 'src/docs/images/visio/readme.ad').write(readme) new File(docDir, 'src/docs/visio/.').mkdirs() new File(docDir, 'src/docs/visio/readme.ad').write(readme) def sourcePath = new File(docDir, 'src/docs/.').canonicalPath def scriptPath = new File(projectDir, 'scripts/VisioPageToPngConverter.ps1').canonicalPath powershell ${scriptPath} -SourcePath ${sourcePath}.executeCmd() } } scripts/VisioPageToPngConverter.ps1 # Convert all pages in all visio files in the given directory to png files. # A Visio windows might flash shortly. # The converted png files are stored in the same directory # The name of the png file is concatenated from the Visio file name and the page name. # In addtion all the comments are stored in adoc files. # If the Viso file is named MyVisio.vsdx and the page is called FirstPage # the name of the png file will be MyVisio-FirstPage.png and the comment will # be stored in MyVisio-FirstPage.adoc. # But for the name of the adoc files there is an alternative. It can be given in the first # line of the comment. If it is given in the comment it has to be given in curly brackes # with the prefix adoc:, e.g. {adoc:MyCommentFile.adoc} # Prerequisites: Viso and PowerShell has to be installed on the computer. # Parameter: SourcePath where visio files can be found # Example powershell VisoPageToPngConverter.ps1 -SourcePath c:\convertertest\ Param ( [Parameter(Mandatory=$true,ValueFromPipeline=$true,Position=0)] [Alias('p')][String]$SourcePath ) Write-Output starting to export visio If (!(Test-Path -Path $SourcePath)) { Write-Warning The path $SourcePath does not exist or is not accessible, please input the correct path. Exit } # Extend the source path to get only Visio files of the given directory and not in subdircetories If ($SourcePath.EndsWith(\)) { $SourcePath = $SourcePath } Else { $SourcePath = $SourcePath\ } $VisioFiles = Get-ChildItem -Path $SourcePath* -Recurse -Include *.vsdx,*.vssx,*.vstx,*.vxdm,*.vssm,*.vstm,*.vsd,*.vdw,*.vss,*.vst If(!($VisioFiles)) { Write-Warning There are no Visio files in the path $SourcePath. Exit } $VisioApp = New-Object -ComObject Visio.Application $VisioApp.Visible = $false # Extract the png from all the files in the folder Foreach($File in $VisioFiles) { $FilePath = $File.FullName Write-Output found $FilePath . $FileDirectory = $File.DirectoryName # Get the folder containing the Visio file. Will be used to store the png and adoc files $FileBaseName = $File.BaseName -replace '[ :/\\*?|&lt;&gt;]','-' # Get the filename to be used as part of the name of the png and adoc files Try { $Document = $VisioApp.Documents.Open($FilePath) $Pages = $VisioApp.ActiveDocument.Pages Foreach($Page in $Pages) { # Create valid filenames for the png and adoc files $PngFileName = $Page.Name -replace '[ :/\\*?|&lt;&gt;]','-' $PngFileName = $FileBaseName-$PngFileName.png $AdocFileName = $PngFileName.Replace(.png, .adoc) #TODO: this needs better logic Write-Output($SourcePath\images\visio\$PngFileName) $Page.Export($SourcePath\images\visio\$PngFileName) $AllPageComments =  ForEach($PageComment in $Page.Comments) { # Extract adoc filename from comment text if the syntax is valid # Remove the filename from the text and save the comment in a file with a valid name $EofStringIndex = $PageComment.Text.IndexOf(.adoc}) if ($PageComment.Text.StartsWith({adoc) -And ($EofStringIndex -gt 6)) { $AdocFileName = $PageComment.Text.Substring(6, $EofStringIndex -1) $AllPageComments += $PageComment.Text.Substring($EofStringIndex + 6) } else { $AllPageComments += $PageComment.Text+`n } } If ($AllPageComments) { $AdocFileName = $AdocFileName -replace '[:/\\*?|&lt;&gt;]','-' #TODO: this needs better logic $stream = [System.IO.StreamWriter] $SourcePath\visio\$AdocFileName $stream.WriteLine($AllPageComments) $stream.close() } } $Document.Close() } Catch { if ($Document) { $Document.Close() } Write-Warning One or more visio page(s) in file $FilePath have been lost in this converting. Write-Warning Error was: $_ } } $VisioApp.Quit() "
},

{
    "id": 30,
    "uri": "015_tasks/03_task_convertToEpub.html",
    "menu": "tasks",
    "title": "convertToEpub",
    "text": " Table of Contents convertToEpub At a Glance Dependency About This Task Further Reading Source .gravatar img { margin-left: 3px; border-radius: 4px; } convertToEpub 1 minute to read At a Glance Dependency generateDocBook About This Task This task uses pandoc to convert the DocBook output from AsciiDoctor to ePub. This publishes the output as an eBook which can be read using any eBook reader. The resulting file can be found in build/docs/epub . Further Reading Turn your Document into an Audio-Book blog post. Source pandoc.gradle task convertToEpub ( group: 'docToolchain', description: 'converts file to .epub via pandoc. Needs pandoc installed.', type: Exec ) { // All files with option `epub` in config.groovy is converted to docbook and then to epub. def sourceFilesEpub = sourceFiles.findAll { 'epub' in it.formats } sourceFilesEpub.each { def sourceFile = it.file.replace('.adoc', '.xml') def targetFile = sourceFile.replace('.xml', '.epub') workingDir $targetDir/docbook executable = pandoc args = ['-r','docbook', '-t','epub', '-o',../epub/$targetFile, sourceFile] } doFirst { new File($targetDir/epub/).mkdirs() } } "
},

{
    "id": 31,
    "uri": "015_tasks/03_task_prependFilename.html",
    "menu": "tasks",
    "title": "prependFilename",
    "text": " Table of Contents prependFilename .gravatar img { margin-left: 3px; border-radius: 4px; } prependFilename 1 minute to read When Asciidoctor renders a file, the file context only knows the name of the top-level AsciiDoc file but an include file doesn&#8217;t know that it is being included. It will simply get the name of the master file and has no chance to get his own names as attribute. This task simply crawls through all AsciiDoc files and prepends the name of the current file like this: :filename: 015_tasks/03_task_prependFilename.adoc This way, each file can get its own file name. This enables features like the inclusion of file contributors (see exportContributors-task). Note The task skips all files named config.* , _config.* , feedback.* and _feedback.* . scripts/prependFilename.gradle import static groovy.io.FileType.* task prependFilename( description: 'crawls through all AsciiDoc files and prepends the name of the current file', group: 'docToolchain helper', ) { doLast { File sourceFolder = new File(${docDir}/${inputPath}) println(sourceFolder:  + sourceFolder.canonicalPath) sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^.*(ad|adoc|asciidoc)$') { if (file.name.split('[.]')[0] in [feedback, _feedback, config, _config]) { println skipped +file.name } else { def text = file.getText('utf-8') def name = file.canonicalPath - sourceFolder.canonicalPath name = name.replace(\\, /).replaceAll(^/, ) if (text.contains(:filename:)) { text = text.replaceAll(:filename:.*, :filename: $name) println updated +name } else { text = :filename: $name\n + text println added +name } file.write(text,'utf-8') } } } } } "
},

{
    "id": 32,
    "uri": "015_tasks/03_task_generateHTML.html",
    "menu": "tasks",
    "title": "generateHTML",
    "text": " Table of Contents generateHTML Text based Diagrams Source .gravatar img { margin-left: 3px; border-radius: 4px; } generateHTML 3 minutes to read This is the standard AsciiDoctor generator which is supported out of the box. The result is written to build/html5 . The HTML files need the images folder to be in the same directory to display correctly. Note If you would like to have a single-file HTML as result, you can configure AsciiDoctor to store the images inline as data-uri . Just set :data-uri: in the config of your AsciiDoc file. But be warned - such a file can become very big easily and some browsers might get into trouble rendering them. https://rdmueller.github.io/single-file-html/ Text based Diagrams For docToolchain, it is configured to use the asciidoctor-diagram plugin which is used to create PlantUML diagrams. The plugin also supports a bunch of other text based diagrams, but PlantUML is the most used. To use it, just specify your PlantUML code like this: .example diagram [plantuml, {plantUMLDir}demoPlantUML, png] # (1) ---- class BlockProcessor class DiagramBlock class DitaaBlock class PlantUmlBlock BlockProcessor &lt;|-- DiagramBlock DiagramBlock &lt;|-- DitaaBlock DiagramBlock &lt;|-- PlantUmlBlock ---- The element of this list specifies the diagram tool plantuml to be used. The second element is the name of the image to be created and the third specifies the image type. Note The {plantUMLDir} ensures that PlantUML also works for the generatePDF task. Without it, generateHTML works fine, but the PDF will not contain the generated images. Important Make sure to specify a unique image name for each diagram, otherwise they will overwrite each other. The above example renders as Figure 1. example diagram If you want to control the size of the generated diagram in the output, you can configure the width attribute (in pixels) or scale attribute (floating point ratio) passed to asciidoctor-diagram . For example, if you take the example diagram above and change the declaration to one of the below versions [plantuml, target={plantUMLDir}demoPlantUMLWidth, format=png, width=250] # rest of the diagram definition [plantuml, target={plantUMLDir}demoPlantUMLScale, format=png, scale=0.75] # rest of the diagram definition it will render like this: Figure 2. example diagram (with specified width) Figure 3. example diagram (with specified scale) Note PlantUML needs Graphviz dot installed to work. If you can&#8217;t install it, you can use the Java based version of the dot library. Just add !pragma graphviz_dot smetana as the first line of your diagram definition. This is still an experimental feature, but already works quite well! https://rdmueller.github.io/plantuml-without-graphviz/ Tip Blog-Posts: PlantUML with Gradle , plantUML with Asciidoctor-pdf , plantUML revisited , How to use PlantUML without Graphviz Source AsciiDocBasics.gradle task generateHTML ( type: AsciidoctorTask, group: 'docToolchain', description: 'use html5 as asciidoc backend') { attributes ( 'plantUMLDir' : file(${docDir}/${config.outputPath}/html5).toURI().relativize(new File(${docDir}/${config.outputPath}/html5/plantUML/).toURI()).getPath(), ) // specify output folder explicitly to avoid cleaning targetDir from other generated content outputDir = file(targetDir + '/html5/') separateOutputDirs(false) def sourceFilesHTML = sourceFiles.findAll { 'html' in it.formats } // onlyIf { // sourceFilesHTML // } sources { sourceFilesHTML.each { include it.file logger.info it.file } } resources { config.imageDirs.each { imageDir -&gt; from(new File(file(srcDir),imageDir)) logger.info ('imageDir: '+imageDir) into './images' } config.resourceDirs.each { resource -&gt; from(new File(file(srcDir),resource.source)) logger.info ('resource: '+resource.source) into resource.target } } backends = ['html5'] doFirst { if (sourceFilesHTML.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type HTML. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } "
},

{
    "id": 33,
    "uri": "015_tasks/03_task_generateDocBook.html",
    "menu": "tasks",
    "title": "generateDocbook",
    "text": " Table of Contents generateDocbook Source .gravatar img { margin-left: 3px; border-radius: 4px; } generateDocbook 1 minute to read This is only a helper task - it generates the intermediate format for convertToDocx &lt;&lt;&gt;&gt; and convertToEpub . Source AsciiDocBasics.gradle task generateDocbook ( type: AsciidoctorTask, group: 'docToolchain', description: 'use docbook as asciidoc backend') { def sourceFilesDOCBOOK = sourceFiles.findAll { 'docbook' in it.formats } // onlyIf { // sourceFilesDOCBOOK // } sources { sourceFilesDOCBOOK.each { include it.file logger.info it.file } } backends = ['docbook'] doFirst { if (sourceFilesDOCBOOK.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type docbook. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } "
},

{
    "id": 34,
    "uri": "015_tasks/03_task_downloadTemplate.html",
    "menu": "tasks",
    "title": "downloadTemplate",
    "text": " Table of Contents downloadTemplate .gravatar img { margin-left: 3px; border-radius: 4px; } downloadTemplate 1 minute to read "
},

{
    "id": 35,
    "uri": "015_tasks/03_task_createReferenceDoc.html",
    "menu": "tasks",
    "title": "createReferenceDoc",
    "text": " Table of Contents createReferenceDoc Before You Begin About This Task Config.groovy Notes Source .gravatar img { margin-left: 3px; border-radius: 4px; } createReferenceDoc Before You Begin Install pandoc . 1 minute to read About This Task This task creates a reference docx file used by pandoc during docbook-to-docx conversion. Use task convertToDocx to edit this file so it uses your preferred styles. Important The contents of the reference docx are ignored, but its stylesheets and document properties (including margins, page size, header and footer) are used in the new docx. For more information, see Pandoc User&#8217;s Guide: Options affecting specific writers (--reference-doc) And if you have problems with changing the default table style: see https://github.com/jgm/pandoc/issues/3275 . Config.groovy Notes The 'referenceDocFile' property must be set to your custom reference file in Config.groovy: inputPath = '.' // use a style reference file in the input path for conversion from docbook to docx referenceDocFile = ${inputPath}/my-ref-file.docx Source pandoc.gradle task createReferenceDoc ( group: 'docToolchain helper', description: 'creates a docx file to be used as a format style reference in task convertToDocx. Needs pandoc installed.', type: Exec ) { workingDir $docDir executable = pandoc args = [-o, ${docDir}/${referenceDocFile}, --print-default-data-file, reference.docx] doFirst { if(!(referenceDocFile?.trim())) { throw new GradleException(Option `referenceDocFile` is not defined in config.groovy or has an empty value.) } } } "
},

{
    "id": 36,
    "uri": "015_tasks/03_task_convertToDocx.html",
    "menu": "tasks",
    "title": "convertToDocx",
    "text": " Table of Contents convertToDocx Before You Begin Source .gravatar img { margin-left: 3px; border-radius: 4px; } convertToDocx Before You Begin Install pandoc . Ensure that 'docbook' and 'docx' are added to the inputFiles formats in Config.groovy. As an optional step, specify a reference doc file with custom stylesheets (see task createReferenceDoc ). 1 minute to read Tip Blog-Post: Render AsciiDoc to docx (MS Word) Source pandoc.gradle task convertToDocx ( group: 'docToolchain', description: 'converts file to .docx via pandoc. Needs pandoc installed.', type: Exec ) { // All files with option `docx` in config.groovy is converted to docbook and then to docx. def sourceFilesDocx = sourceFiles.findAll { 'docx' in it.formats } sourceFilesDocx.each { def sourceFile = it.file.replace('.adoc', '.xml') def targetFile = sourceFile.replace('.xml', '.docx') workingDir $targetDir/docbook executable = pandoc if(referenceDocFile?.trim()) { args = [-r,docbook, -t,docx, -o,../docx/$targetFile, --reference-doc=${docDir}/${referenceDocFile}, sourceFile] } else { args = [-r,docbook, -t,docx, -o,../docx/$targetFile, sourceFile] } } doFirst { new File($targetDir/docx/).mkdirs() } } "
},

{
    "id": 37,
    "uri": "020_tutorial/020_arc42.html",
    "menu": "tutorial",
    "title": "arc42 Template",
    "text": " Table of Contents Get the arc42 Template Get the arc42 Template 2 minutes to read To work with docToolchain, you first need some documents. So let&#8217;s fetch the arc42 template for software architecture documentation. docToolchain comes with a task called downloadTemplate . Let&#8217;s invoke it and see what happens. Linux / WSL2 with bash ./dtcw downloadTemplate Windows with Powershell ./dtcw.ps1 downloadTemplate There is an interesting bug with the german version of the arc42 template in conjunction with running docToolchain in powershell: The encoding of the files will be wrong. To fix that, just run ./dtcw.ps1 fixEncoding . Result of downloadTemplate-Task $ ./dtcw downloadTemplate dtcw - docToolchain wrapper V0.24 docToolchain V2.0.0 Bash is running on WSL this might cause problems with plantUML see https://doctoolchain.github.io/docToolchain/#wsl for more details Java Version 11 docker available home folder exists use local homefolder install /home/rdmueller/.doctoolchain/ &gt; Configure project : arc42/arc42.adoc &gt; Task :downloadTemplate Install arc42 documentation template. For more information about arc42 see https://arc42.org [ant:input] Which language do you want to install? (EN, DE, ES, RU) &lt;-------------&gt; 0% EXECUTING [6s] [ant:input] Do you want the template with or without help? (withhelp, plain) &lt;-----&lt;-------------&gt; 0% EXECUTING [10s] Download https://github.com/arc42/arc42-template/raw/master/dist/arc42-template-DE-withhelp-asciidoc.zip arc42 template unpacked into /c/Users/ralfd/projects/dtc-tests/wsl/src/docs/arc42 added template to docToolchainConfig.groovy use 'generateHTML', 'generatePDF' or 'generateSite' to convert the template BUILD SUCCESSFUL in 15s 1 actionable task: 1 executed Out of the box, docToolchain only knows the open source arc42 template for software architecture. That&#8217;s why it doesn&#8217;t ask which template to install. Since the template exists in four different languages and with or without help on how to use it, docToolchain asks you for these two parameters. It then downloads the template right from the source, unzips it and reformats it a little bit to fit the needs of docToolchain. It also adds the template to your configuration file. That&#8217;s it. You have now docToolchain with the arc42 template installed. Let&#8217;s render is as HTML, PDF or Microsite in the next steps. "
},

{
    "id": 38,
    "uri": "015_tasks/03_task_exportDrawIo.html",
    "menu": "tasks",
    "title": "exportDrawIo",
    "text": " Table of Contents exportDrawIo About This Task About diagrams.net How to Change Your Workflow to Use diagrams.net How to Convert a Confluence Page to AsciiDoc .gravatar img { margin-left: 3px; border-radius: 4px; } exportDrawIo 2 minutes to read About This Task There is no exportDrawIo task available in docToolchain because such a task is not required. You can continue to use diagrams.net (formerly known as draw.io) to edit your diagrams simply by making a change to your diagram-authoring workflow. About diagrams.net diagrams.net offers free and open source desktop editors for all major operating system platforms. Visit https://www.diagrams.net/integrations to find a desktop editor application compatible with your operating system. When you use the desktop version, just create your diagram with the .png (or even better, .dio.png ) extension and diagrams.net will always save your diagram as a PNG with the source as metadata. They have also launched a free plugin for VS Code and IntelliJ so you can edit your diagrams offline! How to Change Your Workflow to Use diagrams.net Export your diagrams.net/draw.io diagrams as a PNG with the source embedded in the file metadata. This allows you to embed your diagrams into AsciiDoc source as you normally would (using the image:: macro) with the added advantage of storing the diagram source with the image itself. How to Convert a Confluence Page to AsciiDoc If you are converting a Confluence page with embedded draw.io diagrams to AsciiDoc, use this export workflow to continue using diagrams.net: Export an editable PNG diagram from Confluence. Load the diagram you want to export from Confluence. Click File &#160; &#8250; Export as &#160; &#8250; PNG&#8230;&#8203; . In the Image modal, make sure that Include a copy of my diagram is selected. Click Export to save the PNG file with the pattern [file].dio.png . Commit the exported PNG file to source control. Your diagram can now be managed in source control, added to your documentation source and edited using a diagrams.net desktop version. Note Specifying .dio (short for  d raw io ) in the name will help you identify PNG files containing an embedded XML diagram source. // Please, replace #yourelement with a real element id on your webpage MarketplaceWidget.setupMarketplaceWidget('card', 15635, #myelement); "
},

{
    "id": 39,
    "uri": "020_tutorial/040_generateSite.html",
    "menu": "tutorial",
    "title": "generateSite",
    "text": " Table of Contents generateSite generateSite The generateSite task is more advanced. In many cases, your goal will not be to create a single HTML document but a whole documentation website. This is where generateSite shines. It uses a static site generator to turn your document into a nice looking microsite with landing-page, local search and edit links. Linux / WSL2 with bash ./dtcw generateSite Windows with Powershell ./dtcw.ps1 generateSite output of generateHTML $ ./dtcw docker generateSite dtcw - docToolchain wrapper V0.24 docToolchain V2.0.0 Bash is running on WSL this might cause problems with plantUML see https://doctoolchain.github.io/docToolchain/#wsl for more details Java Version 11 docker available home folder exists force use of docker /usr/bin/docker use docker installation Starting a Gradle Daemon (subsequent builds will be faster) &gt; Configure project : arc42/arc42.adoc &gt; Task :generateSite created /project/build/microsite/tmp copy internal theme /home/dtcuser/docToolchain/src/site copy project theme /project/src/site &gt; Task :copyImages &gt; Task :bake Warning: Nashorn engine is planned to be removed from a future JDK release BUILD SUCCESSFUL in 1m 15s 3 actionable tasks: 3 executed The output is written to build/microsite/output/index.html . You can open this file in your browser and view the results. But you will notice that some features which rely on javascript will be broken. To avoid this, use the task previewSite to start a little webserver which presents you the site without flaws. Figure 1. generated output of generateHTML task "
},

{
    "id": 40,
    "uri": "020_tutorial/030_generateHTML.html",
    "menu": "tutorial",
    "title": "generateHTML &amp; generatePDF",
    "text": " Table of Contents generateHTML &amp; generatePDF generateHTML &amp; generatePDF The generateHTML and generatePDF tasks are the most basic tasks which just invoke asciidoctor to generate the output you want. Linux / WSL2 with bash ./dtcw generateHTML Windows with Powershell ./dtcw.ps1 generateHTML output of generateHTML $ ./dtcw generateHTML dtcw - docToolchain wrapper V0.23 docToolchain V2.0.0 Bash is running on WSL this might cause problems with plantUML see https://doctoolchain.github.io/docToolchain/#wsl for more details Java Version 11 docker available home folder exists use local homefolder install /home/rdmueller/.doctoolchain/ Starting a Gradle Daemon, 22 busy Daemons could not be reused, use --status for details &gt; Configure project : arc42/arc42.adoc &gt; Task :generateHTML Converting /c/Users/ralfd/projects/dtc-tests/wsl/src/docs/arc42/arc42.adoc BUILD SUCCESSFUL in 26s 1 actionable task: 1 executed The output is written to build/html5/arc42/arc42.html and build/pdf/arc42/Arc42.pdf . Figure 1. generated output of generateHTML task As you can see in Figure 1 , the HTML output is rendered as a single page with a table of contents (TOC) on the left. If you have chosen the withhelp version, you will notice some little question marks on the right. Hover over these with your mouse and you will see the full help text for each paragraph. This way, you can leave these help texts in your docs without annoying your readers. Figure 2. generated output of generatePDF task Figure 2 shows you the generated PDF. It also contains a TOC on the first pages and your PDF reader will also show the TOC on the left for you to navigate through the document. Both, the HTML and PDF output can be styled to fit your needs. "
},

{
    "id": 41,
    "uri": "020_tutorial/100_diagrams.net.html",
    "menu": "tutorial",
    "title": "Diagrams: Diagrams.net",
    "text": " Table of Contents Diagrams: Diagrams.net Diagrams: Diagrams.net There is no exportDiagramsnet or exportDrawio task, because working with these diagrams is more convenient than with the other diagramming tools. Diagrams.net (formerly known as Draw.io) is a neat little tool which is able to store the source of your diagrams in the meta-data of your .png or .svg files. To recognize these files as diagrams.net files, give them the extension myfile .dio.png or myfile .dio.svg . In IntelliJ, with the asciidoctor plugin and the diagrams.net plugin installed, type something like image::mynewdiagram.dio.png[] IntelliJ will show you that the file doesn&#8217;t exist. Press alt + shift + enter to create the file. IntelliJ will then launch a local copy of the diagrams.net editor. Every change in the diagram will be directly reflected in your document. Figure 1. just a demo image asciidoctor plugin diagrams.net plugin // Please, replace #yourelement with a real element id on your webpage MarketplaceWidget.setupMarketplaceWidget('card', 7391, .button1); MarketplaceWidget.setupMarketplaceWidget('card', 15635, .button2); "
},

{
    "id": 42,
    "uri": "020_tutorial/050_multipleRepositories.html",
    "menu": "tutorial",
    "title": "Multi-Repo",
    "text": " Table of Contents How to generate docs from multiple repositories git clone solution git submodule solution artifact solution How to generate docs from multiple repositories Some static site generators sell multi-repository functionality as a feature. This feature is mainly achieved through a build in git client. Since we almost always work on systems which already have git installed, docToolchain does not come with its own git client. The solution we propose instead is just a simple bash script which does the magic for you. Here is how. Imagine you have a documentation repository set up with the docToolchain wrapper dtcw and some code documents. Now you would like to include the documents from another repository. git clone solution To get the contents of the other repository, one way is to do a git clone of it right to the build folder of your own repository. Let&#8217;s call the script in which we store this command clonerefs.sh , because we clone a reference to the remote documentation. clonerefs.sh #!/usr/bin/env bash git clone git@github.com:docToolchain/docToolchain.git build/refs/docToolchain this works fine if you execute it once, but the second time it will complain that the folder build/refs/docToolchain is not empty. So let&#8217;s create a cloneOrPull function which tries first to clone the repo and pulls an update if the clone fails. clonerefs.sh #!/usr/bin/env bash function cloneOrPull { echo  echo $1 (git clone $1 $2 2&gt; /dev/null &amp;&amp; echo cloned repo )|| git -C $2 pull } cloneOrPull git@github.com:docToolchain/docToolchain.git build/refs/docToolchain That&#8217;s better. We can now include the docs from our main docs. But what if we don&#8217;t want to include them but just let them render? We could clone the repository directly to our src/docs folder. But that would require that it only contains docs and no src/docs folder itself. So we have to copy it over to our src/docs folder. clonerefs.sh #!/usr/bin/env bash function cloneOrPull { echo  echo $1 (git clone $1 $2 2&gt; /dev/null &amp;&amp; echo cloned repo )|| git -C $2 pull } cloneOrPull git@github.com:docToolchain/docToolchain.git build/refs/docToolchain cp build/refs/docToolchain/src/docs/manual src/docs/. But we don&#8217;t want to add these folders to our main repository, so let&#8217;s add it to the .gitignore file. .gitignore [...] src/docs/manual [...] There is one more thing we can optimize. Currently, the script clones the repository with the full history. This is far to much traffic if you only want to use the latest version of your docs. Let&#8217;s add --depth 1 to the `git clone`command to only fetch the latest version. clonerefs.sh #!/usr/bin/env bash function cloneOrPull { echo  echo $1 (git clone --depth 1 $1 $2 2&gt; /dev/null &amp;&amp; echo cloned repo )|| git -C $2 pull } cloneOrPull git@github.com:docToolchain/docToolchain.git build/refs/docToolchain cp build/refs/docToolchain/src/docs/manual src/docs/. This script will now let you clone remote repositories and merge them with your main documentation before building. git submodule solution Another solution is to refrence your sub-repositories as git submodules . Git submodules are pointers in your main repository to a certain version of another repository. Most CI/CD systems clone your repository together will als configured submodules. This makes this approach quite convenient. A draw back is that submodules are not often used and thus developers are not used to them. artifact solution A third solution could be that sub-repositories publish their docs as zip file somewhere. Maybe to an artifactory instance. Your main repository could fetch the published artifacts and use them in the build process. That would be exactly what you do with code. "
},

{
    "id": 43,
    "uri": "020_tutorial/010_Install.html",
    "menu": "tutorial",
    "title": "Install docToolchain",
    "text": " Table of Contents Welcome! Installing docToolchain Get the Wrapper run a first command Problems &amp; Solutions Welcome! Nice to meet you. Glad that you want to learn more about how to use docToolchain. Let&#8217;s start with the installation. The following steps are the same as described in the manual. Please follow them step by step. If you encounter problems, please create a github issue and the community will help you. Installing docToolchain 5 minutes to read Get the Wrapper The basic strategy on how to install docToolchain has changed with V2.x. You now install a small wrapper scripts to your project which does the rest for you: Linux / WSL2 with bash cd &lt;your project&gt; wget doctoolchain.github.io/dtcw chmod +x dtcw if you don&#8217;t have wget installed, you can also use curl : cd &lt;your project&gt; curl -Lo dtcw doctoolchain.github.io/dtcw chmod +x dtcw Windows with Powershell cd &lt;your project&gt; Invoke-WebRequest doctoolchain.github.io/dtcw.ps1 -Outfile dtcw.ps1 Windows with cmd.exe cd &lt;your project&gt; curl -Lo dtcw.bat doctoolchain.github.io/dtcw.bat dtcw.bat wraps the dtcw.ps1 script and executes it in powershell. This might be easier to use if you haven&#8217;t yet configured your powershell as a developer. In order to support development for your project on both, Linux and Windows, it makes sense to download all three scripts to your project. if you use cmd.exe , please follow the instructions for powershell in the manual. We will not further differentiate between both. dtcw uses bash as shell and thus tries to find java from within bash. Chances are that when you use dtcw from another shell like zsh, you shell knows about the right java version but your bash doesn&#8217;t. In that case, please switch to bash and make sure that the right java version is known. Afterwards, you can switch back to your shell. The wrapper helps you in several ways: it ensures that everyone who uses your repository, uses the same version of docToolchain it keeps all the technology out of your repository it wraps all your commands and directs them to your local docToolchain installation even more, it checks the way you have docToolchain installed and if it hasn&#8217;t been installed, it will install it for you via direct download or docker run a first command To get started, you can run tasks as a first command: Linux / WSL2 with bash ./dtcw tasks --group=doctoolchain Windows with Powershell ./dtcw.ps1 tasks --group=doctoolchain This command will show you all tasks available through docToolchain, but first it will make sure that docToolchain is installed: it checks for a configuration file and creates a default one if none is available. it prefers a local installation if docker is available and no local installation, it will use docker. The download of the images might take a while when run for the first time if docker is not available, it will download and install docToolchain to your home folder it will then run a gradle build through the gradle wrapper. This will download and install gradle when run for the first time. the gradle build will also fetch some dependencies. This will also take some time when run for the first time. As you can see, the installation is an incremental process and will take some time because of additional downloads. After the first few commands, you will have all the needed dependencies installed and things will be faster. Problems &amp; Solutions dtcw does not run When you get an error similiar to this one&#8230;&#8203; ./dtcw local tasks --group=doctoolchain ./dtcw: line 1: syntax error near unexpected token `newline' ./dtcw: line 1: `&lt;!DOCTYPE html&gt;' it is likely that the download of the wrapper didn&#8217;t work. Your will see that an HTML page has been downloaded instead of the wrapper. Please try to re-download it. wrong java version When docToolchain starts but crashes with a stacktrace that starts like this&#8230;&#8203; * What went wrong: Could not compile settings file '/Users/falk/.doctoolchain/docToolchain-2.0.0/settings.gradle'. &gt; startup failed: General error during semantic analysis: Unsupported class file major version 61 java.lang.IllegalArgumentException: Unsupported class file major version 61 then you&#8217;ve got a wrong java version. dtcw tries to check the java version upfront by doing a java --version but gradle sometimes picks up a different version. In that case, please try to re-install a correct java version. "
},

{
    "id": 44,
    "uri": "025_development/040_debugging.html",
    "menu": "development",
    "title": "Debugging",
    "text": " Table of Contents Debugging Environment Gradle jBake Templates Theming, Menu and Images Debugging 2 minutes to read Things not working as you expected? Here are some tips that might help you. Environment To get the best out of docToolchain, we recommend that you set up a development environment. This way you get to see the inner workings and you also get to add extra debug output to the tasks that you want to inspect. Gradle You get more hints about what is going on with Gradle when you add the --info flag to your ./dtcw generateSite command: ./dtcw generateSite --info This outputs all config settings as seen by docToolchain along with many other internal settings. jBake Templates If something goes wrong with a template, you typically don’t receive much information about the problem. Take a look at menu.gsp to see how you can use try/catch blocks to get an error message. But to find out where the problem is occurring, you’ll need to use the poor man’s debugger and add some System.out.println statements. Make sure that you use the full System.out.println statement and not only println otherwise you won’t see any output. Theming, Menu and Images How the system creates the menu entries might seem like magic, but sometimes you cannot work out why an image is not shown. Remember, there is a way that you can check the generated files. Check the build/microsite/tmp folder to see the folder that is fed into jBake. In this folder, all files will have additional jbake attributes which are used to build the menu. They are generated from the original attributes of the file and folder/filename information. Now check the build/microsite/output folder to see the generated result. This often helps you to find out where an image actually is located. "
},

{
    "id": 45,
    "uri": "025_development/010_setup_dev_env.html",
    "menu": "development",
    "title": "Setting Up a Dev Environment",
    "text": " Table of Contents Setting Up a Dev Environment Before You Begin Do a Local Install for Docker and SDKMAN! Create Gradle-Independent Tasks Create or Change a Theme Special Functionality for Themes (Config Fragments) Setting Up a Dev Environment 4 minutes to read Before You Begin When you install docToolchain, all of the code is hidden. The information on this page explains how to get access to the code so you can customise the setup in your dev environment. Do a Local Install for Docker and SDKMAN! You need a local installation of docToolchain for development. Docker and SDKMAN! are derived from it. Docker simply contains a local install, and SDKMAN! installs docToolchain locally, but the location is controlled by SDKMAN! not docToolchain. The docToolchain-Wrapper installs docToolchain locally to $HOME/.doctoolchain/docToolchain-$v2.6.7/ . All task invokations through the docToolchain-Wrapper dtcw are redirected to $HOME/.doctoolchain/docToolchain-$v2.6.7/bin/doctoolchain . This shell script calls the Gradle-Wrapper for most tasks. What you need to do is: Create a local install which is connected to your GitHub fork of docToolchain. Create a folder called $HOME/.doctoolchain/docToolchain-2.0.0-dev/ . Check out the ng-branch of your fork to this folder. To use this version in your test project, edit the version at the start of your dtcw script to 2.0.0-dev . You now have the full repo locally cloned. To save memory, some parts of the repo are zipped. If you have problems, check out the prepareDist-Task . Create Gradle-Independent Tasks All tasks currently use Gradle to run. You can bypass Gradle for tasks where it doesn’t add any value (and make docToolchain run faster as a result!). To do this, use the bin/doctoolchain scripts and create a switch. Create or Change a Theme It’s not just the docToolchain code that is hidden. The themes for the static site generator jBake are also hidden. Follow these procedures to customise themes. How to Overwrite a Project Theme When docToolchain builds a static website, it first copies an internal theme to a temp folder, then copies an external theme (if defined) over it. Finally, it copies the project theme over the top. This gives you the opportunity to overwrite some parts of the theme on a per-project basis. To do this: Run the copyThemes task to copy the internal and external themes to the microsite.siteFolder . Check the files (take a look at jbake.org to get a better understanding). Modify the relevant files and delete all of the other files. How to Modify an Existing Theme or Create a Theme from Scratch As we have already mentioned, an external theme is simply a zipped copy of the 'microsite.siteFolder'. All themes are downloaded when referenced from a dtcw configuration, and are stored in $HOME/.doctoolchain/themes/[hash of url] . To modify an existing theme, go to its folder and check out the theme’s project instead of the downloaded copy. This will create a connection back to the GitHub repo so that you can modify the theme directly in $HOME/.doctoolchain/themes/[hash of url] . To create a new theme from scratch, use a simple md5 hash. For example, if you configure your new theme as myTheme then myTheme.md5() will be the hash. Special Functionality for Themes (Config Fragments) It’s likely that you will need a new config item for your self-generated theme. And you can also prompt users to set a value for this new config item when they install the theme for the first time. To do this, create a file called configFragment.groovy in the site folder of your theme. For example: // the title of the microsite, displayed in the upper-left corner // Example: my new site title = '##site-title##' The first line is the message that will be shown to the user (can be over several lines). The second line (starting with Example :) is the default value for the prompt. The third line is the config item itself. If the value is surrounded by ## , the user will be prompted for this value and it will be replaced with the user’s input. Otherwise the config item will be added without a prompt to the user’s current docToochainConfig.groovy . "
},

{
    "id": 46,
    "uri": "025_development/020_run_tests.html",
    "menu": "development",
    "title": "Running Tests",
    "text": " Table of Contents Running Tests Before You Begin Prepare the Project Execute Tests Workaround to Ensure Correct Proxy Settings for Tests Running Tests 2 minutes to read Before You Begin Git and Graphviz must both be installed. Your Gradle setup must be able to work with proxies. You must be using Java 8. Prepare the Project git clone git@github.com:docToolchain/docToolchain.git cd docToolchain/ git checkout V1.0.0 # (1) git submodule update -i (1) The version to test. Not required if you work on the HEAD revision on Master. Execute Tests rm -r build &amp;&amp; ./gradlew test --info The rm command ensures that you have a clean test running. This is vital because if artifacts of an older test run still exist, Gradle will skip steps (‘Up-to-date’) and you might get false positives. Workaround to Ensure Correct Proxy Settings for Tests The docToolchain setup is based on the Gradle-Test-Kit and makes use of the Spock test execution framework . The Gradle test runner is started in its own test environment and its own JVM instance. As a result, the global proxy settings are ignored. To execute the test with the correct proxy settings, you must use a workaround. Copy the proxy settings from the gradle.properties file located in the user directory to the gradle.properties file located in the docToolchain folder itself. Note: The files downloaded by the Gradle test runner are placed in a different folder than the default Gradle cache. You will find them in the Tmp folder C:\Users\YOUR_USER_NAME\AppData\Local\Temp\.gradle-test-kit-YOUR_USER_NAME\caches. "
},

{
    "id": 47,
    "uri": "025_development/030_create_new_release.html",
    "menu": "development",
    "title": "Creating a New Release",
    "text": " Table of Contents Creating a New Release Before You Begin GitHub Docker Hub Blog Post docToolchain-Wrapper (dtcw) SDKMAN! Creating a New Release 2 minutes to read Before You Begin We use semantic versioning and we also keep a changelog . All of this is done on a best-efforts basis. A release consists of five parts, each explained below. GitHub Update the version in gradle.properties . Update the changelog. Create a section for the version. Copy to the new section all unreleased features which will be in the release. Commit and push the new version. Draft a new release . Copy the contents of the changelog for this version to the description then submit. Set the version as v X.Y.Z. Run ./gradlew createDist to zip the source in build (the distribution file). Add the zipped file and submit the new release. Docker Hub The image build for rdmueller/doctoolchain depends on the GitHub repo docToolchain/docker-image . Update the Dockerfile to reflect the new version. Create a new release . Reference the GitHub release in the changelog (the build on Dockerhub will be automatically triggered). Important! Currently, the autobuild only works for paying customers. To manually build and upload the image, download the repo, switch to branch ng-beta , cd to the alpine folder and execute docker build -t rdmueller/doctoolchain:v2.0.0-rc15 . . After that, use Docker Desktop to push the resulting image to Docker Hub. Blog Post Create a blog post to announce the new release. The SDKMAN! announcement will reference it. docToolchain-Wrapper (dtcw) Everything went well? Great! Now let’s update the version used in the wrapper: dtcw dtcw.ps1 dtcw.bat SDKMAN! A GitHub action sdkman deploy has been created to deploy to SDKMAN! Set the version to the same as for the other releases, but without the prepended v: X.Y.Z. Use as a download link the link to the docToolchain-dist.zip from the GitHub release. Tip: the link looks like https://github.com/docToolchain/docToolchain/releases/download/v1.3.1/docToolchain-dist.zip . "
},

{
    "id": 48,
    "uri": "025_development/005_contributing_to_docs.html",
    "menu": "development",
    "title": "Contributing to Docs",
    "text": " Table of Contents Contributing to Docs Prerequisites Go to page you want to edit or fix Fork the Repository Edit the Page Commit the Changes Comparing changes Contributing to Docs 5 minutes to read The easiest way to contribute to this project is to contribute to the documentation. Hier is a quick step-by-step guide on how to fix something on a documentation page. Prerequisites You need a github.com account. If you don&#8217;t have one, you can create one here: https://github.com/signup It might help if you go through the Github Hello World tutorial before you continue but it is not necessary. Go to page you want to edit or fix You already found this tutorial, so you already know how to go to the page you want to edit or fix. All documentation can be found at http://doctoolchain.org/ . The source code of each page is available at https://github.com/doctoolchain/doctoolchain/ in the /src/docs/ folder. But there is an easier way to find the exact source. The documentation pages all look something like this: In the upper right corner you can see the Improve this doc link: This will take you directly to the source of the page, already in edit mode. If you are not logged in, GitHub will ask you to do so. The Create an issue link will be helpful if you want to report a bug or request a feature for a page. It takes you directly to the issue tracker with a pre-filled issue. For now, let&#8217;s click on the Improve this doc link. Fork the Repository If you click the link for the first time, you will be asked to fork the repository. A fork is a copy of the repository. Maybe you are used to working on the main repository or a branch within the main repository. This is not possible in this case, because you don&#8217;t have write access, only read access. The solution is to fork the repository. This way, you create a copy in your own space, and you will have write access to it. Edit the Page You will now be taken to the page you want to edit already in edit mode. What you see is asciidoctor markup. Check out the {url-asciidoc-quick-reference}[AsciiDoc quick reference] for more information. The blue box on top tells you what you already know: a copy has been created for you and you are editing it. Important since you work on your own copy of the docs, you can&#8217;t break anything. You even don&#8217;t have write access to the main repository. So feel free to edit the page as you like. Use the Preview button to see what your edits will look like. Since this is only a preview and GitHub doesn&#8217;t know about docToolchain, this preview will only show you if your AsciiDoc syntax is correct. Some other features like the TOC or include statements will not be available in the preview. Do your edit and then Commit the Changes Below the editor, there is a small Propose Changes form. Enter a headline and a description of the changes you made and click Propose changes . This will save your changes to your fork of the repository. Note Git works with diffs - it only saves the changes you made, not a full copy of the new page. This is important to know if you want to understand the inner workings of Git. After you&#8217;ve clicked the button, you will be taken to a page which shows you what you changed. Comparing changes This view lets you review your changes. These diffs are not easy to read, but I promise that over time you get used to it. Red lines are deletions, green lines are additions. As you can see in the screenshot, I added an empty space in line 17. Line 17 has been deleted (red line) and replaced with a new line (green line). Line 1 looks mysterious, because it seems that is has been replaced with an identical copy. This is because the line ending changed but is not visible in the diff. The grey box on top shows you It is quite likely that you still know what you did a minute ago, so let&#8217;s click on the Create pull request button. "
},

{
    "id": 49,
    "uri": "025_development/050_who-uses-dtc.html",
    "menu": "-",
    "title": "moved",
    "text": " document.location.href = '../10_about/10_about-the-project.html'; "
},

{
    "id": 50,
    "uri": "010_manual/60_further_reading.html",
    "menu": "manual",
    "title": "Useful Resources",
    "text": " Table of Contents Useful Resources Underlying Technologies Books Past and upcoming Talks Useful Resources 4 minutes to read This chapter lists some additional references to interesting resources. Underlying Technologies This project relies on many underlying technologies. Here is a lists and some explaining words Markup AsciiDoc AsciiDoc is our preferred markup language for technical docs Asciidoctor User-Manual AsciiDoc Syntax Quick Reference Video: Asciidoctor Deep Dive by Alexander Schwartz Markdown Since we use jBake as Static-Site-Generator, you can also use Markdown to write your docs. flexmark-java is the Markdown parser and flavour used by jBake if you prefer to use the exportMarkdown -Task, then markdown-to-asciidoc library is used. The IntelliJ-Plugin also uses this library. Templates arc42 is the perfect documentation template for your softare solution architecture documentation. arc42 arc42 Tips &amp; Tricks arc42 FAQ Docs-as-Code docs-as-code is the approach you use when you treat your docs the same as your code. We&#8217;ve create a little microsite with some additional resources. Docs-as-Code write the docs: docs-as-code is another independent resource. Static Site Generator jBake is the underlying static site generator for the generateSite -Task docsy is the standard theme we use twitter bootstrap is the CSS framework Books Note links to Amazon are affiliate links English Books DOCS-LIKE-CODE by Anne Gentle Modern technical writing: An Introduction to Software Documentation by Andrew Etter arc42 by Example by Gernot Starke, Stefan Zörner, Michael Simons, Ralf D. Müller Communicating Software Architectures with arc42 by Gernot Starke und Peter Hruschka Visualise, document and explore your software architecture by Simon Brown German Books arc42 in Aktion: Praktische Tipps zur Architekturdokumentation von Gernot Starke und Peter Hruschka Softwarearchitekturen dokumentieren und kommunizieren: Entwürfe, Entscheidungen und Lösungen nachvollziehbar und wirkungsvoll festhalten von Stefan Zörner Past and upcoming Talks Dokumentation am (Riesen-)Beispiel – arc42, AsciiDoc und Co. in Aktion Gernot Starke and Ralf D. Müller Anhand eines großen Systems zeigen Gernot und Ralf, wie Sie mit ziemlich wenig Aufwand angemessene und vernünftige Dokumentation für unterschiedliche Stakeholder produzieren – sodass Entwicklungsteams dabei auch noch Spaß haben. Unser Rezept: AsciiDoc mit arc42 mischen, Automatisierung mit Gradle und Maven hinzufügen und mit Diagramm- oder Modellierungstools Ihrer Wahl kombinieren. Schon sind schicke HTML- und reviewfähige PDF-Dokumente fertig. Auf Wunsch gibts DOCX und Confluence als Zugabe. Wir zeigen, wie Sie Doku genau wie Quellcode verwalten können, stakeholderspezifische Dokumente erzeugen und Diagramme automatisiert integrieren können. Einige Teile dieser Doku können Sie sogar automatisiert testen. Zwischendurch bekommen Sie zahlreiche Tipps, wie und wo Sie systematisch den Aufwand für Dokumentation reduzieren können und trotzdem lesbare, verständliche und praxistaugliche Ergebnisse produzieren. Gesunde Dokumentation mit AsciiDoctor Alexander Schwartz Autoren möchten Inhalte effizient dokumentieren und vorhandene Inhalte wiederverwenden. Ein Leser möchte das Dokument in einem ansprechenden Layout präsentiert bekommen. Das textbasierte AsciiDoc-Format bietet für einen Entwickler oder technischen Redakteur alle notwendigen Auszeichungselemente, um auch anspruchsvolle Dokumente zu schreiben. So werden unter anderem Tabellen, Fußnoten und annotierte Quellcodes unterstützt. Gleichzeitig ist es ähnlich leichtgewichtig wie z.B. das Markdown Format. Für die Leser wird HTML, PDF oder EPUB generiert. Da AsciiDoc wie Programmcode eingecheckt wird und Merge-Operationen einfach möglich sind, können Programmcode und Dokumentation zusammen versioniert und auf einem einheitlichen Stand gehalten werden. Der Vortrag gibt eine kurze Einführung in AsciiDoc und die dazugehörigen Werkzeuge. "
},

{
    "id": 51,
    "uri": "010_manual/20_install.html",
    "menu": "manual",
    "title": "Installing docToolchain",
    "text": " Table of Contents Installing docToolchain Get the Wrapper run a first command Existing documents arc42 from scratch Build Installing docToolchain 5 minutes to read Get the Wrapper The basic strategy on how to install docToolchain has changed with V2.x. You now install a small wrapper scripts to your project which does the rest for you: Linux / WSL2 with bash cd &lt;your project&gt; wget doctoolchain.github.io/dtcw chmod +x dtcw if you don&#8217;t have wget installed, you can also use curl : cd &lt;your project&gt; curl -Lo dtcw doctoolchain.github.io/dtcw chmod +x dtcw Windows with Powershell cd &lt;your project&gt; Invoke-WebRequest doctoolchain.github.io/dtcw.ps1 -Outfile dtcw.ps1 Windows with cmd.exe cd &lt;your project&gt; curl -Lo dtcw.bat doctoolchain.github.io/dtcw.bat dtcw.bat wraps the dtcw.ps1 script and executes it in powershell. This might be easier to use if you haven&#8217;t yet configured your powershell as a developer. In order to support development for your project on both, Linux and Windows, it makes sense to download all three scripts to your project. if you use cmd.exe , please follow the instructions for powershell in the manual. We will not further differentiate between both. dtcw uses bash as shell and thus tries to find java from within bash. Chances are that when you use dtcw from another shell like zsh, you shell knows about the right java version but your bash doesn&#8217;t. In that case, please switch to bash and make sure that the right java version is known. Afterwards, you can switch back to your shell. The wrapper helps you in several ways: it ensures that everyone who uses your repository, uses the same version of docToolchain it keeps all the technology out of your repository it wraps all your commands and directs them to your local docToolchain installation even more, it checks the way you have docToolchain installed and if it hasn&#8217;t been installed, it will install it for you via direct download or docker run a first command To get started, you can run tasks as a first command: Linux / WSL2 with bash ./dtcw tasks --group=doctoolchain Windows with Powershell ./dtcw.ps1 tasks --group=doctoolchain This command will show you all tasks available through docToolchain, but first it will make sure that docToolchain is installed: it checks for a configuration file and creates a default one if none is available. it prefers a local installation if docker is available and no local installation, it will use docker. The download of the images might take a while when run for the first time if docker is not available, it will download and install docToolchain to your home folder it will then run a gradle build through the gradle wrapper. This will download and install gradle when run for the first time. the gradle build will also fetch some dependencies. This will also take some time when run for the first time. As you can see, the installation is an incremental process and will take some time because of additional downloads. After the first few commands, you will have all the needed dependencies installed and things will be faster. Existing documents If you already have some existing documents in AsciiDoc format in your project, you need to tell docToolchain where it can find these documents. To do so, take a look at the created docToolchainConfig.groovy and update it. arc42 from scratch If you don&#8217;t have existing documents yet, or if you need a fresh start, you can get the arc42 template in AsciiDoc format. Run the following command to ask docToolchain to download the template of your choice: Linux / WSL2 with bash ./dtcw downloadTemplate Windows ./dtcw.ps1 downloadTemplate Build By now, the docToolchain wrapper dtcw resides in your project folder and the arc42 template is installed. Now let&#8217;s render it as HTML and PDF. You should be able to execute the following commands: Linux / WSL2 with bash ./dtcw generateHTML ./dtcw generatePDF Windows ./dtcw.ps1 generateHTML ./dtcw.ps1 generatePDF As a result, you will see the progress of your build together with some warnings which you can just ignore for the moment. The first build generated some files within the build : build |-- html5 | |-- arc42 | | `-- arc42.html | `-- images | |-- 05_building_blocks-EN.png | |-- 08-Crosscutting-Concepts-Structure-EN.png | `-- arc42-logo.png `-- pdf |-- arc42 | `-- arc42.pdf `-- images |-- 05_building_blocks-EN.png |-- 08-Crosscutting-Concepts-Structure-EN.png `-- arc42-logo.png Congratulations! If you see the same folder structure, you just managed to render the standard arc42 template as html and pdf! If you didn&#8217;t get the right output, please raise an issue on github Blog-Posts: Behind the great Firewall , Enterprise AsciiDoctor "
},

{
    "id": 52,
    "uri": "010_manual/30_config.html",
    "menu": "manual",
    "title": "Configuration",
    "text": " Table of Contents Configuration mainConfigFile and docDir AsciiDoc config Command Line Parameters Configuration 2 minutes to read This appendix covers all configuration introduced by docToolchain. AsciiDoc, AsciiDoctor, Gradle and other tools and libraries used know of more configuration settings and you can read about those in the corresponding documentation. mainConfigFile and docDir docToolchain should be easy to use. That&#8217;s why the goal is to have one config file with all settings for each project. But first of all, docToolchain has to know where your documentation project is located. If docDir is defined, the default for mainConfigFile is Config.groovy in the root folder of your docDir . You have several options to specify the location of your documentation project ( docDir ) and the location of your config file ( mainConfigFile ). Commandline Specify the property on the commandline ./dtcw generateHTML -PmainConfigFile=Config.groovy Tip you can verify the location of your Config.groovy by executing docToolchain with the --info parameter which sets the loglevel to info . It will print the location on the command line (among other settings) dynamic configuration properties Sometime you need a more dynamic configuration. Since the configuration file is an executable .groovy file, you can not only configure static values but also fetch dynamic once. For example, example = System.properties.myProperty You can then specify the property with the -D parameter like this ./dtcw docker generateHTML -DmyProperty=myValue In the same way, you can use environment variables example = System.getenv(myEnvVariable) But in this case, you have to ensure that the environment varaible can be accessed. It will not work for docker based execution of dtcw Content of the mainConfigFile Unresolved directive in &lt;stdin&gt; - include::../../../Config.groovy[tags=**] AsciiDoc config Command Line Parameters "
},

{
    "id": 53,
    "uri": "010_manual/040_contributors.html",
    "menu": "-",
    "title": "moved",
    "text": " document.location.href = '../10_about/30_community.html'; "
},

{
    "id": 54,
    "uri": "010_manual/010_introduction_and_goals.html",
    "menu": "-",
    "title": "moved",
    "text": " document.location.href = '../10_about/20_what-is-doctoolchain.html'; "
},

{
    "id": 55,
    "uri": "010_manual/40_features.html",
    "menu": "manual",
    "title": "Using docToolchain to Build Docs",
    "text": " Table of Contents Using docToolchain to Build Docs Using docToolchain to Build Docs 1 minute to read docToolchain implements quite a lot of features via scripts which you call through the command line. We call these tasks and they are listed under the top-level menu Tasks in this documentation. "
},

{
    "id": 56,
    "uri": "010_manual/50_Frequently_asked_Questions.html",
    "menu": "manual",
    "title": "Solutions to Common Problems",
    "text": " Table of Contents Solutions to Common Problems Images exportVisio Sparx Enterprise Architect Known error Messages Solutions to Common Problems 7 minutes to read This section tries to answer the most common and frequently asked questions about how to work with docToolchain. It will also contain questions relevant to the tools used to build docToolchain, but the main focus is docToolchain itself. If you are stuck, make sure that you also check other sources like Stack Overflow . There is also a great FAQ for all your arc42 questions: https://faq.arc42.org/home/ If you have a question or problem for which you can&#8217;t find a solution, you can for this repo, add your question and create a pull request raise the issue through the GitHub issue tracker ask your question on Stack Overflow discuss the problem on Slack Images Asciidoctor User Manual on images Asciidoctor Quick Reference on images AsciiDoctor Writer Guide on images Q: Why are images not shown in the preview of my editor? A: this is most likely because your editor doesn&#8217;t know where they are stored. If you follow the default settings, you probably store your images in a subfolder images . The build script knows about it, because the attribute imagesdir has been set to ./images , but your editor doesn&#8217;t care about the build script - it only checks the currently opened AsciiDoc file. The solution is to add a line to each file which checks if the imagesdir is set and if not, sets it to a valid value: ifndef::imagesdir[:imagesdir: ../images] Q: Which image format should I use? A: AsciiDoc and AsciiDoctor support several formats like GIF, PNG, JPG and SVG. However, if you want to use most features, some formats are better to use than others: GIF is not supported by the PDF renderer. Use JPG or PNG instead. JPG is great for photos but not for diagrams (you might get compression artifacts). So, if you want to use photos from your flipcharts - JPG might work for you. SVG great for high resolution diagrams, but not good supported by DOCX as output format. OpenOffice Writer might display the image a bit stretched, MS Word didn&#8217;t display it at all in some experiments. PDF output might display a warning that newer SVG versions are not supported (happends especially with diagrams.net images). PNG this is the preferred format for images used with docToolchain. All output formats support it and if diagrams are rendered with a resolution high enough to display all details, it will also be scaled well with all output formats. Q: Why are my images rotated in the output? A: This most likely happens when you&#8217;ve taken photos with a mobile device and include them in you docs. A mobile device does not rotate the image itself, it only stores the orientation of the device in the meta data of the photo. Your operating system will show you the image as expected, but the rendered AsciiDoc will not. This can be „fixed“ with Imagemagick, by using convert -auto-orient or mogrify -auto-orient (thanx to @rotnroll666 for this tip). You can also try to just open the image in your favourite editor and re-save it. exportVisio Q: I get an error message saying that a library is not registered when I try to run the exportVisio -task. Ausnahme beim Festlegen von Visible: Das COM-Objekt des Typs Microsoft.Office.Interop.Visio.ApplicationClass kann nicht in den Schnittstellentyp Microsoft.Office.Interop.Visio.IVApplication umgewandelt werden. Dieser Vorgang konnte nicht durchgeführt werden, da der QueryInterface-Aufruf an die COM-Komponente für die Schnittstelle mit der IID {000D0700-0000-0000-C000-000000000046} aufgrund des folgenden Fehlers nicht durchgeführt werden konnte: Bibliothek nicht registriert. (Ausnahme von HRESULT: 0x8002801D (TYPE_E_LIBNOTREGISTERED)). In ...\scripts\VisioPageToPngConverter.ps1:48 Zeichen:1 + $VisioApp.Visible = $false + ~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (:) [], SetValueInvocationException + FullyQualifiedErrorId : ExceptionWhenSetting A: When Visio is installed, it registers itself as a com library. It seems that this registration can break. You can fix this by visiting the windows system settings &#8594; install or uninstall a program, select visio , select change and then repair . Sparx Enterprise Architect Q: Sparx Enterprise Architect is a Windows tool, isn&#8217;t it? Yes, it is, but it is written to support CrossOver in order to run on Linux based systems. Wine, the open source branch of CrossOver, seems to work as well. Take a look at this page to see how to install it on a linux based system: https://www.sparxsystems.com/support/faq/ea_on_linux.html I (Ralf) once gave it a try and even managed to get remote control over EA via VBS and COM up and running (which is the pre-requisite for docToolchain). Known error Messages Q: I get the error saying 'Failed to create MD5 hash for file content'. * What went wrong: Failed to capture snapshot of input files for task ':generateHTML' property 'sourceDir' during up-to-date check. &gt; Failed to create MD5 hash for file content.` There are two known reasons for this error. One of the .adoc files is opened in an editor, so that windows can&#8217;t get the lock for that file. &#8594; Close all .adoc files. You use the Bash script doctoolchain on a windows system. &#8594; Use doctoolchain.bat instead. It works even in a Bash Shell. Q: I get the error saying 'Unsupported major.minor version 52.0' This is a sign that you use an outdated version of Java. Please upgrade to Java 8 at least and 14 max. The docToolchain-wrapper (dtcw) in v2.0 will check the java version for you so that you will not see this error message in the future. Q: I get an error message saying 'Error occurred during initialization of VM' Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details FAILURE: Build failed with an exception. * What went wrong: Unable to start the daemon process. … Error occurred during initialization of VM Could not reserve enough space for 2097152KB object heap Somehow docToolchain can&#8217;t allocate the memory which is configured out of the box. Try to free up some memory or comment out the line org.gradle.jvmargs=-Xmx2048m in gradle.properties Q: I get the error saying Could not initialize class java.awt.GraphicsEnvironment$LocaleGE This seems to be a problem with WSL on Windows. Some sources mention to run Java in headless mode, but in this case, it doesn&#8217;t solve the problem. The root cause seems to be plantUML trying to get some font information. Only real solution seems to be to shutdown WSL from a powershell window wiht wsl --shutdown and retry. Warning this will kill all your WSL terminals without warning. Another solution seems to be to install a fresh version of your java runtime (I thought it is immutable, but it really helps). Best Solution is to switch to powershell. Another solution is to avoid PlantUML and generate Diagrams through a kroki.io server. Another variant of this is Can&#8217;t connect to X11 window server using '192.168.189.153:0' as the value of the DISPLAY variable. . In this case, it might help to install an X-Server (x410 for example) and configure the DISPLAY variable correctly. An easy way to test you configuration is to run xeyes in WSL. NOTE Please make sure that you wsl is up-to-date bei executing wsl --update - this is not part of your regular windows update! "
},

{
    "id": 57,
    "uri": "ea/Use_Cases_notes.html",
    "menu": "ea",
    "title": "Use_Cases_notes.ad",
    "text": " docToolchain is a gradle/maven build which turns asciidoc documentation into HTML5 rendered files. create stunning docs invoked by gradle or maven command "
},

{
    "id": 58,
    "uri": "ea/UseCases.html",
    "menu": "ea",
    "title": "UseCases.ad",
    "text": " docToolchain is a gradle/maven build which turns asciidoc documentation into HTML5 rendered files. create stunning docs invoked by gradle or maven command "
},

{
    "id": 59,
    "uri": "ea/Activity_notes.html",
    "menu": "ea",
    "title": "Activity_notes.ad",
    "text": " Activity1 Just a test for issue #1 https://github.com/rdmueller/docToolchain/issues/1 "
},

{
    "id": 60,
    "uri": "ea/readme.html",
    "menu": "ea",
    "title": "readme.ad",
    "text": " Table of Contents Warning! This folder contains exported diagrams or notes from Enterprise Architect. Please note that these are generated files but reside in the src -folder in order to be versioned. This is to make sure that they can be used from environments other than windows. Warning! The contents of this folder will be overwritten with each re-export! use gradle exportEA to re-export files "
},

{
    "id": 61,
    "uri": "ea/Use_Cases_links.html",
    "menu": "ea",
    "title": "Use_Cases_links.ad",
    "text": " . and this is just a test for issue #2 https://github.com/rdmueller/docToolchain/issues/2 "
},

{
    "id": 62,
    "uri": "ea/issue1.html",
    "menu": "ea",
    "title": "issue1.ad",
    "text": " Activity1 Just a test for issue #1 https://github.com/rdmueller/docToolchain/issues/1 "
},

{
    "id": 63,
    "uri": "ea/issue2.html",
    "menu": "ea",
    "title": "issue2.ad",
    "text": " . and this is just a test for issue #2 https://github.com/rdmueller/docToolchain/issues/2 "
},

{
    "id": 64,
    "uri": "ea/Architect_notes_issue2.html",
    "menu": "ea",
    "title": "Architect_notes_issue2.ad",
    "text": " "
},

{
    "id": 65,
    "uri": "ea/Use_Cases_notes_UseCases.html",
    "menu": "ea",
    "title": "Use_Cases_notes_UseCases.ad",
    "text": " docToolchain is a gradle/maven build which turns asciidoc documentation into HTML5 rendered files. create stunning docs invoked by gradle or maven command "
},

{
    "id": 66,
    "uri": "ea/Use_Cases_links_issue2.html",
    "menu": "ea",
    "title": "Use_Cases_links_issue2.ad",
    "text": " . and this is just a test for issue #2 https://github.com/rdmueller/docToolchain/issues/2 "
},

{
    "id": 67,
    "uri": "ea/Architect_notes.html",
    "menu": "ea",
    "title": "Architect_notes.ad",
    "text": " "
},

{
    "id": 68,
    "uri": "ea/Activity_notes_issue1.html",
    "menu": "ea",
    "title": "Activity_notes_issue1.ad",
    "text": " Activity1 Just a test for issue #1 https://github.com/rdmueller/docToolchain/issues/1 "
},

{
    "id": 69,
    "uri": "050_ADRs/ADR-2-anotherTest.html",
    "menu": "ADRs",
    "title": "ADR-2-anotherTest.ad",
    "text": " Table of Contents second ADR second ADR another Test "
},

{
    "id": 70,
    "uri": "050_ADRs/ADR-1-firstTest.html",
    "menu": "ADRs",
    "title": "ADR-1-firstTest.adoc",
    "text": " Table of Contents a first Architecture Decision Record (ADR) a first Architecture Decision Record (ADR) just a test "
},

{
    "id": 71,
    "uri": "10_about/30_community.html",
    "menu": "about",
    "title": "Acknowledgements and Contributors",
    "text": " Table of Contents Acknowledgements and Contributors Why Contributions Matter Get Involved! Our Contributors Acknowledgements and Contributors 2 minutes to read Why Contributions Matter Without our amazing community of contributors, the docToolchain project wouldn’t exist in its current form. As an open source project, we depend on the skills and expertise of many to deliver a quality outcome. From developers to technical writers, many people have made valuable contributions to the code and the docs. We’re so grateful to them. We are also thankful for those in our community who take the time to give feedback, create issues, answer questions and send pull requests. Get Involved! There are so many technologies that support docToolchain, including AsciiDoc, AsciiDoctor, Gradle and arc42. We need all the help we can get to make improvements and keep our project humming. Simply create an issue and send a pull request. Our Contributors Please get in touch to update your entry or let us know if you have contributed in some way and we will add you to the list. Stefan Bodewig MoePad Niels wschaef Gernot Starke Alexander Schwartz Alexander Heusingfeld Dan Allen Stefan Pfeiffer isidorotrevino Jakub Jablonski Frank Pohl Ixchel Ruiz Schalk Cronjé Mario García Joe David M. Carr Fabian Nonnenmacher Christoph Stoettner Roman Funk ghp-dev Christoph Raaflaub Jorge Aguilera Stefan Bohn Jochen Kraushaar Luis Muniz Andreas Offenhaeuser Daniel Bast Sabatmonk Maarten Gribnau Michael Prieß Heiko Stehli Peter Stange Nils Mahlstädt @ hmmh Kevin Werner J. Staub Vladi Bjelakovic Daniel Kessel Björn Seebeck Txemanu Nikolay Orozov Andrea Macaluso Michael Roßner Jan Hendriks Daniel Kocot Alexander Schmitt Jérémie Bresson Jody Winter "
},

{
    "id": 72,
    "uri": "10_about/10_about-the-project.html",
    "menu": "about",
    "title": "About the Project",
    "text": " Table of Contents About the Project History How docToolchain Is Used Organisations Using It Open Source Projects Using It About the Project 2 minutes to read History docToolchain is an open source project dedicated to automating the creation of technical documentation. Before the project started, the founders had never heard of the term 'docs as code'. All they knew was that they were sick of keeping their architecture diagrams up to date by copying them from a UML tool to a word processor. Being lazy developers, they thought There must be a better way to do this!. So they automated the diagram export, ditched their word processors, and started using a markup renderer. This enabled them to reference the diagrams from within the text and update them before rendering the document. And so, docToolchain was born. How docToolchain Is Used The main focus of docToolchain is technical documentation. It was traditionally used only for internal docs projects that were not visible to the public. From v2.0.0 we included a static site generator which means that open source projects and other organisations with public-facing docs can use it. Organisations Using It docToolchain is currently being used by the following organisations. If you&#8217;re also using it, please let us know! We love watching our user community grow, and your support keeps the project alive to fight the good docs fight! Deutsche Bahn - DB Systel Open Source Projects Using It If your open source project is using docToochain, please let us know by sending a pull request: docToolchain ( source ) Html Sanity Check ( source ) "
},

{
    "id": 73,
    "uri": "10_about/20_what-is-doctoolchain.html",
    "menu": "about",
    "title": "What Is docToolchain?",
    "text": " Table of Contents What Is docToolchain? Introduction Docs as Code arc42 How docToolchain Brings Everything Together What You Get with docToolchain .gravatar img { margin-left: 3px; border-radius: 4px; } What Is docToolchain? 4 minutes to read Introduction docToolchain is a documentation generation tool that uses the Docs as Code approach as a basis for its architecture, plus some additional automation provided by the arc42 template . Learn all about the project &lt;link to About the Project page&gt;[here]. Docs as Code ‘Docs as code’ refers to a philosophy that you should write documentation using the same tools as you use to write code. If you need to write technical docs for your software project, why not use the same tools and processes as you use for your source code? There are so many benefits: You don’t have to learn a complicated docs management system. Developers feel more at home in the docs because they look and feel like code. You can manage docs using standard version control like GitHub. arc42 arc42 has been a part of docToolchain since the earliest version. But what is arc42? Dr. Gernot Starke and Peter Hruschka created the arc42 template as a standard for software architecture documentation. They used their experience of software architectures both in the template structure and the explanations that appear in each chapter to guide you when you’re writing your documentation. arc42 is available in well-known formats including MS Word, textile, and Confluence. All of these formats are automatically generated from a single golden master which is formatted in AsciiDoc . How docToolchain Brings Everything Together To follow a docs as code approach, you need a build script that automates steps like exporting diagrams and rendering Markdown (or AsciiDoc in the case of docToolchain) to the target format. Creating this type of build script is not easy (and even harder to maintain). There are also lots of questions to answer: “How do I create .docx?” and “Why doesn’t lib x work with lib y?” docToolchain is the result of one developer’s journey through the docs as code universe. The goal of docToolchain is to automate the creation of technical docs through an easy-to-use build script that only needs to be configured not modified, and that is nurtured and cared for by a &lt;link to Community page&gt;[diverse open source community]. What You Get with docToolchain A Ready-Made Document Management System By using a version control system like Git , you get a perfect document management system for free. Git allows you to version your docs, branch them, and also leaves an audit trail. You can even check who wrote which part of the docs. Isn’t that great? And because your docs are simple plain text, it’s easy to do a diff and see exactly what has changed. Bonus: storing your docs in the same repo as your code means they’re always in sync! Built-In Collaboration and Review As a distributed version control system, Git comes with doc collaboration and review processes built in. People can fork the docs and send pull requests for the changes they make. You review the changes. Done! Most Git frontends like Bitbucket , GitLab and GitHub also allow you to reject pull requests with comments. Image References and Code Snippets Instead of pasting images into a binary document format, docToolchain lets you reference images. This ensures that your imagery is always up to date every time you rebuild your documents. You can also reference code snippets directly from your source code. You&#8217;ll save so much time because your docs and code will always be in sync and completely up to date! Compound and Stakeholder-Tailored Docs As if image refs and code snippets weren&#8217;t enough, docToolchain also lets you split docs into several sub-documents plus a master for greater cohesion. And you&#8217;re not restricted to one master. You can create master docs for different stakeholders that only contain the chapters they need. And So Much More&#8230;&#8203; If you can dream it, you can script it! Want to include a list of open issues from Jira? You can! Want to include a changelog from Git? Go for it! Want to use inline text-based diagrams? Knock yourself out! "
},

{
    "id": 74,
    "uri": "010_manual/single-page.html",
    "menu": "-",
    "title": "docToolchain Manual",
    "text": " toc-title 1. autobuildSite 1.1. generateHTML 1.2. fixEncoding 1.3. prependFilename 1.4. collectIncludes 1.5. generatePDF 1.6. generateSite 1.7. generateDocbook 1.8. generateDeck 1.9. publishToConfluence 1.10. convertToDocx 1.11. createReferenceDoc 1.12. convertToEpub 1.13. exportEA 1.14. exportVisio 1.15. exportDrawIo 1.16. exportChangeLog 1.17. exportContributors 1.18. exportJiraIssues 1.19. exportJiraSprintChangelogIssues 1.20. exportPPT 1.21. exportExcel 2. exportMarkdown 2.1. Source 2.2. exportOpenAPI 2.3. htmlSanityCheck 2.4. dependencyUpdates .gravatar img { margin-left: 3px; border-radius: 4px; } docToolchain Manual .gravatar img { margin-left: 3px; border-radius: 4px; } document.location.href = '../10_about/20_what-is-doctoolchain.html'; Unresolved directive in &lt;stdin&gt; - include::020_install.adoc[leveloffset=+1] Unresolved directive in &lt;stdin&gt; - include::025_features.adoc[leveloffset=+1] .gravatar img { margin-left: 3px; border-radius: 4px; } 1. autobuildSite 1 minute to read This script starts an endless loop which checks for changes to your docs source then re-runs the generateSite -task whenever it detects changes. The output will be logged to build/generateSite.log . bin/autobuildSite.bash #!/bin/bash DIR_TO_WATCH='src/' #COMMAND='rm -r build || true &amp;&amp; mkdir -p build/microsite/output/images/ &amp;&amp; ./dtcw generateSite 2&gt;&amp;1 | tee build/generateSite.log' COMMAND='mkdir -p build/microsite/output/images/ &amp;&amp; ./dtcw generateSite 2&gt;&amp;1 | tee build/generateSite.log' #execute first time cp src/docs/images/ready.png build/microsite/output/images/status.png #eval $COMMAND #wait for changes and execute while true ; do watch --no-title --chgexit ls -lR ${DIR_TO_WATCH} | sha1sum cp src/docs/images/building.png build/microsite/output/images/status.png eval $COMMAND cp src/docs/images/ready.png build/microsite/output/images/status.png sleep 6 done .gravatar img { margin-left: 3px; border-radius: 4px; } 1.1. generateHTML 3 minutes to read This is the standard AsciiDoctor generator which is supported out of the box. The result is written to build/html5 . The HTML files need the images folder to be in the same directory to display correctly. Note If you would like to have a single-file HTML as result, you can configure AsciiDoctor to store the images inline as data-uri . Just set :data-uri: in the config of your AsciiDoc file. But be warned - such a file can become very big easily and some browsers might get into trouble rendering them. https://rdmueller.github.io/single-file-html/ Text based Diagrams For docToolchain, it is configured to use the asciidoctor-diagram plugin which is used to create PlantUML diagrams. The plugin also supports a bunch of other text based diagrams, but PlantUML is the most used. To use it, just specify your PlantUML code like this: .example diagram [plantuml, {plantUMLDir}demoPlantUML, png] # (1) ---- class BlockProcessor class DiagramBlock class DitaaBlock class PlantUmlBlock BlockProcessor &lt;|-- DiagramBlock DiagramBlock &lt;|-- DitaaBlock DiagramBlock &lt;|-- PlantUmlBlock ---- The element of this list specifies the diagram tool plantuml to be used. The second element is the name of the image to be created and the third specifies the image type. Note The {plantUMLDir} ensures that PlantUML also works for the generatePDF task. Without it, generateHTML works fine, but the PDF will not contain the generated images. Important Make sure to specify a unique image name for each diagram, otherwise they will overwrite each other. The above example renders as Figure 1. example diagram If you want to control the size of the generated diagram in the output, you can configure the width attribute (in pixels) or scale attribute (floating point ratio) passed to asciidoctor-diagram . For example, if you take the example diagram above and change the declaration to one of the below versions [plantuml, target={plantUMLDir}demoPlantUMLWidth, format=png, width=250] # rest of the diagram definition [plantuml, target={plantUMLDir}demoPlantUMLScale, format=png, scale=0.75] # rest of the diagram definition it will render like this: Figure 2. example diagram (with specified width) Figure 3. example diagram (with specified scale) Note PlantUML needs Graphviz dot installed to work. If you can&#8217;t install it, you can use the Java based version of the dot library. Just add !pragma graphviz_dot smetana as the first line of your diagram definition. This is still an experimental feature, but already works quite well! https://rdmueller.github.io/plantuml-without-graphviz/ Tip Blog-Posts: PlantUML with Gradle , plantUML with Asciidoctor-pdf , plantUML revisited , How to use PlantUML without Graphviz Source AsciiDocBasics.gradle task generateHTML ( type: AsciidoctorTask, group: 'docToolchain', description: 'use html5 as asciidoc backend') { attributes ( 'plantUMLDir' : file(${docDir}/${config.outputPath}/html5).toURI().relativize(new File(${docDir}/${config.outputPath}/html5/plantUML/).toURI()).getPath(), ) // specify output folder explicitly to avoid cleaning targetDir from other generated content outputDir = file(targetDir + '/html5/') separateOutputDirs(false) def sourceFilesHTML = sourceFiles.findAll { 'html' in it.formats } // onlyIf { // sourceFilesHTML // } sources { sourceFilesHTML.each { include it.file logger.info it.file } } resources { config.imageDirs.each { imageDir -&gt; from(new File(file(srcDir),imageDir)) logger.info ('imageDir: '+imageDir) into './images' } config.resourceDirs.each { resource -&gt; from(new File(file(srcDir),resource.source)) logger.info ('resource: '+resource.source) into resource.target } } backends = ['html5'] doFirst { if (sourceFilesHTML.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type HTML. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } Unresolved directive in &lt;stdin&gt; - include::../015_tasks/03_copy_themes.adoc[leveloffset=+2] .gravatar img { margin-left: 3px; border-radius: 4px; } 1.2. fixEncoding 1 minute to read About This Task Whenever Asciidoctor has to process a file that is not UTF-8 encoded, Ruby tries to read it, then throws an error similar to this one: asciidoctor: FAILED: /home/demo/test.adoc: Failed to load AsciiDoc document - invalid byte sequence in UTF-8 Unfortunately, finding the incorrectly encoded file is difficult if a lot of includes:: are used, and Asciidoctor will only show the name of the main document. This is not Asciidoctor&#8217;s fault. The fault lies with the Ruby interpreter that sits underneath. The fixEncoding task crawls through all *.ad and *.adoc files and checks their encoding. If it comes across a file which is not UTF-8 encoded, it will rewrite it with the UTF-8 encoding. Source scripts/fixEncoding.gradle import groovy.util.* import static groovy.io.FileType.* task fixEncoding( description: 'finds and converts non UTF-8 adoc files to UTF-8', group: 'docToolchain helper', ) { doLast { File sourceFolder = new File(${docDir}/${inputPath}) println(sourceFolder:  + sourceFolder.canonicalPath) sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^.*(ad|adoc|asciidoc)$') { CharsetToolkit toolkit = new CharsetToolkit(file); // guess the encoding def guessedCharset = toolkit.getCharset().toString().toUpperCase(); if (guessedCharset!='UTF-8') { def text = file.text file.write(text, utf-8) println( converted ${file.name} from '${guessedCharset}' to 'UFT-8') } } } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.3. prependFilename 1 minute to read When Asciidoctor renders a file, the file context only knows the name of the top-level AsciiDoc file but an include file doesn&#8217;t know that it is being included. It will simply get the name of the master file and has no chance to get his own names as attribute. This task simply crawls through all AsciiDoc files and prepends the name of the current file like this: :filename: 015_tasks/03_task_prependFilename.adoc This way, each file can get its own file name. This enables features like the inclusion of file contributors (see exportContributors-task). Note The task skips all files named config.* , _config.* , feedback.* and _feedback.* . scripts/prependFilename.gradle import static groovy.io.FileType.* task prependFilename( description: 'crawls through all AsciiDoc files and prepends the name of the current file', group: 'docToolchain helper', ) { doLast { File sourceFolder = new File(${docDir}/${inputPath}) println(sourceFolder:  + sourceFolder.canonicalPath) sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^.*(ad|adoc|asciidoc)$') { if (file.name.split('[.]')[0] in [feedback, _feedback, config, _config]) { println skipped +file.name } else { def text = file.getText('utf-8') def name = file.canonicalPath - sourceFolder.canonicalPath name = name.replace(\\, /).replaceAll(^/, ) if (text.contains(:filename:)) { text = text.replaceAll(:filename:.*, :filename: $name) println updated +name } else { text = :filename: $name\n + text println added +name } file.write(text,'utf-8') } } } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.4. collectIncludes 2 minutes to read About This Task This task crawls through your entire project looking for AsciiDoc files with a specific name pattern, then creates a single AsciiDoc file which includes only those files. When you create modular documentation, most includes are static. For example, the arc42-template has 12 chapters and a master template that includes those 12 chapters. Normally when you work with dynamic modules like ADRs (Architecture Decision Records) you create those files on the fly. Maybe not within your /src/docs folder, but alongside the code file for which you wrote the ADR. In order to include these files in your documentation, you have to add the file with its whole relative path to one of your AsciiDoc files. This task will handle it for you! Just stick to this file-naming pattern ^[A-Z]{3,}[-_].* (begin with at least three uppercase letters and a dash/underscore) and this task will collect the file and write it to your build folder. You only have to include this generated file from within your documentation. If you provide templates for the documents, those templates are skipped if the name matches the pattern ^.\*[-\_][tT]emplate[-\_].* . Example You have a file called: /src/java/yourCompany/domain/books/ADR-1-whyWeUseTheAISINInsteadOFISBN.adoc The task will collect this file and write another file called: /build/docs/_includes/ADR_includes.adoc &#8230;&#8203;which will look like this: include::../../../src/java/yourCompany/domain/books/ADR-1-whyWeUseTheAISINInsteadOFISBN.adoc[] Obviously, you&#8217;ll reap the most benefits if the task has several ADR files to collect. 😎 You can then include these files in your main documentation by using a single include: include::{targetDir}/docs/_includes/ADR_includes.adoc[] scripts/collectIncludes.gradle import static groovy.io.FileType.* import java.security.MessageDigest task collectIncludes( description: 'collect all ADRs as includes in one file', group: 'docToolchain' ) { doFirst { new File(targetDir, '_includes').mkdirs() } doLast { //let's search the whole project for files, not only the docs folder //could be a problem with node projects :-) //running as subproject? set scandir to main project if (project.name!=rootProject.name &amp;&amp; scanDir=='.') { scanDir = project(':').projectDir.path } if (docDir.startsWith('.')) { docDir = file(new File(projectDir, docDir).canonicalPath) } logger.info docToolchain&gt; docDir: ${docDir} logger.info docToolchain&gt; scanDir: ${scanDir} if (scanDir.startsWith('.')) { scanDir = file(new File(docDir, scanDir).canonicalPath) } else { scanDir = file(new File(scanDir, ).canonicalPath) } logger.info docToolchain&gt; scanDir: ${scanDir} logger.info docToolchain&gt; includeRoot: ${includeRoot} if (includeRoot.startsWith('.')) { includeRoot = file(new File(docDir, includeRoot).canonicalPath) } logger.info docToolchain&gt; includeRoot: ${includeRoot} File sourceFolder = scanDir println sourceFolder:  + sourceFolder.canonicalPath def collections = [:] sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^[A-Z]{3,}[-_].*[.](ad|adoc|asciidoc)$') { def type = file.name.replaceAll('^([A-Z]{3,})[-_].*$','\$1') if (!collections[type]) { collections[type] = [] } logger.info file:  + file.canonicalPath def fileName = (file.canonicalPath - scanDir.canonicalPath)[1..-1] if (file.name ==~ '^.*[Tt]emplate.*$') { logger.info ignore template file:  + fileName } else { if (file.name ==~ '^.*[A-Z]{3,}_includes.adoc$') { logger.info ignore generated _includes files:  + fileName } else { if ( fileName.startsWith('docToolchain') || fileName.replace(\\, /).matches('^.*/docToolchain/.*$')) { //ignore docToolchain as submodule } else { logger.info include corrected file:  + fileName collections[type] &lt;&lt; fileName } } } } } println targetFolder:  + (targetDir - docDir) logger.info targetDir - includeRoot:  + (targetDir - includeRoot) def pathDiff = '../' * ((targetDir - docDir) .replaceAll('^/','') .replaceAll('/$','') .replaceAll([^/],'').size()+1) logger.info pathDiff:  + pathDiff collections.each { type, fileNames -&gt; if (fileNames) { def outFile = new File(targetDir+'/_includes', type + '_includes.adoc') logger.info outFile.canonicalPath-sourceFolder.canonicalPath outFile.write(// this is autogenerated\n) fileNames.sort().each { fileName -&gt; outFile.append (include::../+pathDiff+fileName.replace(\\, /)+[]\n\n) } } } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.5. generatePDF 2 minutes to read This task makes use of the asciidoctor-pdf plugin to render your documents as a pretty PDF file. The file will be written to build/pdf . Note The used plugin is still in alpha status, but the results are already quite good. If you want to use another way to create a PDF, use PhantomJS for instance and script it. The PDF is generated directly from your AsciiDoc sources without the need of an intermediate format or other tools. The result looks more like a nicely rendered book than a print-to-pdf HTML page. It is very likely that you need to theme you PDF - change colors, fonts, page header, and footer. This can be done by creating a custom-theme.yml file. As a starting point, copy the file src/docs/pdfTheme/custom-theme.yml from docToolchain to your project and reference it from your main .adoc`file via setting the `:pdf-stylesdir: . For instance, insert :pdf-stylesdir: ../pdfTheme at the top of your document to reference the custom-theme.yml from the /pdfTheme folder. Documentation on how to modify a theme can be found in the asciidoctor-pdf theming guide . Tip Blog-Post: Beyond HTML Source AsciiDocBasics.gradle task generatePDF ( type: AsciidoctorTask, group: 'docToolchain', description: 'use pdf as asciidoc backend') { attributes ( 'plantUMLDir' : file(${docDir}/${config.outputPath}/pdf/images/plantUML/).path, ) attributes ( 'data-uri': 'true', 'plantUMLDir' : file(${docDir}/${config.outputPath}/images/).path, 'imagesoutdir' : file(${docDir}/${config.outputPath}/images/).path ) def sourceFilesPDF = sourceFiles.findAll { 'pdf' in it.formats } // onlyIf { // sourceFilesPDF // } sources { sourceFilesPDF.each { include it.file logger.info it.file } } backends = ['pdf'] /** //check if a remote pdfTheme is defined def pdfTheme = System.getenv('DTC_PDFTHEME') def themeFolder = pdfTheme.md5() if (pdfTheme) { //check if it is already installed //TODO: finish this... } **/ doFirst { if (sourceFilesPDF.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type PDF. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.6. generateSite 4 minutes to read When you have one document, the output of generateHTML might fit your needs. When your documentation grows, and you have several documents, you need a microsite which bundles all the information. The generateSite task uses jBake to create a static site with a Landingpage, Blog and Search. Pages The microsite is page oriented instead of document oriented. It is likely that you already organized your documents by chapter. Use these chapters as pages to create a great user experience. The arc42-template sources are a good example. In order to include a page in the microsite, add a meta-data header to it. page meta-data :jbake-menu: arc42 :jbake-title: Solution Strategy :jbake-order: 4 :jbake-type: page_toc :jbake-status: published :filename: 015_tasks/03_task_generateSite.adoc :imagesdir: ../../images :toc: [[section-solution-strategy]] === Solution Strategy jbake-menu the top-level menu for this page. Defaults to the top-level folder name of the adoc file within the docDir . jbake-title the title to be displayed in the drop-down top-level menu. Defaults to the first headline of the file. jbake-order give the drop-down entries a sort order. Defaults to a prefixed number of the file like 04 _filename.adoc or to the prefixed number of the second level folder name. jbake-type the page type. This controls which template is used to render the page. You will mostly use page for a full-width page or page_toc for a page with a table of contents (toc) rendered on the left. Defaults to page_toc . jbake-status draft or published . Only published pages will be rendered. Defaults to published for files with a jbake-order and draft for files without order or files prefixed with _ . filename needed for edit and feedback-links (coming soon). Defaults to the filename :-) ifndef this fixes the imagesdir according to the nesting level of your docs folder Defaults to the main docDir/images toc for :jbake-type: page_toc , you need this line to generate the toc. Note Start your pages with a == level headline. You can fix the level offset when you include the page in a larger document with include::chapter.adoc[leveloffset=+1] Templates and Style The jBake templates and CSS etc. is hidden away from you for convenience. The basic template use twitter Bootstrap 5 as CSS framework. Use the copyThemes task to copy all hidden jBake resources to your project. You can then remove the ones you don&#8217;t need and change those you want to change. Note copyThemes overwrites existing files, but your code is under version control, isn&#8217;t it? Landing Page Put a index.gsp page as landing page in src/site/templates . This landing page is plain HTML5 styled with twitter bootstrap. Header and footer is added by docToolchain. An example can be found through copyThemes or on github . Blog The microsite also contains a simple but powerful blog. Use it to inform your team about changes and architecture decision records (ADRs). To create a new blog post, create a new file in src/docs/blog/&lt;year&gt;/&lt;post-name&gt;.adoc with the following template: blog post template :jbake-title: &lt;title-of your post&gt; :jbake-date: &lt;date formatted as 2021-02-28&gt; :jbake-type: post :jbake-tags: &lt;blog, asciidoc&gt; :jbake-status: published :imagesdir: ../../images == {jbake-title} {jbake-author} {jbake-date} &lt;insert your text here&gt; Search The microsite does not bring its own local search. Instead, it only has a search input-field which can be used to link to another search engine. CI/CD When run in an automated build, you should set the environment variable DTC_HEADLESS to true or 1 to ensure that docToolchain will not ask to install the configured theme. It will simply assume that you do want to install it. To avoid the download of the theme with every build, you can copy the themes folder from $HOME/.doctoolchain/themes to the corresponding folder in your build container. See also previewSite Source scripts/generateSite.gradle import groovy.util.* import static groovy.io.FileType.* buildscript { repositories { maven { url mavenRepository } } dependencies { classpath 'org.asciidoctor:asciidoctorj-diagram:2.0.2' } } dependencies { jbake 'org.asciidoctor:asciidoctorj-diagram:2.0.2' jbake 'io.pebbletemplates:pebble:3.1.2' } apply plugin: 'org.jbake.site' apply plugin: 'org.gretty' def color = { color, text -&gt; def colors = [black: 30, red: 31, green: 32, yellow: 33, blue: 34, magenta: 35, cyan: 36, white: 37] return new String((char) 27) + [${colors[color]}m${text} + new String((char) 27) + [0m } jbake { version = '2.7.0-rc.2' srcDirName = ${targetDir}/microsite/tmp/site destDirName = ${targetDir}/microsite/output configuration['asciidoctor.option.requires'] = asciidoctor-diagram config.microsite.each { key, value -&gt; configuration['site.'+key-'config.microsite.'] = value //println 'site.'+key-'config.microsite.' + = + value } configuration['asciidoctor.attributes'] = [ sourceDir=${targetDir}, 'source-highlighter=prettify@', //'imagesDir=../images@', imagesoutDir=${targetDir}/microsite/output/images@, imagesDir=${config.microsite.contextPath}/images@, targetDir=${targetDir}, docDir=${docDir}, projectRootDir=${new File(docDir).canonicalPath}@, ] } bakePreview { port = '8046' } gretty { httpPort = ${config.microsite.previewPort?:8042} as Integer contextPath = ${config.microsite.contextPath} extraResourceBases = [${targetDir}/microsite/output] } task generateSite( group: 'docToolchain', description: 'generate a microsite using jBake.') { doLast { new File(${targetDir}/microsite/tmp).mkdirs() println created println new File(${targetDir}/microsite/tmp/).canonicalPath //copy internal theme println copy internal theme ${new File(projectDir, 'src/site').canonicalPath} copy { from('src/site') into(${targetDir}/microsite/tmp/site) } //check if a remote pdfTheme is defined def siteTheme = System.getenv('DTC_SITETHEME')?: def themeFolder = new File(projectDir, ../themes/ + siteTheme.md5()) try { if (siteTheme) { println use siteTheme $siteTheme //check if it is already installed if (!themeFolder.exists()) { if (System.getenv('DTC_HEADLESS')) { ant.yesno = y } else { println ${color 'green', \nTheme '$siteTheme' is not installed yet. } def input = ant.input(message:  ${color 'green', 'do you want me to download and install it to '} ${color 'green', ' ' + themeFolder.canonicalPath} ${color 'green', 'for you?'}\n, validargs: 'y,n', addproperty: 'yesno') } if (ant.yesno == y) { themeFolder.mkdirs() download { src siteTheme dest new File(themeFolder, 'siteTheme.zip') } copy { from zipTree(new File(themeFolder, 'siteTheme.zip')) into themeFolder } delete { delete new File(themeFolder, 'siteTheme.zip') } } else { println ${color 'green', \nI will continue without the theme for now... } siteTheme =  } } //copy external theme if (siteTheme) { copy { from(themeFolder) {} into(${targetDir}/microsite/tmp/) } //check if the config has to be updated // check if config still contains /** microsite **/ def configFile = new File(docDir, mainConfigFile) def configFileText = configFile.text if (configFileText.contains(/** start:microsite **/)) { def configFragment = new File(targetDir,'/microsite/tmp/site/configFragment.groovy') if (configFragment.exists()) { println ${color 'green',  It seems that this theme is used for the first time in this project. Let's configure it! If you are unsure, change these settings later in your config file $configFile.canonicalPath } def comment =  def conf =  def example =  def i = 0 configFragment.eachLine { line -&gt; if (line.trim()) { if (line.startsWith(//)) { conf +=   + line + \n def tmp = line[2..-1].trim() comment += color('green', tmp) + \n if (tmp.toLowerCase().startsWith(example)) { example = tmp.replaceAll([^ ]* , ) } } else { //only prompt if there is something to prompt if (line.contains(##)) { def property = line.replaceAll([ =].*, ) if (!example) { example = config.microsite[property] } comment = color('blue', $property) + \n + comment if (example) { ant.input(message: comment, addproperty: 'res' + i, defaultvalue: example) } else { ant.input(message: comment, addproperty: 'res' + i) } (comment, example) = [, ] line = line.replaceAll(##.+##, ant['res' + i]) conf +=   + line + \n i++ } else { conf +=   + line + \n } } } else { conf += \n } } configFile.write(configFileText.replaceAll((?sm)/[*][*] start:microsite [*][*]/.*/[*][*] end:microsite [*][*]/, %%marker%%).replace(%%marker%%, conf)) println color('green', config written\ntry\n ./dtcw generateSite previewSite\nto see your microsite!) } //copy the dummy docs (blog, landing page) to the project repository copy { from(new File(themeFolder, 'site/doc')) {} into(new File(docDir, inputPath)) } } } } } catch (Exception e) { println color('red', e.message) if (e.message.startsWith(Not Found)) { themeFolder.deleteDir() throw new GradleException(Couldn't find theme. Did you specify the right URL?\n+e.message) } else { throw new GradleException(e.message) } } //copy project theme if (config.microsite.siteFolder) { def projectTheme = new File(new File(docDir, inputPath), config.microsite.siteFolder) println copy project theme ${projectTheme.canonicalPath} copy { from(projectTheme) {} into(${targetDir}/microsite/tmp/site) } } //copy docs copy { from(new File(docDir, inputPath)) {} into(${targetDir}/microsite/tmp/site/doc) } //fix MetaData-Header File sourceFolder = new File(targetDir, '/microsite/tmp/site/doc') logger.info(sourceFolder:  + sourceFolder.canonicalPath) sourceFolder.traverse(type: FILES) { file -&gt; if (file.name ==~ '^.*(ad|adoc|asciidoc)$') { if (file.name.startsWith(_)) { //ignore } else { def origText = file.text //parse jbake attributes def text =  def jbake = [ status: published, order: -1, type: 'page_toc' ] def parseAttribs = true def beforeToc =  origText.eachLine { line -&gt; if (parseAttribs &amp;&amp; line.startsWith(:jbake)) { line = (line - :jbake-).split(: +, 2) jbake[line[0]] = line[1] } else { if (line.startsWith([)) { // stop parsing jBake-attribs when a [source] - block starts which might contain those attribs as example parseAttribs = false } text += line+\n //there are some attributes which have to be set before the toc if (line.startsWith(:toc) ) { beforeToc += line+\n } } } def name = file.canonicalPath - (sourceFolder.canonicalPath+File.separator) if (File.separator=='\\') { name = name.split(\\\\) } else { name = name.split(/) } if (name.size()&gt;1) { if (!jbake.menu) { jbake.menu = name[0] if (jbake.menu ==~ /[0-9]+[-_].*/) { jbake.menu = jbake.menu.split([-_], 2)[1] } } def docname = name[-1] if (docname ==~ /[0-9]+[-_].*/) { jbake.order = docname.split([-_],2)[0] docname = docname.split([-_],2)[1] } if (name.size() &gt; 2) { if ((jbake.order as Integer)==0) { // let's take the order from the second level dir or file and not the file def secondLevel = name[1] if (secondLevel ==~ /[0-9]+[-_].*/) { jbake.order = secondLevel.split([-_],2)[0] } } else { if ((jbake.order as Integer) &gt; 0) { // } else { jbake.status = draft } } } if (jbake.order==-1 &amp;&amp; docname.startsWith('index')) { jbake.order = 0 jbake.status = published } // news blog if (jbake.order==-1 &amp;&amp; jbake.type=='post') { jbake.order = 0 try { jbake.order = Date.parse(yyyy-MM-dd, jbake.date).time / 100000 } catch ( Exception e) { System.out.println unparsable date ${jbake.date} in $name } jbake.status = published } def leveloffset = 0 text.eachLine { line -&gt; if (!jbake.title &amp;&amp; line ==~ ^=+ .*) { jbake.title = (line =~ ^=+ (.*))[0][1] def level = (line =~ ^(=+) .*)[0][1] if (level===) { leveloffset = 1 } } } if (!jbake.title) { jbake.title = docname } if (leveloffset==1) { //leveloffset needed // we always start with == not with = text = text.replaceAll((?ms)^(=+) , '$1= ') } def header = '' jbake.each { key, value -&gt; if (key=='order') { header += :jbake-${key}: ${value as Integer}\n } else { header += :jbake-${key}: ${value}\n } } file.write(header + \n$beforeToc\n\n:toc: left\n\n++++\n&lt;!-- endtoc --&gt;\n++++\n + text, utf-8) } } } } /** println =*80 println (new File(${targetDir}/microsite/tmp/site/doc).canonicalPath) new File(${targetDir}/microsite/tmp/site/doc).eachFileRecurse { file -&gt; if (file.name.endsWith('.adoc')) { System.out.println &gt;&gt; +file.name } } **/ } } task previewSite( group: 'docToolchain', dependsOn: [], description: 'start a little webserver to preview your Microsite', ) { if (new File(${targetDir}/microsite/output).exists()) { finalizedBy 'jettyRun' } doLast { if (new File(${targetDir}/microsite/output).exists()) { // everything is fine } else { throw new GradleException(&gt; &gt; Microsite not built yet, please run './dtcw generateSite' first &gt;) } } } task copyImages(type: Copy) { config.imageDirs.each { imageDir -&gt; from(new File (new File(docDir, inputPath),imageDir)) {} logger.info ('imageDir: '+imageDir) into(${targetDir}/microsite/output/images) } config.resourceDirs.each { resource -&gt; from(new File(file(srcDir),resource.source)) logger.info ('resource: '+resource.source) into(${targetDir}/microsite/output/ + resource.target) } } bake.dependsOn copyImages generateSite.finalizedBy bake .gravatar img { margin-left: 3px; border-radius: 4px; } 1.7. generateDocbook 1 minute to read This is only a helper task - it generates the intermediate format for convertToDocx &lt;&lt;&gt;&gt; and convertToEpub . Source AsciiDocBasics.gradle task generateDocbook ( type: AsciidoctorTask, group: 'docToolchain', description: 'use docbook as asciidoc backend') { def sourceFilesDOCBOOK = sourceFiles.findAll { 'docbook' in it.formats } // onlyIf { // sourceFilesDOCBOOK // } sources { sourceFilesDOCBOOK.each { include it.file logger.info it.file } } backends = ['docbook'] doFirst { if (sourceFilesDOCBOOK.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type docbook. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.8. generateDeck 1 minute to read About This Task This task makes use of the asciidoctor-reveal.js backend to render your documents into a HTML-based presentation. It creates a PowerPoint presentation, then enriches it by adding reveal.js slide definitions in AsciiDoc to the speaker notes. For best results, use this task with the exportPPT task. Source AsciiDocBasics.gradle task generateDeck ( type: AsciidoctorTask, group: 'docToolchain', description: 'use revealJs as asciidoc backend to create a presentation') { attributes ( 'idprefix': 'slide-', 'idseparator': '-', 'docinfo1': '', 'revealjs_theme': 'black', 'revealjs_progress': 'true', 'revealjs_touch': 'true', 'revealjs_hideAddressBar': 'true', 'revealjs_transition': 'linear', 'revealjs_history': 'true', 'revealjs_slideNumber': 'true' ) options template_dirs : [new File(new File (projectDir,'/resources/asciidoctor-reveal.js'),'templates').absolutePath ] def sourceFilesREVEAL = sourceFiles.findAll { 'revealjs' in it.formats } // onlyIf { // sourceFilesREVEAL // } sources { sourceFilesREVEAL.each { include it.file logger.info it.file } } outputDir = file(targetDir+'/decks/') resources { from('resources') { include 'reveal.js/**' } from(sourceDir) { include 'images/**' } into() logger.info ${docDir}/${config.outputPath}/images } doFirst { if (sourceFilesREVEAL.size()==0) { throw new Exception ( &gt;&gt; No source files defined for type 'revealjs'. &gt;&gt; Please specify at least one inputFile in your docToolchainConfig.groovy ) } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.9. publishToConfluence 6 minutes to read This target takes the generated HTML file, splits it by headline and pushes it to Confluence. This enables you to use the docs-as-code approach while getting feedback from non-techies through Confluence comments. And it fulfills the requirement of &#8230;&#8203; but all documentation is in Confluence. Special features: [source] -blocks are converted to code-macro blocks in confluence. only pages and images which have changed between task runs are really published and hence only for those changes notifications are sent to watchers. This is quite important - otherwise watchers are easily annoyed by too many notifications. :keywords: Keywords are attached as labels to every generated Confluence page. The rules for page labels should be kept in mind. See https://confluence.atlassian.com/doc/add-remove-and-search-for-labels-136419.html . Several keywords are allowed. They must be separated by comma, e.g. :keywords: label_1, label-2, label3, &#8230;&#8203; . Labels (keywords) must not contain a space character! Use '_' or '-'. Note code-macro-blocks in confluence render an error if the language attribute contains an unknown language. See https://confluence.atlassian.com/doc/code-block-macro-139390.html for a list of valid language and how to add further languages. Configuration We tried to make the configuration self-explaining, but there are always some note to add. ancestorId this is the page ID of the parent page to which you want your docs to be published. Go to this page, click on edit and the needed ID will show up in the URL. Specify the ID as string within the config file. api for cloud instances, [context] is wiki preambleTitle the title of the page containing the preamble (everything the first second level heading). Default is 'arc42' disableToC This boolean configuration define if the Table of Content (ToC) is disabled from the page once uploaded in confluence. (it is false by default, so the ToC is active) pagePrefix/pageSuffix Confluence can&#8217;t handle two pages with the same name. Moreover, the script matches pages regardless of the case. It will thus refuse to replace a page whose title only differs in case with an existing page. So you should create a new confluence space for each piece of larger documentation. If you are restricted and can&#8217;t create new spaces, you can use this pagePrefix / pageSuffix to define a prefix/suffix for this doc so that it doesn&#8217;t conflict with other page names. credentials Use username and password or even better username and api-token. You can create new API-tokens in your profile . To avoid having your password or api-token versioned through git, you can store it outside of this configuration as environment variable or in another file - the key here is that the config file is a groovy script. e.g. you can do things like credentials = user:${new File(/home/me/apitoken).text}.bytes.encodeBase64().toString() To simplify the injection of credentials from external sources there is a fallback. Should you leave the credentials field empty, the variables confluenceUser and confluencePass from the build environment will be used for authentication. You can set these through any means allowed by gradle like the gradle.properties file in the project or your home directory, environment variables or command-line flags. For all ways to set these variables, have a look at the gradle manual . apikey In cases where you have to use full user authorization because of internal confluence permission handling, you need to add the API-token in addition to the credentials. The API-token cannot be added to the credentials as it is used for user and password exchange. Therefore the API-token can be added as parameter apikey, which makes the addition of the token as a separate header field with key: keyId and value of apikey . Example including storing of the real value outside this configuration: apikey = ${new File(/home/me/apitoken).text} . extraPageContent If you need to prefix your pages with a warning that this is generated content - this is the right place. enableAttachments If value is set to true , your links to local file references will be uploaded as attachments. The current implementation only supports a single folder. This foldername will be used as a prefix to validate if your file should be uploaded or not. In case you enable this feature, and use a folder which starts with attachment*, an adaption of this prefix is required. jiraServerId the jira server id your confluence instance is connected to. If value is set, all anchors pointing to a jira ticket will be replaced by the confluence jira macro. To function properly jiraRoot has to be configured (see exportJiraIssues ). Example: All files to attach will require to be linked inside the document. link:attachement/myfolder/myfile.json[My API definition] attachmentPrefix The expected foldername of your output dir. Default : attachment proxy If you need to provide a proxy to access Confluence, you may set a map with keys host (e.g. 'my.proxy.com' ), port (e.g. '1234' ) and schema (e.g. 'http' ) of your proxy. useOpenapiMacro If this option is present and equal to confluence-open-api then any source block marked with class openapi will be wrapped in Elitesoft Swagger Editor macro: (see Elitesoft Swagger Editor ) For backward compatibility: If this option is present and equal to true , then again the Elitesoft Swagger Editor macro will be used. If this option is present and equal to open-api then any source block marked with class openapi will be wrapped in Open API Documentation for Confluence macro: (see Open API Documentation for Confluence ). A download source (yaml) button is shown by default. This is how you&#8217;d include your openapi YAML file: [source.openapi,yaml] ---- include::myopeanapi.yaml[] ---- publishToConfluence.gradle //Configureation for publishToConfluence confluence = [:] // 'input' is an array of files to upload to Confluence with the ability // to configure a different parent page for each file. // // Attributes // - 'file': absolute or relative path to the asciidoc generated html file to be exported // - 'url': absolute URL to an asciidoc generated html file to be exported // - 'ancestorName' (optional): the name of the parent page in Confluence as string; // this attribute has priority over ancestorId, but if page with given name doesn't exist, // ancestorId will be used as a fallback // - 'ancestorId' (optional): the id of the parent page in Confluence as string; leave this empty // if a new parent shall be created in the space // - 'preambleTitle' (optional): the title of the page containing the preamble (everything // before the first second level heading). Default is 'arc42' // // The following four keys can also be used in the global section below // - 'spaceKey' (optional): page specific variable for the key of the confluence space to write to // - 'createSubpages' (optional): page specific variable to determine whether .sect2 sections shall be split from the current page into subpages // - 'pagePrefix' (optional): page specific variable, the pagePrefix will be a prefix for the page title and it's sub-pages // use this if you only have access to one confluence space but need to store several // pages with the same title - a different pagePrefix will make them unique // - 'pageSuffix' (optional): same usage as prefix but appended to the title and it's subpages // only 'file' or 'url' is allowed. If both are given, 'url' is ignored confluence.with { input = [ [ file: build/docs/html5/arc42-template-de.html ], ] // endpoint of the confluenceAPI (REST) to be used // verfiy that you got the correct endpoint by browsing to // https://[yourServer]/[context]/rest/api/user/current // you should get a valid json which describes your current user // a working example is https://arc42-template.atlassian.net/wiki/rest/api/user/current api = 'https://[yourServer]/[context]/rest/api/' // Additionally, spaceKey, createSubpages, pagePrefix and pageSuffix can be globally defined here. The assignment in the input array has precedence // the key of the confluence space to write to spaceKey = 'asciidoc' // the title of the page containing the preamble (everything the first second level heading). Default is 'arc42' preambleTitle = '' // variable to determine whether .sect2 sections shall be split from the current page into subpages createSubpages = false // the pagePrefix will be a prefix for each page title // use this if you only have access to one confluence space but need to store several // pages with the same title - a different pagePrefix will make them unique pagePrefix = '' pageSuffix = '' /* WARNING: It is strongly recommended to store credentials securely instead of commiting plain text values to your git repository!!! Tool expects credentials that belong to an account which has the right permissions to to create and edit confluence pages in the given space. Credentials can be used in a form of: - passed parameters when calling script (-PconfluenceUser=myUsername -PconfluencePass=myPassword) which can be fetched as a secrets on CI/CD or - gradle variables set through gradle properties (uses the 'confluenceUser' and 'confluencePass' keys) Often, same credentials are used for Jira &amp; Confluence, in which case it is recommended to pass CLI parameters for both entities as -Pusername=myUser -Ppassword=myPassword */ //optional API-token to be added in case the credentials are needed for user and password exchange. //apikey = [API-token] // HTML Content that will be included with every page published // directly after the TOC. If left empty no additional content will be // added // extraPageContent = '&lt;ac:structured-macro ac:name=warning&gt;&lt;ac:parameter ac:name=title /&gt;&lt;ac:rich-text-body&gt;This is a generated page, do not edit!&lt;/ac:rich-text-body&gt;&lt;/ac:structured-macro&gt; extraPageContent = '' // enable or disable attachment uploads for local file references enableAttachments = false // default attachmentPrefix = attachment - All files to attach will require to be linked inside the document. // attachmentPrefix = attachment // Optional proxy configuration, only used to access Confluence // schema supports http and https // proxy = [host: 'my.proxy.com', port: 1234, schema: 'http'] // Optional: specify which Confluence OpenAPI Macro should be used to render OpenAPI definitions // possible values: [confluence-open-api, open-api, true]. true is the same as confluence-open-api for backward compatibility // useOpenapiMacro = confluence-open-api } CSS Styling Some AsciiDoctor features depend on particular CSS style definitions. Unless these styles are defined, some formatting that is present in the HTML version will not be represented when published to Confluence. To configure Confluence to include additional style definitions: Log in to Confluence as a space admin Go to the desired space Select Space tools, Look and Feel, Stylesheet Click Edit, enter the desired style definitions, and click Save The default style definitions can be found in the AsciiDoc project as asciidoctor-default.css . Note that you likely do NOT want to include the whole thing, as some of the definitions are likely to disrupt Confluence&#8217;s layout. The following style definitions are believed to be Confluence-compatible, and enable use of the built-in roles ( big / small , underline / overline / line-through , COLOR / COLOR -background for the sixteen HTML color names ): .big{font-size:larger} .small{font-size:smaller} .underline{text-decoration:underline} .overline{text-decoration:overline} .line-through{text-decoration:line-through} .aqua{color:#00bfbf} .aqua-background{background-color:#00fafa} .black{color:#000} .black-background{background-color:#000} .blue{color:#0000bf} .blue-background{background-color:#0000fa} .fuchsia{color:#bf00bf} .fuchsia-background{background-color:#fa00fa} .gray{color:#606060} .gray-background{background-color:#7d7d7d} .green{color:#006000} .green-background{background-color:#007d00} .lime{color:#00bf00} .lime-background{background-color:#00fa00} .maroon{color:#600000} .maroon-background{background-color:#7d0000} .navy{color:#000060} .navy-background{background-color:#00007d} .olive{color:#606000} .olive-background{background-color:#7d7d00} .purple{color:#600060} .purple-background{background-color:#7d007d} .red{color:#bf0000} .red-background{background-color:#fa0000} .silver{color:#909090} .silver-background{background-color:#bcbcbc} .teal{color:#006060} .teal-background{background-color:#007d7d} .white{color:#bfbfbf} .white-background{background-color:#fafafa} .yellow{color:#bfbf00} .yellow-background{background-color:#fafa00} Source publishToConfluence.gradle task publishToConfluence( description: 'publishes the HTML rendered output to confluence', group: 'docToolchain' ) { doLast { logger.info(docToolchain&gt; docDir: +docDir) binding.setProperty('config',config) binding.setProperty('docDir',docDir) evaluate(new File(projectDir, 'scripts/asciidoc2confluence.groovy')) } } scripts/asciidoc2confluence.groovy /** * Created by Ralf D. Mueller and Alexander Heusingfeld * https://github.com/rdmueller/asciidoc2confluence * * this script expects an HTML document created with AsciiDoctor * in the following style (default AsciiDoctor output) * &lt;div class=sect1&gt; * &lt;h2&gt;Page Title&lt;/h2&gt; * &lt;div class=sectionbody&gt; * &lt;div class=sect2&gt; * &lt;h3&gt;Sub-Page Title&lt;/h3&gt; * &lt;/div&gt; * &lt;div class=sect2&gt; * &lt;h3&gt;Sub-Page Title&lt;/h3&gt; * &lt;/div&gt; * &lt;/div&gt; * &lt;/div&gt; * &lt;div class=sect1&gt; * &lt;h2&gt;Page Title&lt;/h2&gt; * ... * &lt;/div&gt; * */ /* Additions for issue #342 marked as #342-dierk42 ;-) */ // some dependencies /** @Grapes( [@Grab('org.jsoup:jsoup:1.8.2'), @Grab('org.codehaus.groovy.modules.http-builder:http-builder:0.6' ), @Grab('org.apache.httpcomponents:httpmime:4.5.1')] ) **/ import org.jsoup.Jsoup import org.jsoup.parser.Parser import org.jsoup.nodes.Entities.EscapeMode import org.jsoup.nodes.Document import org.jsoup.nodes.Document.OutputSettings import org.jsoup.nodes.Element import org.jsoup.nodes.TextNode import org.jsoup.select.Elements import groovyx.net.http.RESTClient import groovyx.net.http.HttpResponseException import groovyx.net.http.HTTPBuilder import groovyx.net.http.EncoderRegistry import groovyx.net.http.ContentType import java.security.MessageDigest //to upload attachments: import org.apache.http.entity.mime.MultipartEntity import org.apache.http.entity.mime.content.StringBody import org.apache.http.entity.mime.content.InputStreamBody import org.apache.http.entity.mime.HttpMultipartMode import groovyx.net.http.Method def CDATA_PLACEHOLDER_START = '&lt;cdata-placeholder&gt;' def CDATA_PLACEHOLDER_END = '&lt;/cdata-placeholder&gt;' def baseUrl def allPages // configuration def confluenceSpaceKey def confluenceCreateSubpages def confluencePagePrefix def baseApiPath = new URI(config.confluence.api).path // helper functions def MD5(String s) { MessageDigest.getInstance(MD5).digest(s.bytes).encodeHex().toString() } // for getting better error message from the REST-API // LuisMuniz: return the action's result, if successful. def trythis(Closure action) { try { action.call() } catch (HttpResponseException error) { println something went wrong - got an http response code +error.response.status+: switch (error.response.status) { case '401': println (error.response.data.toString().replaceAll(^.*Reason,Reason)) println please check your confluence credentials in config file or passed parameters throw new Exception(missing authentication credentials) break case '400': println error.response.data.message println please check the ancestorId in your config file throw new Exception(Parent does not exist) break default: println error.response.data } null } } def parseAdmonitionBlock(block, String type) { content = block.select(.content).first() titleElement = content.select(.title) titleText = '' if(titleElement != null) { titleText = &lt;ac:parameter ac:name=\title\&gt;${titleElement.text()}&lt;/ac:parameter&gt; titleElement.remove() } block.after(&lt;ac:structured-macro ac:name=\${type}\&gt;${titleText}&lt;ac:rich-text-body&gt;${content}&lt;/ac:rich-text-body&gt;&lt;/ac:structured-macro&gt;) block.remove() } /* #342-dierk42 add labels to a Confluence page. Labels are taken from :keywords: which are converted as meta tags in HTML. Building the array: see below Confluence allows adding labels only after creation of a page. Therefore we need extra API calls. Currently the labels are added one by one. Suggestion for improvement: Build a label structure of all labels an place them with one call. Replaces exisiting labels. No harm Does not check for deleted labels when keywords are deleted from source document! */ def addLabels = { def pageId, def labelsArray -&gt; //https://docs.atlassian.com/confluence/REST/latest/ def api = new RESTClient(config.confluence.api) //this fixes the encoding (dierk42: Is this needed here? Don't know) api.encoderRegistry = new EncoderRegistry( charset: 'utf-8' ) def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'X-Atlassian-Token':'no-check' ] // Attach each label in a API call of its own. The only prefix possible // in our own Confluence is 'global' labelsArray.each { label -&gt; label_data = [ prefix : 'global', name : label ] trythis { // attach label to page pageId // https://developer.atlassian.com/display/CONFDEV/Confluence+REST+API+Examples#ConfluenceRESTAPIExamples-Updatingapage def res = api.post(contentType: ContentType.JSON, path: 'content/' + pageId + /label, body: label_data, headers: headers) } println added label  + label +  to page ID  + pageId } } def uploadAttachment = { def pageId, String url, String fileName, String note -&gt; def is def localHash if (url.startsWith('http')) { is = new URL(url).openStream() //build a hash of the attachment localHash = MD5(new URL(url).openStream().text) } else { is = new File(url).newDataInputStream() //build a hash of the attachment localHash = MD5(new File(url).newDataInputStream().text) } //https://docs.atlassian.com/confluence/REST/latest/ def api = new RESTClient(config.confluence.api) //this fixes the encoding api.encoderRegistry = new EncoderRegistry( charset: 'utf-8' ) if (config.confluence.proxy) { api.setProxy(config.confluence.proxy.host, config.confluence.proxy.port, config.confluence.proxy.schema ?: 'http') } def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'X-Atlassian-Token':'no-check' ] //Add api key and value to REST API request header if configured - required for authentification. if (config.confluence.apikey) { headers.keyid = config.confluence.apikey } //check if attachment already exists def result = nothing def attachment = api.get(path: 'content/' + pageId + '/child/attachment', query: [ 'filename': fileName, ], headers: headers).data def http if (attachment.size==1) { // attachment exists. need an update? def remoteHash = 0 if (attachment.results[0].extensions.comment != null) { remoteHash = attachment.results[0].extensions.comment.replaceAll((?sm).*#([^#]+)#.*,'$1') } if (remoteHash!=localHash) { //hash is different -&gt; attachment needs to be updated http = new HTTPBuilder(config.confluence.api + 'content/' + pageId + '/child/attachment/' + attachment.results[0].id + '/data') println  updated attachment } } else { http = new HTTPBuilder(config.confluence.api + 'content/' + pageId + '/child/attachment') } if (http) { if (config.confluence.proxy) { http.setProxy(config.confluence.proxy.host, config.confluence.proxy.port, config.confluence.proxy.schema ?: 'http') } http.request(Method.POST) { req -&gt; requestContentType: multipart/form-data MultipartEntity multiPartContent = new MultipartEntity(HttpMultipartMode.BROWSER_COMPATIBLE) // Adding Multi-part file parameter file multiPartContent.addPart(file, new InputStreamBody(is, fileName)) // Adding another string parameter comment multiPartContent.addPart(comment, new StringBody(note + \r\n# + localHash + #)) req.setEntity(multiPartContent) headers.each { key, value -&gt; req.addHeader(key, value) } } } } def realTitle = { pageTitle -&gt; confluencePagePrefix + pageTitle + confluencePageSuffix } def rewriteMarks = { body -&gt; // Confluence strips out mark elements. Replace them with default formatting. body.select('mark').wrap('&lt;span style=background:#ff0;color:#000&gt;&lt;/style&gt;').unwrap() } // #352-LuisMuniz: Helper methods // Fetch all pages of the space. Only keep relevant info in the pages Map // The map is indexed by lower-case title def retrieveAllPages = { RESTClient api, Map headers, String spaceKey -&gt; if (allPages != null) { println allPages already retrieved allPages } else { boolean morePages=true int start=0 def request = [ 'type' : 'page', 'spaceKey': spaceKey, 'expand' : 'ancestors', 'limit' : 100 ] allPages =[:] while(morePages) { def results = trythis { request.start=start def args = [ 'headers': headers, 'path' : ${baseApiPath}content, 'query' : request, ] api.get(args).data.results } ?: [] if (results.empty) { morePages=false } else { start += results.size } results.inject(allPages) { Map acc, Map match -&gt; //unique page names in confluence, so we can get away with indexing by title def ancestors = match.ancestors.collect { it.id } acc[match.title.toLowerCase()] = [ title : match.title, id : match.id, parentId: ancestors.isEmpty() ? null : ancestors.last() ] acc } } allPages } } // Retrieve a page by id with contents and version def retrieveFullPage = { RESTClient api, Map headers, String id -&gt; trythis { api.get( [ 'headers': headers, 'path' : ${baseApiPath}content/${id}, 'query' : ['expand': 'body.storage,version'], ] ).data } ?: [:] } //if a parent has been specified, check whether a page has the same parent. boolean hasRequestedParent(Map existingPage, String requestedParentId) { if (requestedParentId) { existingPage.parentId == requestedParentId } else { true } } def rewriteDescriptionLists = { body -&gt; def TAGS = [ dt: 'th', dd: 'td' ] body.select('dl').each { dl -&gt; // WHATWG allows wrapping dt/dd in divs, simply unwrap them dl.select('div').each { it.unwrap() } // group dts and dds that belong together, usually it will be a 1:1 relation // but HTML allows for different constellations def rows = [] def current = [dt: [], dd: []] rows &lt;&lt; current dl.select('dt, dd').each { child -&gt; def tagName = child.tagName() if (tagName == 'dt' &amp;&amp; current.dd.size() &gt; 0) { // dt follows dd, start a new group current = [dt: [], dd: []] rows &lt;&lt; current } current[tagName] &lt;&lt; child.tagName(TAGS[tagName]) child.remove() } rows.each { row -&gt; def sizes = [dt: row.dt.size(), dd: row.dd.size()] def rowspanIdx = [dt: -1, dd: sizes.dd - 1] def rowspan = Math.abs(sizes.dt - sizes.dd) + 1 def max = sizes.dt if (sizes.dt &lt; sizes.dd) { max = sizes.dd rowspanIdx = [dt: sizes.dt - 1, dd: -1] } (0..&lt;max).each { idx -&gt; def tr = dl.appendElement('tr') ['dt', 'dd'].each { type -&gt; if (sizes[type] &gt; idx) { tr.appendChild(row[type][idx]) if (idx == rowspanIdx[type] &amp;&amp; rowspan &gt; 1) { row[type][idx].attr('rowspan', ${rowspan}) } } else if (idx == 0) { tr.appendElement(TAGS[type]).attr('rowspan', ${rowspan}) } } } } dl.wrap('&lt;table&gt;&lt;/table&gt;') .unwrap() } } def rewriteInternalLinks = { body, anchors, pageAnchors -&gt; // find internal cross-references and replace them with link macros body.select('a[href]').each { a -&gt; def href = a.attr('href') if (href.startsWith('#')) { def anchor = href.substring(1) def pageTitle = anchors[anchor] ?: pageAnchors[anchor] if (pageTitle &amp;&amp; a.text()) { // as Confluence insists on link texts to be contained // inside CDATA, we have to strip all HTML and // potentially loose styling that way. a.html(a.text()) a.wrap(&lt;ac:link${anchors.containsKey(anchor) ? ' ac:anchor=' + anchor + '' : ''}&gt;&lt;/ac:link&gt;) .before(&lt;ri:page ri:content-title=\${realTitle pageTitle}\/&gt;) .wrap(&lt;ac:plain-text-link-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-link-body&gt;) .unwrap() } } } } def rewriteJiraLinks = { body -&gt; // find links to jira tickets and replace them with jira macros body.select('a[href]').each { a -&gt; def href = a.attr('href') if (href.startsWith(config.jira.api + /browse/)) { def ticketId = a.text() a.before(&lt;ac:structured-macro ac:name=\jira\ ac:schema-version=\1\&gt; &lt;ac:parameter ac:name=\key\&gt;${ticketId}&lt;/ac:parameter&gt; &lt;ac:parameter ac:name=\serverId\&gt;${config.confluence.jiraServerId}&lt;/ac:parameter&gt; &lt;/ac:structured-macro&gt;) a.remove() } } } def rewriteCodeblocks = { body -&gt; body.select('pre &gt; code').each { code -&gt; if (code.attr('data-lang')) { code.select('span[class]').each { span -&gt; span.unwrap() } code.select('i[class]').each { i -&gt; i.unwrap() } code.select('b').each { b -&gt; b.before( // ) b.unwrap() } code.before(&lt;ac:parameter ac:name=\language\&gt;${code.attr('data-lang')}&lt;/ac:parameter&gt;) } code.parent() // pre now .wrap('&lt;ac:structured-macro ac:name=code&gt;&lt;/ac:structured-macro&gt;') .unwrap() code.wrap(&lt;ac:plain-text-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-body&gt;) .unwrap() } } def rewriteOpenAPI = { org.jsoup.nodes.Element body -&gt; if (config.confluence.useOpenapiMacro == true || config.confluence.useOpenapiMacro == 'confluence-open-api') { body.select('div.openapi pre &gt; code').each { code -&gt; def parent=code.parent() def rawYaml=code.wholeText() code.parent() .wrap('&lt;ac:structured-macro ac:name=confluence-open-api ac:schema-version=1 ac:macro-id=1dfde21b-6111-4535-928a-470fa8ae3e7d&gt;&lt;/ac:structured-macro&gt;') .unwrap() code.wrap(&lt;ac:plain-text-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-body&gt;) .replaceWith(new TextNode(rawYaml)) } } else if (config.confluence.useOpenapiMacro == 'open-api') { body.select('div.openapi pre &gt; code').each { code -&gt; def parent=code.parent() def rawYaml=code.wholeText() code.parent() .wrap('&lt;ac:structured-macro ac:name=open-api ac:schema-version=1 data-layout=default ac:macro-id=4302c9d8-fca4-4f14-99a9-9885128870fa&gt;&lt;/ac:structured-macro&gt;') .unwrap() // default: show download button code.before('&lt;ac:parameter ac:name=showDownloadButton&gt;true&lt;/ac:parameter&gt;') code.wrap(&lt;ac:plain-text-body&gt;${CDATA_PLACEHOLDER_START}${CDATA_PLACEHOLDER_END}&lt;/ac:plain-text-body&gt;) .replaceWith(new TextNode(rawYaml)) } } } def unescapeCDATASections = { html -&gt; def start = html.indexOf(CDATA_PLACEHOLDER_START) while (start &gt; -1) { def end = html.indexOf(CDATA_PLACEHOLDER_END, start) if (end &gt; -1) { def prefix = html.substring(0, start) + CDATA_PLACEHOLDER_START def suffix = html.substring(end) def unescaped = html.substring(start + CDATA_PLACEHOLDER_START.length(), end) .replaceAll('&amp;lt;', '&lt;').replaceAll('&amp;gt;', '&gt;') .replaceAll('&amp;amp;', '&amp;') html = prefix + unescaped + suffix } start = html.indexOf(CDATA_PLACEHOLDER_START, start + 1) } html } //modify local page in order to match the internal confluence storage representation a bit better //definition lists are not displayed by confluence, so turn them into tables //body can be of type Element or Elements def deferredUpload = [] def parseBody = { body, anchors, pageAnchors -&gt; rewriteOpenAPI body body.select('div.paragraph').unwrap() body.select('div.ulist').unwrap() body.select('div.sect3').unwrap() [ 'note':'info', 'warning':'warning', 'important':'warning', 'caution':'note', 'tip':'tip' ].each { adType, cType -&gt; body.select('.admonitionblock.'+adType).each { block -&gt; parseAdmonitionBlock(block, cType) } } //special for the arc42-template body.select('div.arc42help').select('.content') .wrap('&lt;ac:structured-macro ac:name=expand&gt;&lt;/ac:structured-macro&gt;') .wrap('&lt;ac:rich-text-body&gt;&lt;/ac:rich-text-body&gt;') .wrap('&lt;ac:structured-macro ac:name=info&gt;&lt;/ac:structured-macro&gt;') .before('&lt;ac:parameter ac:name=title&gt;arc42&lt;/ac:parameter&gt;') .wrap('&lt;ac:rich-text-body&gt;&lt;p&gt;&lt;/p&gt;&lt;/ac:rich-text-body&gt;') body.select('div.arc42help').unwrap() body.select('div.title').wrap(&lt;strong&gt;&lt;/strong&gt;).before(&lt;br /&gt;).wrap(&lt;div&gt;&lt;/div&gt;) body.select('div.listingblock').wrap(&lt;p&gt;&lt;/p&gt;).unwrap() // see if we can find referenced images and fetch them new File(tmp/images/.).mkdirs() // find images, extract their URLs for later uploading (after we know the pageId) and replace them with this macro: // &lt;ac:image ac:align=center ac:width=500&gt; // &lt;ri:attachment ri:filename=deployment-context.png/&gt; // &lt;/ac:image&gt; body.select('img').each { img -&gt; img.attributes().each { attribute -&gt; //println attribute.dump() } def src = img.attr('src') def imgWidth = img.attr('width')?:500 def imgAlign = img.attr('align')?:center println  image: +src //it is not an online image, so upload it to confluence and use the ri:attachment tag if(!src.startsWith(http)) { def newUrl = baseUrl.toString().replaceAll('\\\\','/').replaceAll('/[^/]*$','/')+src def fileName = java.net.URLDecoder.decode((src.tokenize('/')[-1]),UTF-8) newUrl = java.net.URLDecoder.decode(newUrl,UTF-8) trythis { deferredUpload &lt;&lt; [0,newUrl,fileName,automatically uploaded] } img.after(&lt;ac:image ac:align=\${imgAlign}\ ac:width=\${imgWidth}\&gt;&lt;ri:attachment ri:filename=\${fileName}\/&gt;&lt;/ac:image&gt;) } // it is an online image, so we have to use the ri:url tag else { img.after(&lt;ac:image ac:align=\imgAlign\ ac:width=\${imgWidth}\&gt;&lt;ri:url ri:value=\${src}\/&gt;&lt;/ac:image&gt;) } img.remove() } if(config.confluence.enableAttachments){ attachmentPrefix = config.confluence.attachmentPrefix ? config.confluence.attachmentPrefix : 'attachment' body.select('a').each { link -&gt; def src = link.attr('href') println  attachment src: +src //upload it to confluence and use the ri:attachment tag if(src.startsWith(attachmentPrefix)) { def newUrl = baseUrl.toString().replaceAll('\\\\','/').replaceAll('/[^/]*$','/')+src def fileName = java.net.URLDecoder.decode((src.tokenize('/')[-1]),UTF-8) newUrl = java.net.URLDecoder.decode(newUrl,UTF-8) trythis { deferredUpload &lt;&lt; [0,newUrl,fileName,automatically uploaded non-image attachment by docToolchain] } def uriArray=fileName.split(/) def pureFilename = uriArray[uriArray.length-1] def innerhtml = link.html() link.after(&lt;ac:structured-macro ac:name=\view-file\ ac:schema-version=\1\&gt;&lt;ac:parameter ac:name=\name\&gt;&lt;ri:attachment ri:filename=\${pureFilename}\/&gt;&lt;/ac:parameter&gt;&lt;/ac:structured-macro&gt;) link.after(&lt;ac:link&gt;&lt;ri:attachment ri:filename=\${pureFilename}\/&gt;&lt;ac:plain-text-link-body&gt; &lt;![CDATA[\${innerhtml}\]]&gt;&lt;/ac:plain-text-link-body&gt;&lt;/ac:link&gt;) link.remove() } } } if(config.confluence.jiraServerId){ rewriteJiraLinks body } rewriteMarks body rewriteDescriptionLists body rewriteInternalLinks body, anchors, pageAnchors //sanitize code inside code tags rewriteCodeblocks body def pageString = unescapeCDATASections body.html().trim() //change some html elements through simple substitutions pageString = pageString .replaceAll('&lt;br&gt;','&lt;br /&gt;') .replaceAll('&lt;/br&gt;','&lt;br /&gt;') .replaceAll('&lt;a([^&gt;]*)&gt;&lt;/a&gt;','') .replaceAll(CDATA_PLACEHOLDER_START,'&lt;![CDATA[') .replaceAll(CDATA_PLACEHOLDER_END,']]&gt;') return pageString } // the create-or-update functionality for confluence pages // #342-dierk42: added parameter 'keywords' def pushToConfluence = { pageTitle, pageBody, String parentId, anchors, pageAnchors, keywords -&gt; def api = new RESTClient(config.confluence.api) def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'Content-Type':'application/json; charset=utf-8' ] //Add api key and value to REST API request header if configured - required for authentification. if (config.confluence.apikey) { headers.keyid = config.confluence.apikey } String realTitleLC = realTitle(pageTitle).toLowerCase() //this fixes the encoding api.encoderRegistry = new EncoderRegistry( charset: 'utf-8' ) if (config.confluence.proxy) { api.setProxy(config.confluence.proxy.host, config.confluence.proxy.port, config.confluence.proxy.schema ?: 'http') } //try to get an existing page localPage = parseBody(pageBody, anchors, pageAnchors) def localHash = MD5(localPage) if(config.confluence.disableToC){ def prefix = (config.confluence.extraPageContent?:'') localPage = prefix+localPage localHash = MD5(localPage) localPage += '&lt;p style=display:none&gt;hash: #'+localHash+'#&lt;/p&gt;' }else{ def default_toc = '&lt;p&gt;&lt;ac:structured-macro ac:name=toc/&gt;&lt;/p&gt;' def prefix = (config.confluence.tableOfContents?:default_toc)+(config.confluence.extraPageContent?:'') localPage = prefix+localPage def default_children = '&lt;p&gt;&lt;ac:structured-macro ac:name=children&gt;&lt;ac:parameter ac:name=sort&gt;creation&lt;/ac:parameter&gt;&lt;/ac:structured-macro&gt;&lt;/p&gt;' localPage += (config.confluence.tableOfChildren?:default_children) localHash = MD5(localPage) localPage += '&lt;p style=display:none&gt;hash: #'+localHash+'#&lt;/p&gt;' } def request = [ type : 'page', title: realTitle(pageTitle), space: [ key: confluenceSpaceKey ], body : [ storage: [ value : localPage, representation: 'storage' ] ] ] if (parentId) { request.ancestors = [ [ type: 'page', id: parentId] ] } def pages = retrieveAllPages(api, headers, config.confluence.spaceKey) // println Suche nach vorhandener Seite:  + pageTitle Map existingPage = pages[realTitleLC] def page if (existingPage) { if (hasRequestedParent(existingPage, parentId)) { page = retrieveFullPage(api, headers, existingPage.id) } else { page = null } } else { page = null } // println Gefunden:  + page.id +  Titel:  + page.title if (page) { println found existing page:  + page.id + version +page.version.number //extract hash from remote page to see if it is different from local one def remotePage = page.body.storage.value.toString().trim() def remoteHash = remotePage =~ /(?ms)hash: #([^#]+)#/ remoteHash = remoteHash.size()==0?:remoteHash[0][1] // println remoteHash:  + remoteHash // println localHash:  + localHash if (remoteHash == localHash) { println page hasn't changed! deferredUpload.each { uploadAttachment(page?.id, it[1], it[2], it[3]) } deferredUpload = [] // #324-dierk42: Add keywords as labels to page. if (keywords) { addLabels(page.id, keywords) } return page.id } else { trythis { // update page // https://developer.atlassian.com/display/CONFDEV/Confluence+REST+API+Examples#ConfluenceRESTAPIExamples-Updatingapage request.id = page.id request.version = [number: (page.version.number as Integer) + 1] def res = api.put(contentType: ContentType.JSON, requestContentType : ContentType.JSON, path: 'content/' + page.id, body: request, headers: headers) } println &gt; updated page +page.id deferredUpload.each { uploadAttachment(page.id, it[1], it[2], it[3]) } deferredUpload = [] // #324-dierk42: Add keywords as labels to page. if (keywords) { addLabels(page.id, keywords) } return page.id } } else { //#352-LuisMuniz if the existing page's parent does not match the requested parentId, fail if (existingPage &amp;&amp; !hasRequestedParent(existingPage, parentId)) { throw new IllegalArgumentException(Cannot create page, page with the same  + title=${existingPage.title}  + with id=${existingPage.id} already exists in the space.  + A Confluence page title must be unique within a space, consider specifying a 'confluencePagePrefix' in ConfluenceConfig.groovy) } //create a page trythis { page = api.post(contentType: ContentType.JSON, requestContentType: ContentType.JSON, path: 'content', body: request, headers: headers) } println &gt; created page +page?.data?.id deferredUpload.each { uploadAttachment(page?.data?.id, it[1], it[2], it[3]) } deferredUpload = [] // #324-dierk42: Add keywords as labels to page. if (keywords) { addLabels(page?.data?.id, keywords) } return page?.data?.id } } def parseAnchors = { page -&gt; def anchors = [:] page.body.select('[id]').each { anchor -&gt; def name = anchor.attr('id') anchors[name] = page.title anchor.before(&lt;ac:structured-macro ac:name=\anchor\&gt;&lt;ac:parameter ac:name=\\&gt;${name}&lt;/ac:parameter&gt;&lt;/ac:structured-macro&gt;) } anchors } def pushPages pushPages = { pages, anchors, pageAnchors, labels -&gt; pages.each { page -&gt; println page.title def id = pushToConfluence page.title, page.body, page.parent, anchors, pageAnchors, labels page.children*.parent = id // println Push children von id  + id pushPages page.children, anchors, pageAnchors, labels // println Ende Push children von id  + id } } def recordPageAnchor = { head -&gt; def a = [:] if (head.attr('id')) { a[head.attr('id')] = head.text() } a } def promoteHeaders = { tree, start, offset -&gt; (start..7).each { i -&gt; tree.select(h${i}).tagName(h${i-offset}).before('&lt;br /&gt;') } } def retrievePageIdByName = { String name -&gt; def api = new RESTClient(config.confluence.api) def headers = [ 'Authorization': 'Basic ' + config.confluence.credentials, 'Content-Type':'application/json; charset=utf-8' ] trythis { def request = [ 'title' : name, 'spaceKey' : confluenceSpaceKey ] api.get( [ 'headers': headers, 'path' : ${baseApiPath}content, 'query' : request, ] ).data.results?.getAt(0)?.id } ?: null } config.confluence.input.each { input -&gt; input.file = ${docDir}/${input.file} println publish ${input.file} if (input.file ==~ /.*[.](ad|adoc|asciidoc)$/) { println convert ${input.file} groovy asciidoc2html.groovy ${input.file}.execute() input.file = input.file.replaceAll(/[.](ad|adoc|asciidoc)$/, '.html') println to ${input.file} } // assignend, but never used in pushToConfluence(...) (fixed here) confluenceSpaceKey = input.spaceKey ?: config.confluence.spaceKey confluenceCreateSubpages = (input.createSubpages != null) ? input.createSubpages : config.confluence.createSubpages // hard to read in case of using :sectnums: -&gt; so we add a suffix confluencePagePrefix = input.pagePrefix ?: config.confluence.pagePrefix // added confluencePageSuffix = input.pageSuffix ?: config.confluence.pageSuffix confluencePreambleTitle = input.preambleTitle ?: config.confluence.preambleTitle def html = input.file ? new File(input.file).getText('utf-8') : new URL(input.url).getText() baseUrl = input.file ? new File(input.file) : new URL(input.url) Document dom = Jsoup.parse(html, 'utf-8', Parser.xmlParser()) dom.outputSettings().prettyPrint(false);//makes html() preserve linebreaks and spacing dom.outputSettings().escapeMode(org.jsoup.nodes.Entities.EscapeMode.xhtml); //This will ensure xhtml validity regarding entities dom.outputSettings().charset(UTF-8); //does no harm :-) // if ancestorName is defined try to find machingAncestorId in confluence def retrievedAncestorId if (input.ancestorName) { // Retrieve a page id by name retrievedAncestorId = retrievePageIdByName(input.ancestorName) println(Retrieved pageId for given ancestorName '${input.ancestorName}' is ${retrievedAncestorId}) } // if input does not contain an ancestorName, check if there is ancestorId, otherwise check if there is a global one def parentId = retrievedAncestorId ?: input.ancestorId ?: config.confluence.ancestorId // if parentId is still not set, create a new parent page (parentId = null) parentId = parentId ?: null //println(ancestorName: '${input.ancestorName}', ancestorId: ${input.ancestorId} ---&gt; final parentId: ${parentId}) def anchors = [:] def pageAnchors = [:] def sections = pages = [] // #342-dierk42: get the keywords from the meta tags def keywords = [] dom.select('meta[name=keywords]').each { kw -&gt; kws = kw.attr('content').split(',') kws.each { skw -&gt; keywords &lt;&lt; skw.trim() } println Keywords: + keywords } // let's try to select the first page and push it to confluence dom.select('div#preamble div.sectionbody').each { pageBody -&gt; pageBody.select('div.sect2').unwrap() def preamble = [ title: confluencePreambleTitle ?: arc42, body: pageBody, children: [], parent: parentId ] pages &lt;&lt; preamble sections = preamble.children parentId = null anchors.putAll(parseAnchors(preamble)) } // &lt;div class=sect1&gt; are the main headings // let's extract these dom.select('div.sect1').each { sect1 -&gt; Elements pageBody = sect1.select('div.sectionbody') def currentPage = [ title: sect1.select('h2').text(), body: pageBody, children: [], parent: parentId ] pageAnchors.putAll(recordPageAnchor(sect1.select('h2'))) if (confluenceCreateSubpages) { pageBody.select('div.sect2').each { sect2 -&gt; def title = sect2.select('h3').text() pageAnchors.putAll(recordPageAnchor(sect2.select('h3'))) sect2.select('h3').remove() def body = Jsoup.parse(sect2.toString(),'utf-8', Parser.xmlParser()) body.outputSettings(new Document.OutputSettings().prettyPrint(false)) def subPage = [ title: title, body: body ] currentPage.children &lt;&lt; subPage promoteHeaders sect2, 4, 3 anchors.putAll(parseAnchors(subPage)) } pageBody.select('div.sect2').remove() } else { pageBody.select('div.sect2').unwrap() promoteHeaders sect1, 3, 2 } sections &lt;&lt; currentPage anchors.putAll(parseAnchors(currentPage)) } pushPages pages, anchors, pageAnchors, keywords if (parentId) { println published to ${config.confluence.api - rest/api/}spaces/${confluenceSpaceKey}/pages/${parentId} } else { println published to ${config.confluence.api - rest/api/}spaces/${confluenceSpaceKey} } }  .gravatar img { margin-left: 3px; border-radius: 4px; } 1.10. convertToDocx Before You Begin Install pandoc . Ensure that 'docbook' and 'docx' are added to the inputFiles formats in Config.groovy. As an optional step, specify a reference doc file with custom stylesheets (see task createReferenceDoc ). 1 minute to read Tip Blog-Post: Render AsciiDoc to docx (MS Word) Source pandoc.gradle task convertToDocx ( group: 'docToolchain', description: 'converts file to .docx via pandoc. Needs pandoc installed.', type: Exec ) { // All files with option `docx` in config.groovy is converted to docbook and then to docx. def sourceFilesDocx = sourceFiles.findAll { 'docx' in it.formats } sourceFilesDocx.each { def sourceFile = it.file.replace('.adoc', '.xml') def targetFile = sourceFile.replace('.xml', '.docx') workingDir $targetDir/docbook executable = pandoc if(referenceDocFile?.trim()) { args = [-r,docbook, -t,docx, -o,../docx/$targetFile, --reference-doc=${docDir}/${referenceDocFile}, sourceFile] } else { args = [-r,docbook, -t,docx, -o,../docx/$targetFile, sourceFile] } } doFirst { new File($targetDir/docx/).mkdirs() } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.11. createReferenceDoc Before You Begin Install pandoc . 1 minute to read About This Task This task creates a reference docx file used by pandoc during docbook-to-docx conversion. Use task convertToDocx to edit this file so it uses your preferred styles. Important The contents of the reference docx are ignored, but its stylesheets and document properties (including margins, page size, header and footer) are used in the new docx. For more information, see Pandoc User&#8217;s Guide: Options affecting specific writers (--reference-doc) And if you have problems with changing the default table style: see https://github.com/jgm/pandoc/issues/3275 . Config.groovy Notes The 'referenceDocFile' property must be set to your custom reference file in Config.groovy: inputPath = '.' // use a style reference file in the input path for conversion from docbook to docx referenceDocFile = ${inputPath}/my-ref-file.docx Source pandoc.gradle task createReferenceDoc ( group: 'docToolchain helper', description: 'creates a docx file to be used as a format style reference in task convertToDocx. Needs pandoc installed.', type: Exec ) { workingDir $docDir executable = pandoc args = [-o, ${docDir}/${referenceDocFile}, --print-default-data-file, reference.docx] doFirst { if(!(referenceDocFile?.trim())) { throw new GradleException(Option `referenceDocFile` is not defined in config.groovy or has an empty value.) } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.12. convertToEpub 1 minute to read At a Glance Dependency generateDocBook About This Task This task uses pandoc to convert the DocBook output from AsciiDoctor to ePub. This publishes the output as an eBook which can be read using any eBook reader. The resulting file can be found in build/docs/epub . Further Reading Turn your Document into an Audio-Book blog post. Source pandoc.gradle task convertToEpub ( group: 'docToolchain', description: 'converts file to .epub via pandoc. Needs pandoc installed.', type: Exec ) { // All files with option `epub` in config.groovy is converted to docbook and then to epub. def sourceFilesEpub = sourceFiles.findAll { 'epub' in it.formats } sourceFilesEpub.each { def sourceFile = it.file.replace('.adoc', '.xml') def targetFile = sourceFile.replace('.xml', '.epub') workingDir $targetDir/docbook executable = pandoc args = ['-r','docbook', '-t','epub', '-o',../epub/$targetFile, sourceFile] } doFirst { new File($targetDir/epub/).mkdirs() } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.13. exportEA Important Currently this feature is WINDOWS-only. See related issue 4 minutes to read About This Task By default, no special configuration is necessary. However, several optional parameter configurations are available to support a project and packages to be used for export. These parameters can be used independently from one another. A sample of how to edit your projects' Config.groovy is provided in the 'Config.groovy' of the docToolchain project itself. The Optional Parameter Configurations connection Either set the connection to a certain project, or comment it out to use all project files inside the src folder or its child folder. packageFilter Add one or multiple packageGUIDs to be used for export. All packages are analysed, if no packageFilter is set. exportPath Relative path to base 'docDir' to which the diagrams and notes are to be exported. Default: src/docs. Example: docDir = 'D:\work\mydoc\' ; exportPath = 'src/pdocs' ; Images will be exported to 'D:\work\mydoc\src\pdocs\images\ea', Notes will be exported to 'D:\work\mydoc\src\pdocs\ea', searchPath Relative path to base 'docDir', in which Enterprise Architect project files are searched Default: src/docs. Example: docDir = 'D:\work\mydoc\' ; exportPath = 'src/projects' ; Lookup for eap and eapx files starts in 'D:\work\mydoc\src\projects' and goes down the folder structure. Note : In case parameter 'connection' is already defined, the searchPath value is also used. exportEA starts opening the database parameter 'connection' first then looks for further project files either in the searchPath (if set) or in the docDir folder of the project. glossaryAsciiDocFormat Whether or not the EA project glossary is exported depends on this parameter. If not set or an empty string, no glossary is exported. The glossaryAsciiDocFormat string is used to format each glossary entry in a certain AsciiDoc format. The following placeholders are defined for the format string: ID, TERM, MEANING, TYPE. One or more can be used by the output format. For example: A valid output format is to include the glossary as a flat list. The file can be included where needed in the documentation. glossaryAsciiDocFormat = TERM:: MEANING Other format strings can be used to include it as a table row. The glossary terms are sorted in alphabetical order. glossaryTypes This parameter is used in case a glossaryAsciiDocFormat is defined, otherwise it is not evaluated. It&#8217;s used to filter for certain types. If the glossaryTypes list is empty, all entries will be used. For example: glossaryTypes = [Business, Technical] diagramAttributes If set, the string is used to create and store diagram attributes to be included in the document alongside a diagram. These placeholders are defined and populated with the diagram attributes, if used in the diagramAttributes string: %DIAGRAM_AUTHOR% , %DIAGRAM_CREATED% , %DIAGRAM_GUID% , %DIAGRAM_MODIFIED% , %DIAGRAM_NAME% , %DIAGRAM_NOTES% , %NEWLINE% Example: diagramAttributes = Last modification: %DIAGRAM_MODIFIED% You can add the string %NEWLINE% where a line break will be added. The resulting text is stored next to the diagram image using the same path and file name, but a different file extension (.ad). This can be included in the document if required. If diagramAttributes is not set or an empty string, no file is written. additionalOptions This parameter is used to define the specific behavior of the export. Currently these options are supported: KeepFirstDiagram If diagrams are not uniquely named, the last diagram will be saved. If you want to prevent diagrams from being overwritten, add this parameter to additionalOptions. Glossary export By setting the glossaryAsciiDocFormat, the glossary terms stored in the EA project will be exported into a folder named 'glossary' below the configured exportPath. In case multiple EA projects are found for export, one glossary per project is exported - each named using the project&#8217;s GUID plus extension '.ad'. Each individual file will be filtered (see glossaryTypes) and sorted in alphabetical order. In addition, a global glossary is created by using all single glossary files. This global file is named 'glossary.ad' and is also placed in the glossary folder. The global glossary is also filtered and sorted. If there is only one EA project, only the global glossary is written. Further Reading Read the JIRA to Sparx EA and Did you Ever Wish you Had Better Diagrams? blog posts. Source build.gradle task exportEA( dependsOn: [streamingExecute], description: 'exports all diagrams and some texts from EA files', group: 'docToolchain' ) { doFirst { } doLast { logger.info(docToolchain &gt; exportEA:  + docDir) logger.info(docToolchain &gt; exportEA:  + mainConfigFile) def configFile = new File(docDir, mainConfigFile) def config = new ConfigSlurper().parse(configFile.text) def scriptParameterString =  def exportPath =  def searchPath =  def glossaryPath =  def readme = This folder contains exported diagrams or notes from Enterprise Architect. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportEA` to re-export files  if (!config.exportEA.connection.isEmpty()) { logger.info(docToolchain &gt; exportEA: found  + config.exportEA.connection) scriptParameterString = scriptParameterString + -c \${config.exportEA.connection}\ } if (!config.exportEA.packageFilter.isEmpty()) { def packageFilterToCreate = config.exportEA.packageFilter as List logger.info(docToolchain &gt; exportEA: package filter list size:  + packageFilterToCreate.size()) packageFilterToCreate.each { packageFilter -&gt; scriptParameterString = scriptParameterString +  -p \${packageFilter}\ } } if (!config.exportEA.exportPath.isEmpty()) { exportPath = new File(docDir, config.exportEA.exportPath).getAbsolutePath() } else { exportPath = new File(docDir, 'src/docs').getAbsolutePath() } if (!config.exportEA.searchPath.isEmpty()) { searchPath = new File(docDir, config.exportEA.searchPath).getAbsolutePath() } else if (!config.exportEA.absoluteSearchPath.isEmpty()) { searchPath = new File(config.exportEA.absoluteSearchPath).getAbsolutePath() } else { searchPath = new File(docDir, 'src').getAbsolutePath() } scriptParameterString = scriptParameterString +  -d \$exportPath\ scriptParameterString = scriptParameterString +  -s \$searchPath\ logger.info(docToolchain &gt; exportEA: exportPath:  + exportPath) //remove old glossary files/folder if exist new File(exportPath, 'glossary').deleteDir() //set the glossary file path in case an output format is configured, other no glossary is written if (!config.exportEA.glossaryAsciiDocFormat.isEmpty()) { //create folder to store glossaries new File(exportPath, 'glossary/.').mkdirs() glossaryPath = new File(exportPath, 'glossary').getAbsolutePath() scriptParameterString = scriptParameterString +  -g \$glossaryPath\ } //configure additional diagram attributes to be exported if (!config.exportEA.diagramAttributes.isEmpty()) { scriptParameterString = scriptParameterString +  -da \$config.exportEA.diagramAttributes\ } //configure additional diagram attributes to be exported if (!config.exportEA.additionalOptions.isEmpty()) { scriptParameterString = scriptParameterString +  -ao \$config.exportEA.additionalOptions\ } //make sure path for notes exists //and remove old notes new File(exportPath, 'ea').deleteDir() //also remove old diagrams new File(exportPath, 'images/ea').deleteDir() //create a readme to clarify things new File(exportPath, 'images/ea/.').mkdirs() new File(exportPath, 'images/ea/readme.ad').write(readme) new File(exportPath, 'ea/.').mkdirs() new File(exportPath, 'ea/readme.ad').write(readme) //execute through cscript in order to make sure that we get WScript.echo right logger.info(docToolchain &gt; exportEA: parameters:  + scriptParameterString) %SystemRoot%\\System32\\cscript.exe //nologo ${projectDir}/scripts/exportEAP.vbs ${scriptParameterString}.executeCmd() //the VB Script is only capable of writing iso-8859-1-Files. //we now have to convert them to UTF-8 new File(exportPath, 'ea/.').eachFileRecurse { file -&gt; if (file.isFile()) { println exported notes  + file.canonicalPath file.write(file.getText('iso-8859-1'), 'utf-8') } } //sort, filter and reformat a glossary if an output format is configured if (!config.exportEA.glossaryAsciiDocFormat.isEmpty()) { def glossaryTypes if (!config.exportEA.glossaryTypes.isEmpty()) { glossaryTypes = config.exportEA.glossaryTypes as List } new GlossaryHandler().execute(glossaryPath, config.exportEA.glossaryAsciiDocFormat, glossaryTypes); } } } scripts/exportEAP.vbs ' based on the Project Interface Example which comes with EA ' http://stackoverflow.com/questions/1441479/automated-method-to-export-enterprise-architect-diagrams Dim EAapp 'As EA.App Dim Repository 'As EA.Repository Dim FS 'As Scripting.FileSystemObject Dim projectInterface 'As EA.Project Const ForAppending = 8 Const ForWriting = 2 ' Helper ' http://windowsitpro.com/windows/jsi-tip-10441-how-can-vbscript-create-multiple-folders-path-mkdir-command Function MakeDir (strPath) Dim strParentPath, objFSO Set objFSO = CreateObject(Scripting.FileSystemObject) On Error Resume Next strParentPath = objFSO.GetParentFolderName(strPath) If Not objFSO.FolderExists(strParentPath) Then MakeDir strParentPath If Not objFSO.FolderExists(strPath) Then objFSO.CreateFolder strPath On Error Goto 0 MakeDir = objFSO.FolderExists(strPath) End Function ' Replaces certain characters with '_' to avoid unwanted file or folder names causing errors or structure failures. ' Regular expression can easily be extended with further characters to be replaced. Function NormalizeName(theName) dim re : Set re = new regexp re.Pattern = [\\/\[\]\s] re.Global = True NormalizeName = re.Replace(theName, _) End Function Sub WriteNote(currentModel, currentElement, notes, prefix) If (Left(notes, 6) = {adoc:) Then strFileName = Mid(notes,7,InStr(notes,})-7) strNotes = Right(notes,Len(notes)-InStr(notes,})) set objFSO = CreateObject(Scripting.FileSystemObject) If (currentModel.Name=Model) Then ' When we work with the default model, we don't need a sub directory path = objFSO.BuildPath(exportDestination,ea/) Else path = objFSO.BuildPath(exportDestination,ea/&amp;NormalizeName(currentModel.Name)&amp;/) End If MakeDir(path) post =  If (prefix&lt;&gt;) Then post = _ End If MakeDir(path&amp;prefix&amp;post) set objFile = objFSO.OpenTextFile(path&amp;prefix&amp;post&amp;strFileName&amp;.ad,ForAppending, True) name = currentElement.Name name = Replace(name,vbCr,) name = Replace(name,vbLf,) if (Left(strNotes, 3) = vbCRLF&amp;|) Then ' content should be rendered as table - so don't interfere with it objFile.WriteLine(vbCRLF) else 'let's add the name of the object objFile.WriteLine(vbCRLF&amp;vbCRLF&amp;.&amp;name) End If objFile.WriteLine(vbCRLF&amp;strNotes) objFile.Close if (prefix&lt;&gt;) Then ' write the same to a second file set objFile = objFSO.OpenTextFile(path&amp;prefix&amp;.ad,ForAppending, True) objFile.WriteLine(vbCRLF&amp;vbCRLF&amp;.&amp;name&amp;vbCRLF&amp;strNotes) objFile.Close End If End If End Sub Sub SyncJira(currentModel, currentDiagram) notes = currentDiagram.notes set currentPackage = Repository.GetPackageByID(currentDiagram.PackageID) updated = 0 created = 0 If (Left(notes, 6) = {jira:) Then WScript.echo  &gt;&gt;&gt;&gt; Diagram jira tag found strSearch = Mid(notes,7,InStr(notes,})-7) Set objShell = CreateObject(WScript.Shell) 'objShell.CurrentDirectory = fso.GetFolder(./scripts) Set objExecObject = objShell.Exec (cmd /K groovy ./scripts/exportEAPJiraPrintHelper.groovy  &amp; strSearch &amp; &amp; exit) strReturn =  x = 0 y = 0 Do While Not objExecObject.StdOut.AtEndOfStream output = objExecObject.StdOut.ReadLine() ' WScript.echo output jiraElement = Split(output,|) name = jiraElement(0)&amp;:&amp;vbCR&amp;vbLF&amp;jiraElement(4) On Error Resume Next Set requirement = currentPackage.Elements.GetByName(name) On Error Goto 0 if (IsObject(requirement)) then ' element already exists requirement.notes =  requirement.notes = requirement.notes&amp;&lt;a href='&amp;jiraElement(5)&amp;'&gt;&amp;jiraElement(0)&amp;&lt;/a&gt;&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Priority: &amp;jiraElement(1)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Created: &amp;jiraElement(2)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Assignee: &amp;jiraElement(3)&amp;vbCR&amp;vbLF requirement.Update() updated = updated + 1 else Set requirement = currentPackage.Elements.AddNew(name,Requirement) requirement.notes =  requirement.notes = requirement.notes&amp;&lt;a href='&amp;jiraElement(5)&amp;'&gt;&amp;jiraElement(0)&amp;&lt;/a&gt;&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Priority: &amp;jiraElement(1)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Created: &amp;jiraElement(2)&amp;vbCR&amp;vbLF requirement.notes = requirement.notes&amp;Assignee: &amp;jiraElement(3)&amp;vbCR&amp;vbLF requirement.Update() currentPackage.Elements.Refresh() Set dia_obj = currentDiagram.DiagramObjects.AddNew(l=&amp;(10+x*200)&amp;;t=&amp;(10+y*50)&amp;;b=&amp;(10+y*50+44)&amp;;r=&amp;(10+x*200+180),) x = x + 1 if (x&gt;3) then x = 0 y = y + 1 end if dia_obj.ElementID = requirement.ElementID dia_obj.Update() created = created + 1 end if Loop Set objShell = Nothing WScript.echo created &amp;created&amp; requirements WScript.echo updated &amp;updated&amp; requirements End If End Sub ' This sub routine checks if the format string defined in diagramAttributes ' does contain any characters. It replaces the known placeholders: ' %DIAGRAM_AUTHOR%, %DIAGRAM_CREATED%, %DIAGRAM_GUID%, %DIAGRAM_MODIFIED%, ' %DIAGRAM_NAME%, %DIAGRAM_NOTES% ' with the attribute values read from the EA diagram object. ' None, one or multiple number of placeholders can be used to create a diagram attribute ' to be added to the document. The attribute string is stored as a file with the same ' path and name as the diagram image, but with suffix .ad. So, it can ' easily be included in an asciidoc file. Sub SaveDiagramAttribute(currentDiagram, path, diagramName) If Len(diagramAttributes) &gt; 0 Then filledDiagAttr = diagramAttributes set objFSO = CreateObject(Scripting.FileSystemObject) filename = objFSO.BuildPath(path, diagramName &amp; .ad) set objFile = objFSO.OpenTextFile(filename, ForWriting, True) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_AUTHOR%, currentDiagram.Author) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_CREATED%, currentDiagram.CreatedDate) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_GUID%, currentDiagram.DiagramGUID) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_MODIFIED%, currentDiagram.ModifiedDate) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_NAME%, currentDiagram.Name) filledDiagAttr = Replace(filledDiagAttr, %DIAGRAM_NOTES%, currentDiagram.Notes) filledDiagAttr = Replace(filledDiagAttr, %NEWLINE%, vbCrLf) objFile.WriteLine(filledDiagAttr) objFile.Close End If End Sub Sub SaveDiagram(currentModel, currentDiagram) Dim exportDiagram ' As Boolean ' Open the diagram Repository.OpenDiagram(currentDiagram.DiagramID) ' Save and close the diagram set objFSO = CreateObject(Scripting.FileSystemObject) If (currentModel.Name=Model) Then ' When we work with the default model, we don't need a sub directory path = objFSO.BuildPath(exportDestination,/images/ea/) Else path = objFSO.BuildPath(exportDestination,/images/ea/ &amp; NormalizeName(currentModel.Name) &amp; /) End If path = objFSO.GetAbsolutePathName(path) MakeDir(path) diagramName = currentDiagram.Name diagramName = Replace(diagramName,vbCr,) diagramName = Replace(diagramName,vbLf,) diagramName = NormalizeName(diagramName) filename = objFSO.BuildPath(path, diagramName &amp; .png) exportDiagram = True If objFSO.FileExists(filename) Then WScript.echo  ---  &amp; filename &amp;  already exists. If Len(additionalOptions) &gt; 0 Then If InStr(additionalOptions, KeepFirstDiagram) &gt; 0 Then WScript.echo  --- Skipping export -- parameter 'KeepFirstDiagram' set. Else WScript.echo  --- Overwriting -- parameter 'KeepFirstDiagram' not set. exportDiagram = False End If Else WScript.echo  --- Overwriting -- parameter 'KeepFirstDiagram' not set. End If End If If exportDiagram Then projectInterface.SaveDiagramImageToFile(filename) WScript.echo  extracted image to  &amp; filename If Not IsEmpty(diagramAttributes) Then SaveDiagramAttribute currentDiagram, path, diagramName End If End If Repository.CloseDiagram(currentDiagram.DiagramID) ' Write the note of the diagram WriteNote currentModel, currentDiagram, currentDiagram.Notes, diagramName&amp;_notes For Each diagramElement In currentDiagram.DiagramObjects Set currentElement = Repository.GetElementByID(diagramElement.ElementID) WriteNote currentModel, currentElement, currentElement.Notes, diagramName&amp;_notes Next For Each diagramLink In currentDiagram.DiagramLinks set currentConnector = Repository.GetConnectorByID(diagramLink.ConnectorID) WriteNote currentModel, currentConnector, currentConnector.Notes, diagramName&amp;_links Next End Sub ' ' Recursively saves all diagrams under the provided package and its children ' Sub DumpDiagrams(thePackage,currentModel) Set currentPackage = thePackage ' export element notes For Each currentElement In currentPackage.Elements WriteNote currentModel, currentElement, currentElement.Notes,  ' export connector notes For Each currentConnector In currentElement.Connectors ' WScript.echo currentConnector.ConnectorGUID if (currentConnector.ClientID=currentElement.ElementID) Then WriteNote currentModel, currentConnector, currentConnector.Notes,  End If Next if (Not currentElement.CompositeDiagram Is Nothing) Then SyncJira currentModel, currentElement.CompositeDiagram SaveDiagram currentModel, currentElement.CompositeDiagram End If if (Not currentElement.Elements Is Nothing) Then DumpDiagrams currentElement,currentModel End If Next ' Iterate through all diagrams in the current package For Each currentDiagram In currentPackage.Diagrams SyncJira currentModel, currentDiagram SaveDiagram currentModel, currentDiagram Next ' Process child packages Dim childPackage 'as EA.Package ' otPackage = 5 if (currentPackage.ObjectType = 5) Then For Each childPackage In currentPackage.Packages call DumpDiagrams(childPackage, currentModel) Next End If End Sub Function SearchEAProjects(path) For Each folder In path.SubFolders SearchEAProjects folder Next For Each file In path.Files If fso.GetExtensionName (file.Path) = eap OR fso.GetExtensionName (file.Path) = eapx Then WScript.echo found &amp;file.path If (Left(file.name, 1) = _) Then WScript.echo skipping, because it start with `_` (replication) Else OpenProject(file.Path) End If End If Next End Function 'Gets the package object as referenced by its GUID from the Enterprise Architect project. 'Looks for the model node, the package is a child of as it is required for the diagram export. 'Calls the Sub routine DumpDiagrams for the model and package found. 'An error is printed to console only if the packageGUID is not found in the project. Function DumpPackageDiagrams(EAapp, packageGUID) WScript.echo DumpPackageDiagrams WScript.echo packageGUID Dim package Set package = EAapp.Repository.GetPackageByGuid(packageGUID) If (package Is Nothing) Then WScript.echo invalid package - as package is not part of the project Else Dim currentModel Set currentModel = package while currentModel.IsModel = false Set currentModel = EAapp.Repository.GetPackageByID(currentModel.parentID) wend ' Iterate through all child packages and save out their diagrams ' save all diagrams of package itself call DumpDiagrams(package, currentModel) End If End Function Function FormatStringToJSONString(inputString) outputString = Replace(inputString, \, \\) outputString = Replace(outputString, , \) outputString = Replace(outputString, vbCrLf, \n) outputString = Replace(outputString, vbLf, \n) outputString = Replace(outputString, vbCr, \n) FormatStringToJSONString = outputString End Function 'If a valid file path is set, the glossary terms are read from EA repository, 'formatted in a JSON compatible format and written into file. 'The file is read and reformatted by the exportEA gradle task afterwards. Function ExportGlossaryTermsAsJSONFile(EArepo) If (Len(glossaryFilePath) &gt; 0) Then set objFSO = CreateObject(Scripting.FileSystemObject) GUID = Replace(EArepo.ProjectGUID,{,) GUID = Replace(GUID,},) currentGlossaryFile = objFSO.BuildPath(glossaryFilePath,/&amp;GUID&amp;.ad) set objFile = objFSO.OpenTextFile(currentGlossaryFile,ForAppending, True) Set glossary = EArepo.Terms() objFile.WriteLine([) dim counter counter = 0 For Each term In glossary if (counter &gt; 0) Then objFile.Write(,) end if objFile.Write({ term : &amp;FormatStringToJSONString(term.term)&amp;, meaning : &amp;FormatStringToJSONString(term.Meaning)&amp;,) objFile.WriteLine( termID : &amp;FormatStringToJSONString(term.termID)&amp;, type : &amp;FormatStringToJSONString(term.type)&amp; }) counter = counter + 1 Next objFile.WriteLine(]) objFile.Close End If End Function Sub OpenProject(file) ' open Enterprise Architect Set EAapp = CreateObject(EA.App) WScript.echo opening Enterprise Architect. This might take a moment... ' load project EAapp.Repository.OpenFile(file) ' make Enterprise Architect to not appear on screen EAapp.Visible = False ' get repository object Set Repository = EAapp.Repository ' Show the script output window ' Repository.EnsureOutputVisible(Script) call ExportGlossaryTermsAsJSONFile(Repository) Set projectInterface = Repository.GetProjectInterface() Dim childPackage 'As EA.Package ' Iterate through all model nodes Dim currentModel 'As EA.Package If (InStrRev(file,{) &gt; 0) Then ' the filename references a GUID ' like {04C44F80-8DA1-4a6f-ECB8-982349872349} WScript.echo file GUID = Mid(file, InStrRev(file,{)+0,38) WScript.echo GUID ' Iterate through all child packages and save out their diagrams call DumpPackageDiagrams(EAapp, GUID) Else If packageFilter.Count = 0 Then WScript.echo done ' Iterate through all model nodes For Each currentModel In Repository.Models ' Iterate through all child packages and save out their diagrams For Each childPackage In currentModel.Packages call DumpDiagrams(childPackage,currentModel) Next Next Else ' Iterate through all packages found in the package filter given by script parameter. For Each packageGUID In packageFilter call DumpPackageDiagrams(EAapp, packageGUID) Next End If End If EAapp.Repository.CloseFile() ' Since EA 15.2 the Enterprise Architect background process hangs without calling Exit explicitly EAapp.Repository.Exit() End Sub Private connectionString Private packageFilter Private exportDestination Private searchPath Private glossaryFilePath Private diagramAttributes Private additionalOptions exportDestination = ./src/docs searchPath = ./src Set packageFilter = CreateObject(System.Collections.ArrayList) Set objArguments = WScript.Arguments Dim argCount argCount = 0 While objArguments.Count &gt; argCount+1 Select Case objArguments(argCount) Case -c connectionString = objArguments(argCount+1) Case -p packageFilter.Add objArguments(argCount+1) Case -d exportDestination = objArguments(argCount+1) Case -s searchPath = objArguments(argCount+1) Case -g glossaryFilePath = objArguments(argCount+1) Case -da diagramAttributes = objArguments(argCount+1) Case -ao additionalOptions = objArguments(argCount+1) End Select argCount = argCount + 2 WEnd set fso = CreateObject(Scripting.fileSystemObject) WScript.echo Image extractor ' Check both types in parallel - 1st check Enterprise Architect database connection, 2nd look for local project files If Not IsEmpty(connectionString) Then WScript.echo opening database connection now OpenProject(connectionString) End If WScript.echo looking for .eap(x) files in  &amp; fso.GetAbsolutePathName(searchPath) ' Dim f As Scripting.Files SearchEAProjects fso.GetFolder(searchPath) WScript.echo finished exporting images .gravatar img { margin-left: 3px; border-radius: 4px; } 1.14. exportVisio 1 minute to read About This Task This task searches for Visio files in the /src/docs folder then exports all diagrams and element notes to /src/docs/images/visio and /src/docs/visio . Images are stored as /images/visio/[filename]-[pagename].png . Notes are stored as /visio/[filename]-[pagename].adoc You can specify a filename to export notes to by starting any comment with {adoc:[filename].adoc} . It will then be written to /visio/[filename].adoc . Important Information About This Task Currently, only Visio files stored directly in /src/docs are supported. All others will export to the wrong location. Before running this task, close any open Visio instance. Further Reading and Resources Issue #112 . Source exportVisio.gradle task exportVisio( dependsOn: [streamingExecute], description: 'exports all diagrams and notes from visio files', group: 'docToolchain' ) { doLast { //make sure path for notes exists //and remove old notes new File(docDir, 'src/docs/visio').deleteDir() //also remove old diagrams new File(docDir, 'src/docs/images/visio').deleteDir() //create a readme to clarify things def readme = This folder contains exported diagrams and notes from visio files. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportVisio` to re-export files  new File(docDir, 'src/docs/images/visio/.').mkdirs() new File(docDir, 'src/docs/images/visio/readme.ad').write(readme) new File(docDir, 'src/docs/visio/.').mkdirs() new File(docDir, 'src/docs/visio/readme.ad').write(readme) def sourcePath = new File(docDir, 'src/docs/.').canonicalPath def scriptPath = new File(projectDir, 'scripts/VisioPageToPngConverter.ps1').canonicalPath powershell ${scriptPath} -SourcePath ${sourcePath}.executeCmd() } } scripts/VisioPageToPngConverter.ps1 # Convert all pages in all visio files in the given directory to png files. # A Visio windows might flash shortly. # The converted png files are stored in the same directory # The name of the png file is concatenated from the Visio file name and the page name. # In addtion all the comments are stored in adoc files. # If the Viso file is named MyVisio.vsdx and the page is called FirstPage # the name of the png file will be MyVisio-FirstPage.png and the comment will # be stored in MyVisio-FirstPage.adoc. # But for the name of the adoc files there is an alternative. It can be given in the first # line of the comment. If it is given in the comment it has to be given in curly brackes # with the prefix adoc:, e.g. {adoc:MyCommentFile.adoc} # Prerequisites: Viso and PowerShell has to be installed on the computer. # Parameter: SourcePath where visio files can be found # Example powershell VisoPageToPngConverter.ps1 -SourcePath c:\convertertest\ Param ( [Parameter(Mandatory=$true,ValueFromPipeline=$true,Position=0)] [Alias('p')][String]$SourcePath ) Write-Output starting to export visio If (!(Test-Path -Path $SourcePath)) { Write-Warning The path $SourcePath does not exist or is not accessible, please input the correct path. Exit } # Extend the source path to get only Visio files of the given directory and not in subdircetories If ($SourcePath.EndsWith(\)) { $SourcePath = $SourcePath } Else { $SourcePath = $SourcePath\ } $VisioFiles = Get-ChildItem -Path $SourcePath* -Recurse -Include *.vsdx,*.vssx,*.vstx,*.vxdm,*.vssm,*.vstm,*.vsd,*.vdw,*.vss,*.vst If(!($VisioFiles)) { Write-Warning There are no Visio files in the path $SourcePath. Exit } $VisioApp = New-Object -ComObject Visio.Application $VisioApp.Visible = $false # Extract the png from all the files in the folder Foreach($File in $VisioFiles) { $FilePath = $File.FullName Write-Output found $FilePath . $FileDirectory = $File.DirectoryName # Get the folder containing the Visio file. Will be used to store the png and adoc files $FileBaseName = $File.BaseName -replace '[ :/\\*?|&lt;&gt;]','-' # Get the filename to be used as part of the name of the png and adoc files Try { $Document = $VisioApp.Documents.Open($FilePath) $Pages = $VisioApp.ActiveDocument.Pages Foreach($Page in $Pages) { # Create valid filenames for the png and adoc files $PngFileName = $Page.Name -replace '[ :/\\*?|&lt;&gt;]','-' $PngFileName = $FileBaseName-$PngFileName.png $AdocFileName = $PngFileName.Replace(.png, .adoc) #TODO: this needs better logic Write-Output($SourcePath\images\visio\$PngFileName) $Page.Export($SourcePath\images\visio\$PngFileName) $AllPageComments =  ForEach($PageComment in $Page.Comments) { # Extract adoc filename from comment text if the syntax is valid # Remove the filename from the text and save the comment in a file with a valid name $EofStringIndex = $PageComment.Text.IndexOf(.adoc}) if ($PageComment.Text.StartsWith({adoc) -And ($EofStringIndex -gt 6)) { $AdocFileName = $PageComment.Text.Substring(6, $EofStringIndex -1) $AllPageComments += $PageComment.Text.Substring($EofStringIndex + 6) } else { $AllPageComments += $PageComment.Text+`n } } If ($AllPageComments) { $AdocFileName = $AdocFileName -replace '[:/\\*?|&lt;&gt;]','-' #TODO: this needs better logic $stream = [System.IO.StreamWriter] $SourcePath\visio\$AdocFileName $stream.WriteLine($AllPageComments) $stream.close() } } $Document.Close() } Catch { if ($Document) { $Document.Close() } Write-Warning One or more visio page(s) in file $FilePath have been lost in this converting. Write-Warning Error was: $_ } } $VisioApp.Quit() .gravatar img { margin-left: 3px; border-radius: 4px; } 1.15. exportDrawIo 2 minutes to read About This Task There is no exportDrawIo task available in docToolchain because such a task is not required. You can continue to use diagrams.net (formerly known as draw.io) to edit your diagrams simply by making a change to your diagram-authoring workflow. About diagrams.net diagrams.net offers free and open source desktop editors for all major operating system platforms. Visit https://www.diagrams.net/integrations to find a desktop editor application compatible with your operating system. When you use the desktop version, just create your diagram with the .png (or even better, .dio.png ) extension and diagrams.net will always save your diagram as a PNG with the source as metadata. They have also launched a free plugin for VS Code and IntelliJ so you can edit your diagrams offline! How to Change Your Workflow to Use diagrams.net Export your diagrams.net/draw.io diagrams as a PNG with the source embedded in the file metadata. This allows you to embed your diagrams into AsciiDoc source as you normally would (using the image:: macro) with the added advantage of storing the diagram source with the image itself. How to Convert a Confluence Page to AsciiDoc If you are converting a Confluence page with embedded draw.io diagrams to AsciiDoc, use this export workflow to continue using diagrams.net: Export an editable PNG diagram from Confluence. Load the diagram you want to export from Confluence. Click File &#160; &#8250; Export as &#160; &#8250; PNG&#8230;&#8203; . In the Image modal, make sure that Include a copy of my diagram is selected. Click Export to save the PNG file with the pattern [file].dio.png . Commit the exported PNG file to source control. Your diagram can now be managed in source control, added to your documentation source and edited using a diagrams.net desktop version. Note Specifying .dio (short for  d raw io ) in the name will help you identify PNG files containing an embedded XML diagram source. // Please, replace #yourelement with a real element id on your webpage MarketplaceWidget.setupMarketplaceWidget('card', 15635, #myelement); .gravatar img { margin-left: 3px; border-radius: 4px; } 1.16. exportChangeLog 2 minutes to read About This Task As the name suggests, this task exports the changelog to be referenced from within your documentation, if needed. The changelog is written to build/docs/changelog.adoc . This task can be configured to use a different source control system or a different directory. To configure this task, copy template_config/scripts/ChangelogConfig.groovy to your directory and modify to suit your needs. Then use -PchangelogConfigFile=&lt;your config file&gt; to add the path to your configuration file to the task. See the description inside the template for more details. By default, the source is the Git changelog for the path src/docs and only contains the commit messages for changes made to the documentation. All changes to the build or other sources in the repository will not show up. By default, the changelog contains changes made to date , author and commit message already formatted as AsciiDoc table content: | 09.04.2017 | Ralf D. Mueller | fix #24 template updated to V7.0 | 08.04.2017 | Ralf D. Mueller | fixed typo You simply include it like this: .Changes [options=header,cols=1,2,6] |==== | Date | Author | Comment include::../../build/docs/changelog.adoc[] |==== By excluding the table definition, you can easily translate the table headings through different text snippets. Note In a future docToolchain release, you will have the ability to include only certain commit messages from the changelog and exclude others (starting with # or // ?). This feature is not available just yet. Further Reading The only constant in life is change blog post. Source exportChangelog.gradle task exportChangeLog( description: 'exports the change log from a git subpath', group: 'docToolchain' ) { doFirst { new File(targetDir).mkdirs() } doLast { logger.info(docToolchain&gt; docDir: +docDir) logger.info(docToolchain&gt; mainConfigFile: +mainConfigFile) def config = new ConfigSlurper().parse(new File(docDir, mainConfigFile).text) def cmd = ${config.changelog.cmd} . def changes = cmd.execute(null, new File(docDir, config.changelog.dir)).text def changelog = new File(targetDir, 'changelog.adoc') logger.info &gt; changelog exported ${changelog.canonicalPath} changelog.write(changes) } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.17. exportContributors 3 minutes to read About This Task This task crawls through all Asciidoctor source files and extracts a list of contributors, which is then rendered as AsciiDoc images of each contributor&#8217;s gravatar picture. The extracted list is stored in /home/runner/work/docToolchain/docToolchain/build/contributors/015_tasks/03_task_exportContributors.adoc so it can be easily included in your documents. How to Use This Task The best way to use this task is to create a feedback.adoc file similar to this: feedback.adoc ifndef::backend-pdf[] // (1) image::https://img.shields.io/badge/improve-this%20doc-orange.svg[link={manualdir}{filename}, float=right] // (2) image::https://img.shields.io/badge/create-an%20issue-blue.svg[link=https://github.com/docToolchain/documentation/issues/new?title=&amp;body=%0A%0A%5BEnter%20feedback%20here%5D%0A%0A%0A---%0A%23page:{filename}, float=right] // (3) endif::[] include::{targetDir}/contributors/{filename}[] // (4) Key: Do not show this section when docs are rendered as PDF. Create an Improve This Doc button which links to your GitHub sources. Create a Create an Issue button which links to your issue tracker. Include the list of contributors created by this task. (The task automatically adds the estimated reading time to the list of contributors.) About the Avatar-Icons It seems not to be possible to extract a link to the github avatar icons from the log. So, the solution is to use Gravatar icons. For this to work, the contributors email address is hashed and an icon link is generated from that hash. http://www.gravatar.com/avatar/cc5f3bf8b3cb91c985ed4fd046aa451d?d=identicon This result at least in an icon which has a distinct color. Contributors can setup their own image through Gravatar.com . For this to work, the git commits need to use an email address which can be verified by Gravatar.com. Infortunately, this is not the case if a contributor decided to make his email address private in the email settions of her github account. File Attributes This task also exports some GitHub file attributes. The extracted attributes are stored in /home/runner/work/docToolchain/docToolchain/build/fileattribs/015_tasks/03_task_exportContributors.adoc . :lastUpdated: 16.05.2019 06:22 :lastAuthorName: Ralf D. Müller :lastAuthorEmail: ralf.d.mueller@gmail.com :lastAuthorAvatar: http://www.gravatar.com/avatar/cc5f3bf8b3cb91c985ed4fd046aa451d?d=identicon[32,32,role='gravatar',alt='Ralf D. Müller',title='Ralf D. Müller'] :lastMessage: #310 started to document config options You can import and use these attributes in the same way as you import the contributors list. Important please make sure that you do not accidentally publish the email address if your contributors do not want it. For example: feedback.adoc include::{targetDir}/fileattribs/{filename}[] Last updated {lastUpdated} by {lastAuthorName} .gravatar img { margin-left: 3px; border-radius: 4px; } 1.18. exportJiraIssues 2 minutes to read About This Task This task exports all issues for a given query or queries from Jira as either an AsciiDoc table or an Excel file. The configuration for this task can be found within Config.gradle ( gradle.properties can be used as a fallback). Username/password is deprecated, so you need to use username/API-token instead. An API-token can be created through https://id.atlassian.com/manage/api-tokens . We recommend that you keep username and API-token out of your GitHub repository, and instead pass them as environment variables to docToolchain. Configuration Jira configuration support list requests to Jira where results of each requests will be saved in a file with specifies filename. Flags saveAsciidoc &amp; saveExcel allow you to easily configure the format in which results should be saved. Important The old configuration was based on the single Jira query is deprecated (single 'jql' parameter). Support for it will be removed in the near future. Please migrate to the new configuration which allows multiple Jira queries. Config.groovy // Configuration for Jira related tasks jira = [:] jira.with { // endpoint of the JiraAPI (REST) to be used api = 'https://your-jira-instance' /* WARNING: It is strongly recommended to store credentials securely instead of commiting plain text values to your git repository!!! Tool expects credentials that belong to an account which has the right permissions to read the JIRA issues for a given project. Credentials can be used in a form of: - passed parameters when calling script (-PjiraUser=myUsername -PjiraPass=myPassword) which can be fetched as a secrets on CI/CD or - gradle variables set through gradle properties (uses the 'jiraUser' and 'jiraPass' keys) Often, Jira &amp; Confluence credentials are the same, in which case it is recommended to pass CLI parameters for both entities as -Pusername=myUser -Ppassword=myPassword */ // the key of the Jira project project = 'PROJECTKEY' // the format of the received date time values to parse dateTimeFormatParse = yyyy-MM-dd'T'H:m:s.SSSz // i.e. 2020-07-24'T'9:12:40.999 CEST // the format in which the date time should be saved to output dateTimeFormatOutput = dd.MM.yyyy HH:mm:ss z // i.e. 24.07.2020 09:02:40 CEST // the label to restrict search to label = 'label1' // Legacy settings for Jira query. This setting is deprecated &amp; support for it will soon be completely removed. Please use JiraRequests settings jql = project='%jiraProject%' AND labels='%jiraLabel%' ORDER BY priority DESC, duedate ASC // Base filename in which Jira query results should be stored resultsFilename = 'JiraTicketsContent' saveAsciidoc = true // if true, asciidoc file will be created with *.adoc extension saveExcel = true // if true, Excel file will be created with *.xlsx extension // Output folder for this task inside main outputPath resultsFolder = 'JiraRequests' /* List of requests to Jira API: These are basically JQL expressions bundled with a filename in which results will be saved. User can configure custom fields IDs and name those for column header, i.e. customfield_10026:'Story Points' for Jira instance that has custom field with that name and will be saved in a coloumn named Story Points */ requests = [ new JiraRequest( filename:File1_Done_issues, jql:project='%jiraProject%' AND status='Done' ORDER BY duedate ASC, customfields: [customfield_10026:'Story Points'] ), new JiraRequest( filename:'CurrentSprint', jql:project='%jiraProject%' AND Sprint in openSprints() ORDER BY priority DESC, duedate ASC, customfields: [customfield_10026:'Story Points'] ), ] } @groovy.transform.Immutable class JiraRequest { String filename //filename (without extension) of the file in which JQL results will be saved. Extension will be determined automatically for Asciidoc or Excel file String jql // Jira Query Language syntax Map&lt;String,String&gt; customfields // map of customFieldId:displayName values for Jira fields which don't have default names, i.e. customfield_10026:StoryPoints } Further Reading Read the Living Documents for Agile Projects blog post. Source exportJiraIssues.gradle task exportJiraIssues( description: 'exports all jira issues from a given search', group: 'docToolchain' ) { doLast { final String taskSubfolderName = config.jira.resultsFolder final File targetFolder = new File(targetDir + File.separator + taskSubfolderName) if (!targetFolder.exists()) targetFolder.mkdirs() logger.debug(Output folder for 'exportJiraIssues' task is: '${targetFolder}') // map configuration from Config.groovy to existing variables for compatibility with naming of Jira settings in gradle.properties def jiraRoot = config.jira.api def jiraProject = config.jira.project def jiraLabel = config.jira.label def jiraResultsFilename = config.jira.resultsFilename def jiraDateTimeFormatParse = config.jira.dateTimeFormatParse def jiraDateTimeOutput = config.jira.dateTimeFormatOutput def defaultFields = 'priority,created,resolutiondate,summary,assignee,status' def jira = new groovyx.net.http.RESTClient(jiraRoot + '/rest/api/2/') jira.encoderRegistry = new groovyx.net.http.EncoderRegistry(charset: 'utf-8') def headers = [ 'Authorization': Basic  + config.jira.credentials, 'Content-Type' : 'application/json; charset=utf-8' ] def jiraRequests = config.jira.requests if (config.jira.jql) { logger.warn(&gt;&gt;&gt;Found legacy Jira requests. Please migrate to the new Jira configuration ASAP. Old config with jql will be removed soon) writeAsciiDocFileForLegacyConfiguration(targetFolder, jira, headers, config.jira) } jiraRequests.each {rq -&gt; logger.quiet(Request to Jira API for '${rq.filename}' with query: '${rq.jql}') def allHeaders = ${defaultFields},${rq.customfields.values().join(,)} def allFieldIds = ${defaultFields},${rq.customfields.keySet().join(,)} logger.quiet(Preparing headers for default &amp; custom fields: ${allHeaders}) logger.quiet(Preparing field IDs for default &amp; custom fields: ${allFieldIds}) // Save AsciiDoc file if (config.jira.saveAsciidoc) { def extension = 'adoc' jiraResultsFilename = ${rq.filename}.${extension} logger.info(Results will be saved in '${rq.filename}.${extension}' file) def jiraDataAsciidoc = new File(targetFolder, ${rq.filename}.${extension}) jiraDataAsciidoc.write(.${rq.filename}\n, 'utf-8') jiraDataAsciidoc.append(|=== \n) // AsciiDoc table headers (custom fields map needs values here) jiraDataAsciidoc.append(|Key , 'utf-8') allHeaders.split(,).each {field -&gt; jiraDataAsciidoc.append(|${field.capitalize()} , 'utf-8') } jiraDataAsciidoc.append(\n, 'utf-8') jira.get(path: 'search', query: ['jql' : rq.jql.replaceAll('%jiraProject%', jiraProject).replaceAll('%jiraLabel%', jiraLabel), 'maxResults': 1000, fields: ${allFieldIds} ], headers: headers ).data.issues.each { issue -&gt; //logger.quiet(&gt;&gt; Whole issue ${issue.key}:\n ${issue.fields}) jiraDataAsciidoc.append(| ${jiraRoot}/browse/${issue.key}[${issue.key}] , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.priority.name} , 'utf-8') jiraDataAsciidoc.append(| ${Date.parse(jiraDateTimeFormatParse, issue.fields.created).format(jiraDateTimeOutput)} , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.resolutiondate ? Date.parse(jiraDateTimeFormatParse, issue.fields.resolutiondate).format(jiraDateTimeOutput) : ''} , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.summary} , 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.assignee ? issue.fields.assignee.displayName : 'not assigned'}, 'utf-8') jiraDataAsciidoc.append(| ${issue.fields.status.name} , 'utf-8') rq.customfields.each { field -&gt; def foundCustom = issue.fields.find {it.key == field.key} //logger.quiet(Examining issue '${issue.key}' for custom field '${field.key}' has found: '${foundCustom}') jiraDataAsciidoc.append(| ${foundCustom ? foundCustom.value : '-'}\n, 'utf-8') } } jiraDataAsciidoc.append(|=== \n) } else { logger.quiet(Set saveAsciidoc=true in '${mainConfigFile}' to save results in AsciiDoc file) } // Save Excel file if (config.jira.saveExcel) { def extension = 'xlsx' jiraResultsFilename = ${rq.filename}.${extension} logger.quiet(&gt;&gt; Results will be saved in '${rq.filename}.${extension}' file) //def jiraDataAsciidoc = new File(targetFolder, ${rq.filename}.${extension}) def jiraDataXls = new File(targetFolder, jiraResultsFilename) def jiraFos = new FileOutputStream(jiraDataXls) Workbook wb = new XSSFWorkbook(); CreationHelper hyperlinkHelper = wb.getCreationHelper(); def sheetName = ${rq.filename} def ws = wb.createSheet(sheetName) String rgbS = A7A7A7 byte[] rgbB = Hex.decodeHex(rgbS) XSSFColor color = new XSSFColor(rgbB, null) //IndexedColorMap has no usage until now. So it can be set null. XSSFCellStyle headerCellStyle = (XSSFCellStyle) wb.createCellStyle() headerCellStyle.setFillForegroundColor(color) headerCellStyle.setFillPattern(FillPatternType.SOLID_FOREGROUND) def titleRow = ws.createRow(0); int cellNumber = 0; titleRow.createCell(cellNumber).setCellValue(Key) allHeaders.split(,).each {field -&gt; titleRow.createCell(++cellNumber).setCellValue(${field.capitalize()}) } def lastRow = titleRow.getRowNum() titleRow.setRowStyle(headerCellStyle) jira.get(path: 'search', query: ['jql' : rq.jql.replaceAll('%jiraProject%', jiraProject).replaceAll('%jiraLabel%', jiraLabel), 'maxResults': 1000, fields: ${allFieldIds} ], headers: headers ).data.issues.each { issue -&gt; int cellPosition = 0 def row = ws.createRow(++lastRow) Hyperlink link = hyperlinkHelper.createHyperlink(HyperlinkType.URL) link.setAddress(${jiraRoot}/browse/${issue.key}) Cell cellWithUrl = row.createCell(cellPosition) cellWithUrl.setCellValue(${issue.key}) cellWithUrl.setHyperlink(link) row.createCell(++cellPosition).setCellValue(${issue.fields.priority.name}) row.createCell(++cellPosition).setCellValue(${Date.parse(jiraDateTimeFormatParse, issue.fields.created).format(jiraDateTimeOutput)}) row.createCell(++cellPosition).setCellValue(${issue.fields.resolutiondate ? Date.parse(jiraDateTimeFormatParse, issue.fields.resolutiondate).format(jiraDateTimeOutput) : ''}) row.createCell(++cellPosition).setCellValue(${issue.fields.summary}) row.createCell(++cellPosition).setCellValue(${issue.fields.assignee ? issue.fields.assignee.displayName : ''}) row.createCell(++cellPosition).setCellValue(${issue.fields.status.name}) // Custom fields rq.customfields.each { field -&gt; def position = ++cellPosition def foundCustom = issue.fields.find {it.key == field.key} row.createCell(position).setCellValue(${foundCustom ? foundCustom.value : '-'}) } } // set jira issue key column fits the content width for(int colNum = 0; colNum&lt;allHeaders.size()+1;colNum++) { ws.autoSizeColumn(colNum) } // Set summary column width slightly wider but fixed size, so it doesn't change with every summary update ws.setColumnWidth(4, 25*384) wb.write(jiraFos) } else { logger.quiet(Set saveExcel=true in '${mainConfigFile}' to save results in Excel file) } } } } // This method can be removed when support for legacy Jira configuration is gone def writeAsciiDocFileForLegacyConfiguration(def targetFolder, def restClient, def headers, def jiraConfig) { def resultsFilename = ${jiraConfig.resultsFilename}_legacy.adoc def openIssues = new File(targetFolder, ${resultsFilename}) openIssues.write(.Table {Title}\n, 'utf-8') openIssues.append(|=== \n) openIssues.append(|Key |Priority |Created | Assignee | Summary\n, 'utf-8') def legacyJql = jiraConfig.jql.replaceAll('%jiraProject%', config.jira.project).replaceAll('%jiraLabel%', config.jira.label) println (Results for legacy query '${legacyJql}' will be saved in '${resultsFilename}' file) restClient.get(path: 'search', query: ['jql' : legacyJql, 'maxResults': 1000, 'fields' : 'created,resolutiondate,priority,summary,timeoriginalestimate, assignee' ], headers: headers ).data.issues.each { issue -&gt; openIssues.append(| ${jiraRoot}/browse/${issue.key}[${issue.key}] , 'utf-8') openIssues.append(| ${issue.fields.priority.name} , 'utf-8') openIssues.append(| ${Date.parse(jiraConfig.dateTimeFormatParse, issue.fields.created).format(jiraConfig.dateTimeFormatOutput)} , 'utf-8') openIssues.append(| ${issue.fields.assignee ? issue.fields.assignee.displayName : 'not assigned'}, 'utf-8') openIssues.append(| ${issue.fields.summary} , 'utf-8') } openIssues.append(|=== \n) } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.19. exportJiraSprintChangelogIssues 1 minute to read About This Task This task exports a simplified (key and summary) list of Jira issues for a specific sprint defined in the task configuration. Only a few additional fields (such as assignee) can be switched using configuration flags. Once you define the sprint, the relevant AsciiDoc and Excel files will be generated. If a sprint is not defined in the configuration, changelogs for all sprints that match the configuration will be saved in separate AsciiDoc files and in different tabs within an Excel file. The task configuration can be found within Config.gradle . In addition to the configuration snippet below, it is important to configure the Jira API and credentials in the Jira section of the configuration inside the same file. Configuration Config.groovy // Sprint changelog configuration generate changelog lists based on tickets in sprints of an Jira instance. // This feature requires at least Jira API &amp; credentials to be properly set in Jira section of this configuration sprintChangelog = [:] sprintChangelog.with { sprintState = 'closed' // it is possible to define multiple states, i.e. 'closed, active, future' ticketStatus = Done, Closed // it is possible to define multiple ticket statuses, i.e. Done, Closed, 'in Progress' showAssignee = false showTicketStatus = false showTicketType = true sprintBoardId = 12345 // Jira instance probably have multiple boards; here it can be defined which board should be used // Output folder for this task inside main outputPath resultsFolder = 'Sprints' // if sprintName is not defined or sprint with that name isn't found, release notes will be created on for all sprints that match sprint state configuration sprintName = 'PRJ Sprint 1' // if sprint with a given sprintName is found, release notes will be created just for that sprint allSprintsFilename = 'Sprints_Changelogs' // Extension will be automatically added. } Source exportJiraSprintChangelog.gradle task exportJiraSprintChangelog( description: 'exports all jira issues from Sprint for release notes', group: 'docToolchain' ) { doLast { // Pre defined ticket fields for Changelog based on Jira Sprints def defaultTicketFields = 'summary,status,assignee,issuetype' // retrieving sprints for a given board def sprints = { apiSprints, headers, boardId, sprintState -&gt; apiSprints.get(path: agile/latest/board/${boardId}/sprint, query:[state: ${sprintState}], headers: headers ).data } // retrieving issues for given sprint def issues = { apiIssues, headers, boardId, sprintId, status -&gt; apiIssues.get(path: agile/latest/board/${boardId}/sprint/${sprintId}/issue, query: ['jql' : status in (${status}) ORDER BY type DESC, status ASC, 'maxResults': 1000, fields: defaultTicketFields ], headers: headers ).data } // preparing target folder for generated files final String taskSubfolderName = config.sprintChangelog.resultsFolder final File targetFolder = new File(targetDir + File.separator + taskSubfolderName) if (!targetFolder.exists()) targetFolder.mkdirs() logger.debug(Output folder for 'exportJiraSprintChangelog' task is: '${targetFolder}') // Getting configuration def jiraRoot = config.jira.api def jiraProject = config.jira.project def sprintState = config.sprintChangelog.sprintState def ticketStatusForReleaseNotes = config.sprintChangelog.ticketStatus def sprintBoardId = config.sprintChangelog.sprintBoardId def showAssignee = config.sprintChangelog.showAssignee def showTicketStatus = config.sprintChangelog.showTicketStatus def showTicketType = config.sprintChangelog.showTicketType def sprintName = config.sprintChangelog.sprintName def allSprintsFilename = config.sprintChangelog.allSprintsFilename logger.info(\n==========================\nJira Release notes config\n==========================) logger.info(Spring Board ID: ${sprintBoardId}) logger.info(Show assignees: ${showAssignee}. Show ticket status: ${showTicketStatus}. Show ticket type: ${showTicketType}) logger.info(Filtering for sprints with configured state: '${sprintState}') logger.info(Filtering for issues with configured statuses: ${ticketStatusForReleaseNotes}) logger.info(Attempt to generate release notes for sprint with a name: '${sprintName}') logger.info(Filename used for all sprints: '${allSprintsFilename}') def api = new groovyx.net.http.RESTClient(jiraRoot + '/rest/') api.encoderRegistry = new groovyx.net.http.EncoderRegistry(charset: 'utf-8') def headers = [ 'Authorization': Basic  + config.jira.credentials, 'Content-Type' : 'application/json; charset=utf-8' ] def allChangelogsFilename = ${allSprintsFilename}.xlsx logger.quiet(Changelogs of all sprints will be saved in '${allChangelogsFilename}' file) def changelogsXls = new File(targetFolder, allChangelogsFilename) def changelogsXlsFos = new FileOutputStream(changelogsXls) Workbook wb = new XSSFWorkbook(); CreationHelper hyperlinkHelper = wb.getCreationHelper(); String rgbS = A7A7A7 byte[] rgbB = Hex.decodeHex(rgbS) // get byte array from hex string XSSFColor color = new XSSFColor(rgbB, null) //IndexedColorMap has no usage until now. So it can be set null. XSSFCellStyle headerCellStyle = (XSSFCellStyle) wb.createCellStyle() headerCellStyle.setFillForegroundColor(color) headerCellStyle.setFillPattern(FillPatternType.SOLID_FOREGROUND) // prepare tickets according to configuration def columns = ['key'].plus(defaultTicketFields.split(',').collect()) if (!showAssignee) { columns = columns.minus('assignee')} if (!showTicketStatus) { columns = columns.minus('status')} if (!showTicketType) { columns = columns.minus('issuetype')} logger.info(Release notes will contain following info: ${columns}) logger.info(\n=====================\n Sprints\n=====================) // def allMatchedSprints = sprints(api, headers, sprintBoardId, sprintState).values def foundExactSprint = allMatchedSprints.any {it.name == sprintName} logger.info(All sprints that matched configuration: ${allMatchedSprints.size()}) def sprintsForChangelog = foundExactSprint ? allMatchedSprints.stream().filter() {it.name == sprintName} : allMatchedSprints logger.info(Found exact Sprint with name '${sprintName}': ${foundExactSprint}.) sprintsForChangelog.each { sprint -&gt; logger.quiet(\nSprint: $sprint.name [id: $sprint.id] state &lt;$sprint.state&gt;) /* ================================================ Create new worksheet inside existing excel file ================================================ */ String safeSprintName = WorkbookUtil.createSafeSheetName(${sprint.name}) def ws = wb.createSheet(safeSprintName) // Add titles (typically key &amp; summary, but assignee, ticket status, ticket type can be configured in Config.groovy too) def titleRow = ws.createRow(0); int cellNumber = 0; columns.each {columnTitle -&gt; titleRow.createCell(cellNumber++).setCellValue(${columnTitle.capitalize()})} def lastRow = titleRow.getRowNum() titleRow.setRowStyle(headerCellStyle) // set summary (at position 1) column wider than other columns ws.setColumnWidth(1, 35*256) /* ========================================= AsciiDoc file for each sprint ========================================= */ def asciidocFilename = ${sprint.name.replaceAll( , _)}.adoc logger.info(Results will be saved in '${asciidocFilename}' file) def changeLogAdoc = new File(targetFolder, ${asciidocFilename}) changeLogAdoc.write(.Table ${sprint.name} Changelog\n, 'utf-8') changeLogAdoc.append(|=== \n) // AsciiDoc table columns columns.each {columnTitle -&gt; changeLogAdoc.append(|${columnTitle} , 'utf-8')} /* ========================================= Add tickets for the sprint ========================================= */ issues(api, headers, sprintBoardId, sprint.id, ticketStatusForReleaseNotes).issues.each {issue -&gt; def assignee = ${issue.fields.assignee ? issue.fields.assignee.displayName : 'unassigned'}  def message = showAssignee ? by ${assignee} :  logger.quiet(Issue: [$issue.key] '$issue.fields.summary' ${message}&lt;$issue.fields.status.name&gt;) /* =========================== Write ticket to Excel =========================== */ int cellPosition = 0 def row = ws.createRow(++lastRow) Hyperlink link = hyperlinkHelper.createHyperlink(HyperlinkType.URL) link.setAddress(${jiraRoot}/browse/${issue.key}) Cell cellWithUrl = row.createCell(cellPosition) cellWithUrl.setCellValue(${issue.key}) cellWithUrl.setHyperlink(link) row.createCell(++cellPosition).setCellValue(${issue.fields.summary}) /* ============================= Write ticket to Asciidoc ============================= */ changeLogAdoc.append(\n, 'utf-8') changeLogAdoc.append(| ${jiraRoot}/browse/${issue.key}[${issue.key}] , 'utf-8') changeLogAdoc.append(| ${issue.fields.summary} , 'utf-8') /* === Write ticket status, assignee, ticket typee if configured to both Asciidoc &amp; Excel files === */ if (showTicketStatus) { row.createCell(++cellPosition).setCellValue(${issue.fields.status.name}) changeLogAdoc.append(| ${issue.fields.status.name} , 'utf-8') } if (showAssignee) { row.createCell(++cellPosition).setCellValue(${assignee}) changeLogAdoc.append(| ${assignee}, 'utf-8') } if (showTicketType) { row.createCell(++cellPosition).setCellValue(${issue.fields.issuetype.name}) changeLogAdoc.append(| ${issue.fields.issuetype.name} , 'utf-8') } } // Close the asciidoc table changeLogAdoc.append(\n|=== \n,'utf-8') // Set auto-width to KEY column ws.autoSizeColumn(0); } // Write to Excel file wb.write(changelogsXlsFos) } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1.20. exportPPT About This Task This task lets you export AsciiDoc content to a series of PowerPoint slides. It is currently a Windows-only task. 1 minute to read Further Reading and Resources Read the Do More with Slides blog post. Find more information about the Windows-only aspect of this task in this issue . Check out asciidoctorj-office-extension for another way to use PPT slides in your docs. Source exportPPT.gradle task exportPPT( dependsOn: [streamingExecute], description: 'exports all slides and some texts from PPT files', group: 'docToolchain' ) { doLast { //make sure path for notes exists //and remove old notes new File(projectDir, 'src/docs/ppt').deleteDir() //also remove old diagrams new File(projectDir, 'src/docs/images/ppt').deleteDir() //create a readme to clarify things def readme = This folder contains exported slides or notes from .ppt presentations. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportPPT` to re-export files  new File(projectDir, 'src/docs/images/ppt/.').mkdirs() new File(projectDir, 'src/docs/images/ppt/readme.ad').write(readme) new File(projectDir, 'src/docs/ppt/.').mkdirs() new File(projectDir, 'src/docs/ppt/readme.ad').write(readme) //execute through cscript in order to make sure that we get WScript.echo right %SystemRoot%\\System32\\cscript.exe //nologo ${projectDir}/scripts/exportPPT.vbs.executeCmd() } } exportPPT.vbs Const ForAppending = 8 Const ppPlaceholderBody = 2 ' Helper ' http://windowsitpro.com/windows/jsi-tip-10441-how-can-vbscript-create-multiple-folders-path-mkdir-command Function MakeDir (strPath) Dim strParentPath, objFSO Set objFSO = CreateObject(Scripting.FileSystemObject) On Error Resume Next strParentPath = objFSO.GetParentFolderName(strPath) If Not objFSO.FolderExists(strParentPath) Then MakeDir strParentPath If Not objFSO.FolderExists(strPath) Then objFSO.CreateFolder strPath On Error Goto 0 MakeDir = objFSO.FolderExists(strPath) End Function Function SearchPresentations(path) For Each folder In path.SubFolders SearchPresentations folder Next For Each file In path.Files If (Left(fso.GetExtensionName (file.Path), 3) = ppt) OR (Left(fso.GetExtensionName (file.Path), 3) = pps) Then WScript.echo found &amp;file.path ExportSlides(file.Path) End If Next End Function Sub ExportSlides(sFile) Set objRegEx = CreateObject(VBScript.RegExp) objRegEx.Global = True objRegEx.IgnoreCase = True objRegEx.MultiLine = True ' . doesn't work for multiline in vbs, [\s,\S] does... objRegEx.Pattern = [\s,\S]*{adoc} ' http://www.pptfaq.com/FAQ00481_Export_the_notes_text_of_a_presentation.htm strFileName = fso.GetFIle(sFile).Name Set oPPT = CreateObject(PowerPoint.Application) Set oPres = oPPT.Presentations.Open(sFile, True, False, False) ' Read Only, No Title, No Window Set oSlides = oPres.Slides strNotesText =  strImagePath = /src/docs/images/ppt/ &amp; strFileName &amp; / MakeDir(. &amp; strImagePath) strNotesPath = /src/docs/ppt/ MakeDir(. &amp; strNotesPath) For Each oSl In oSlides strSlideName = oSl.Name ' WScript.echo fso.GetAbsolutePathName(.) &amp; strImagePath &amp; strSlideName &amp; .jpg oSl.Export fso.GetAbsolutePathName(.) &amp; strImagePath &amp; strSlideName &amp; .jpg, .jpg For Each oSh In oSl.NotesPage.Shapes If oSh.PlaceholderFormat.Type = ppPlaceholderBody Then If oSh.HasTextFrame Then If oSh.TextFrame.HasText Then strCurrentNotes = oSh.TextFrame.TextRange.Text strCurrentNotes = Replace(strCurrentNotes,vbVerticalTab, vbCrLf) strCurrentNotes = Replace(strCurrentNotes,{slide},image::ppt/&amp;strFileName&amp;/&amp;strSlideName&amp;.jpg[]) ' remove speaker notes before marker {adoc} strCurrentNotes = objRegEx.Replace(strCurrentNotes,) strNotesText = strNotesText &amp; vbCrLf &amp; strCurrentNotes &amp; vbCrLf &amp; vbCrLf End If End If End If Next Next ' WScript.echo fso.GetAbsolutePathName(.) &amp; strNotesPath&amp;&amp;strFileName&amp;.ad ' http://stackoverflow.com/questions/2524703/save-text-file-utf-8-encoded-with-vba Set fsT = CreateObject(ADODB.Stream) fsT.Type = 2 'Specify stream type - we want To save text/string data. fsT.Charset = utf-8 'Specify charset For the source text data. fsT.Open 'Open the stream And write binary data To the object fsT.WriteText ifndef::imagesdir[:imagesdir: ../../images]&amp;vbCrLf&amp;CStr(strNotesText) fsT.SaveToFile fso.GetAbsolutePathName(.) &amp; strNotesPath&amp;&amp;strFileName&amp;.ad, 2 'Save binary data To disk oPres.Close() oPPT.Quit() End Sub set fso = CreateObject(Scripting.fileSystemObject) WScript.echo Slide extractor WScript.echo looking for .ppt files in  &amp; fso.GetAbsolutePathName(.) &amp; /src SearchPresentations fso.GetFolder(./src) WScript.echo finished exporting slides .gravatar img { margin-left: 3px; border-radius: 4px; } 1.21. exportExcel 2 minutes to read About This Task Sometimes you need to include tabular data in your documentation. Most likely, this data will be stored as a MS Excel spreadsheet, or you may like to use Excel to create and edit it. Either way, this task lets you export an Excel spreadsheet and include it directly in your docs. It searches for .xlsx files and exports each contained worksheet as .csv and as .adoc . Note that formulas contained in your spreadsheet are evaluated and exported statically. The generated files are written to src/excel/[filename]/[worksheet].(adoc|cvs) . The src folder is used instead of the build folder because a better history of worksheet changes is captured. The files can be included either as AsciiDoc: include::excel/Sample.xlsx/Numerisch.adoc[] &#8230;&#8203;or as a CSV file: [options=header,format=csv] |=== include::excel/Sample.xlsx/Numerisch.csv[] |=== The AsciiDoc version gives you a bit more control because the following are preserved: Horizontal and vertical alignment. col-span and row-span. Line breaks. Column width relative to other columns. Background colors. Further Reading See asciidoctorj-office-extension to learn another way to use Excel spreadsheets in your docs. Source build.gradle task exportExcel( description: 'exports all excelsheets to csv and AsciiDoc', group: 'docToolchain' ) { doFirst { File sourceDir = file(srcDir) def tree = fileTree(srcDir).include('**/*.xlsx').exclude('**/~*') def exportFileDir = new File(sourceDir, 'excel') //make sure path for notes exists exportFileDir.deleteDir() //create a readme to clarify things def readme = This folder contains exported workbooks from Excel. Please note that these are generated files but reside in the `src`-folder in order to be versioned. This is to make sure that they can be used from environments other than windows. # Warning! **The contents of this folder will be overwritten with each re-export!** use `gradle exportExcel` to re-export files  exportFileDir.mkdirs() new File(exportFileDir, '/readme.ad').write(readme) } doLast { File sourceDir = file(srcDir) def exportFileDir = new File(sourceDir, 'excel') def tree = fileTree(srcDir).include('**/*.xlsx').exclude('**/~*') def nl = System.getProperty(line.separator) def export = { sheet, evaluator, targetFileName -&gt; def targetFileCSV = new File(targetFileName + '.csv') def targetFileAD = new File(targetFileName + '.adoc') def df = new org.apache.poi.ss.usermodel.DataFormatter(); def regions = [] sheet.numMergedRegions.times { regions &lt;&lt; sheet.getMergedRegion(it) } logger.debug sheet contains ${regions.size()} regions def color = '' def resetColor = false def numCols = 0 def headerCreated = false def emptyRows = 0 for (int rowNum=0; rowNum&lt;=sheet.lastRowNum; rowNum++) { def row = sheet.getRow(rowNum) if (row &amp;&amp; !headerCreated) { headerCreated = true // create AsciiDoc table header def width = [] numCols = row.lastCellNum numCols.times { columnIndex -&gt; width &lt;&lt; sheet.getColumnWidth((int) columnIndex) } //lets make those numbers nicer: width = width.collect { Math.round(100 * it / width.sum()) } targetFileAD.append('[options=header,cols=' + width.join(',') + ']' + nl) targetFileAD.append('|===' + nl) } def data = [] def style = [] def colors = [] // For each row, iterate through each columns if (row &amp;&amp; (row?.lastCellNum!=-1)) { numCols.times { columnIndex -&gt; def cell = row.getCell(columnIndex) if (cell) { def cellValue = df.formatCellValue(cell, evaluator) if (cellValue.startsWith('*') &amp;&amp; cellValue.endsWith('\u20AC')) { // Remove special characters at currency cellValue = cellValue.substring(1).trim(); } def cellStyle = '' def region = regions.find { it.isInRange(cell.rowIndex, cell.columnIndex) } def skipCell = false if (region) { //check if we are in the upper left corner of the region if (region.firstRow == cell.rowIndex &amp;&amp; region.firstColumn == cell.columnIndex) { def colspan = 1 + region.lastRow - region.firstRow def rowspan = 1 + region.lastColumn - region.firstColumn if (rowspan &gt; 1) { cellStyle += ${rowspan} } if (colspan &gt; 1) { cellStyle += .${colspan} } cellStyle += + } else { skipCell = true } } if (!skipCell) { switch (cell.cellStyle.alignmentEnum.toString()) { case 'RIGHT': cellStyle += '&gt;' break case 'CENTER': cellStyle += '^' break } switch (cell.cellStyle.verticalAlignmentEnum.toString()) { case 'BOTTOM': cellStyle += '.&gt;' break case 'CENTER': cellStyle += '.^' break } color = cell.cellStyle.fillForegroundXSSFColor?.RGB?.encodeHex() color = color != null ? nl + {set:cellbgcolor:#${color}} : '' data &lt;&lt; cellValue if (color == '' &amp;&amp; resetColor) { colors &lt;&lt; nl + {set:cellbgcolor!} resetColor = false } else { colors &lt;&lt; color } if (color != '') { resetColor = true } style &lt;&lt; cellStyle } else { data &lt;&lt;  colors &lt;&lt;  style &lt;&lt; skip } } else { data &lt;&lt;  colors &lt;&lt;  style &lt;&lt;  } } emptyRows = 0 } else { if (emptyRows&lt;3) { //insert empty row numCols.times { data &lt;&lt;  colors &lt;&lt;  style &lt;&lt;  } emptyRows++ } else { break } } targetFileCSV.append(data .collect { \${it.replaceAll('', '')}\ } .join(',') + nl, 'UTF-8') targetFileAD.append(data .withIndex() .collect { value, index -&gt; if (style[index] == skip) {  } else { style[index] + | ${value.replaceAll('[|]', '{vbar}').replaceAll(\n, ' +$0') + colors[index]} } } .join(nl) + nl * 2, 'UTF-8') } targetFileAD.append('|===' + nl) } tree.each { File excel -&gt; println file:  + excel def excelDir = new File(exportFileDir, excel.getName()) excelDir.mkdirs() InputStream inp inp = new FileInputStream(excel) def wb = org.apache.poi.ss.usermodel.WorkbookFactory.create(inp); def evaluator = wb.getCreationHelper().createFormulaEvaluator(); for (int wbi = 0; wbi &lt; wb.getNumberOfSheets(); wbi++) { def sheetName = wb.getSheetAt(wbi).getSheetName() println  -- sheet:  + sheetName def targetFile = new File(excelDir, sheetName) export(wb.getSheetAt(wbi), evaluator, targetFile.getAbsolutePath()) } inp.close(); } } } .gravatar img { margin-left: 3px; border-radius: 4px; } 1 minute to read 2. exportMarkdown The exportMarkdown task can be used to include markdown files into the documentation. The task is scanning the directory /src/docs for markdown files ( *.md ) and converts them into AsciiDoc files. The converted files can then be referenced from within the /build -folder. 2.1. Source exportMarkdown.gradle task exportMarkdown( description: 'exports all markdown files to AsciiDoc', group: 'docToolchain', type: Copy ) { from srcDir include(**/*.md) //include only markdown files includeEmptyDirs = false rename(/(.+).md/, '$1.adoc') //rename all files from *.md to *.adoc filter(Markdown2AdocFilter) // convert the content of the files into targetDir } class Markdown2AdocFilter extends FilterReader { Markdown2AdocFilter(Reader input) { super(new StringReader(nl.jworks.markdown_to_asciidoc.Converter.convertMarkdownToAsciiDoc(input.text))) } } .gravatar img { margin-left: 3px; border-radius: 4px; } 2.2. exportOpenAPI 1 minute to read About This Task This task exports an OpenAPI Specification definition yaml file to a AsciiDoc document. Currently this task depends on OpenAPI Generator (v4.3.1) and its gradle plugin . Configuration Config.groovy // Configuration for OpenAPI related task openApi = [:] // 'specFile' is the name of OpenAPI specification yaml file. Tool expects this file inside working dir (as a filename or relative path with filename) // 'infoUrl' and 'infoEmail' are specification metadata about further info related to the API. By default this values would be filled by openapi-generator plugin placeholders // openApi.with { specFile = 'src/docs/petstore-v2.0.yaml' // i.e. 'petstore.yaml', 'src/doc/petstore.yaml' infoUrl = 'https://my-api.company.com' infoEmail = 'info@company.com' } Source exportOpenApi.gradle task exportOpenApi ( type: org.openapitools.generator.gradle.plugin.tasks.GenerateTask, group: 'docToolchain', description: 'exports OpenAPI specification to the asciidoc file') { if (!specFile) { logger.info(\n---&gt; OpenAPI specification file not found in Config.groovy (https://doctoolchain.github.io/docToolchain/#_exportopenapi)) return } else { logger.info(Found OpenAPI specification in Config.groovy) } outputs.upToDateWhen { false } outputs.cacheIf { false } generatorName = 'asciidoc' outputDir = ${targetDir}/OpenAPI.toString() inputSpec = ${docDir}/${specFile} // plugin is not able to find file if inputPath is defined as '.' logger.debug(\n=====================\nProject Config:\n=====================) logger.debug(Docdir: ${docDir}) logger.debug(Target: ${targetDir}) logger.info(\n=====================\nOpenAPI Config:\n=====================) logger.info(Specification file: ${specFile}) logger.info(inputSpec: ${inputSpec}) logger.info(outputDir: ${outputDir}\n) additionalProperties = [ infoEmail:${config.openApi.infoEmail}, infoUrl:${config.openApi.infoUrl} ] } .gravatar img { margin-left: 3px; border-radius: 4px; } 2.3. htmlSanityCheck 1 minute to read This task invokes the htmlSanityCheck gradle plugin. It is a standalone (batch- and command-line) html sanity checker - it detects missing images, dead links, and duplicate bookmarks. In docToolchain, this task is used to ensure that the generated HTML contains no missing links or other problems. This task is the last default task and creates a report in build/report/htmlchecks/index.html Figure 4. sample report Further information can be found on GitHub: https://github.com/aim42/htmlSanityCheck Tip Blog-Post: Automated Quality-Checks Source htmlSanityCheck.gradle htmlSanityCheck { sourceDir = new File(config.htmlSanityCheck.sourceDir?targetDir+/+config.htmlSanityCheck.sourceDir:$targetDir/html5) // files to check - in Set-notation //sourceDocuments = [ one-file.html, another-file.html, index.html] // where to put results of sanityChecks... checkingResultsDir = new File(config.htmlSanityCheck.checkingResultsDir?:checkingResultsPath) // directory where the results written to in JUnit XML format junitResultsDir = new File(config.htmlSanityCheck.junitResultsDir?:$targetDir/test-results/htmlchecks) // which statuscodes shall be interpreted as warning, error or success defaults to standard httpSuccessCodes = config.htmlSanityCheck.httpSuccessCodes?:[] httpWarningCodes = config.htmlSanityCheck.httpWarningCodes?:[] httpErrorCodes = config.htmlSanityCheck.httpErrorCodes?:[] // fail build on errors? failOnErrors = config.htmlSanityCheck.failOnErrors?:false logger.info docToolchain&gt; HSC sourceDir: ${sourceDir} logger.info docToolchain&gt; HSC checkingResultsDir: ${checkingResultsDir} } .gravatar img { margin-left: 3px; border-radius: 4px; } 2.4. dependencyUpdates 1 minute to read This task uses the Gradle versions plugin created by Ben Manes to check for outdated build dependencies. Use this task to keep all dependencies up to date. Warning If you discover newer version, it doesn&#8217;t mean that versions and dependencies will play nicely together. To ensure that everything works, we recommend the versions selected by docToolchain contributors. Tip For more information, read the Handle Dependency Updates the easy Way blog post. Unresolved directive in &lt;stdin&gt; - include::../010_manual/08_development.adoc[leveloffset=+1] Unresolved directive in &lt;stdin&gt; - include::../010_manual/050_Frequently_asked_Questions.adoc[leveloffset=+1] Unresolved directive in &lt;stdin&gt; - include::../010_manual/04_further_reading.adoc[leveloffset=+1] document.location.href = '../10_about/30_community.html'; Unresolved directive in &lt;stdin&gt; - include::../010_manual/100_config.adoc[leveloffset=+1] "
},

{
    "id": 75,
    "uri": "search.html",
    "menu": "-",
    "title": "search",
    "text": " Search Results "
},

{
    "id": 76,
    "uri": "lunrjsindex.html",
    "menu": "-",
    "title": "null",
    "text": " will be replaced by the index "
},

];
